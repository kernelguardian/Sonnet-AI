{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every line is separated by '\\<eos>' tag and a ',' comma. Also each sonnet's beginning is represented by '\\\\' symbol or by counting 14 lines from initial sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "f = open('dataset.txt', 'r', encoding='utf-8')\n",
    "sonnets_dataset = f.read().split('<eos>')\n",
    "del sonnets_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Sonnets in dataset: 2685\n"
     ]
    }
   ],
   "source": [
    "# Total number of sonnets in dataset\n",
    "len_sonnets = int(len(sonnets_dataset)/14)\n",
    "print(\"Total Number of Sonnets in dataset:\", len_sonnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sonnet: 0 \n",
      " ['with clytia he no longer was received ', ' than while he was a man of wealth believed ', \" balls , concerts , op'ras , tournaments , and plays \", ' expensive dresses , all engaging ways ', ' were used to captivate this lady fair ', ' while scarcely one around but in despair ', ' wife , widow , maid , his fond affection sought ', \" to gain him , ev'ry wily art was brought \", \" but all in vain :— by passion overpow'red \", ' the belle , whose conduct others would have soured ', ' to him appeared a goddess full of charms ', \" superior e'en to helen , in his arms \", ' from whence we may conclude , the beauteous dame ', \" was always deaf to fred'rick ' s ardent flame \"] \n",
      "\n",
      "Sonnet: 1 \n",
      " [' than while he was a man of wealth believed ', \" balls , concerts , op'ras , tournaments , and plays \", ' expensive dresses , all engaging ways ', ' were used to captivate this lady fair ', ' while scarcely one around but in despair ', ' wife , widow , maid , his fond affection sought ', \" to gain him , ev'ry wily art was brought \", \" but all in vain :— by passion overpow'red \", ' the belle , whose conduct others would have soured ', ' to him appeared a goddess full of charms ', \" superior e'en to helen , in his arms \", ' from whence we may conclude , the beauteous dame ', \" was always deaf to fred'rick ' s ardent flame \", '\\nour love is not a fading earthly flower '] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To get each sonnet separately\n",
    "def sonnet_print(sonnet):\n",
    "    return sonnets_dataset[sonnet:sonnet+14]\n",
    "\n",
    "for sonnet in range(0, 2):\n",
    "    _ = sonnet_print(sonnet)\n",
    "    print(\"Sonnet:\", sonnet, \"\\n\", _, \"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with clytia he no longer was received ',\n",
       " ' than while he was a man of wealth believed ',\n",
       " \" balls , concerts , op'ras , tournaments , and plays \",\n",
       " ' expensive dresses , all engaging ways ',\n",
       " ' were used to captivate this lady fair ',\n",
       " ' while scarcely one around but in despair ',\n",
       " ' wife , widow , maid , his fond affection sought ',\n",
       " \" to gain him , ev'ry wily art was brought \",\n",
       " \" but all in vain :— by passion overpow'red \",\n",
       " ' the belle , whose conduct others would have soured ',\n",
       " ' to him appeared a goddess full of charms ',\n",
       " \" superior e'en to helen , in his arms \",\n",
       " ' from whence we may conclude , the beauteous dame ',\n",
       " \" was always deaf to fred'rick ' s ardent flame \",\n",
       " '\\nour love is not a fading earthly flower ',\n",
       " ' its wing è d seed dropped down from paradise ',\n",
       " ' and , nursed by day and night , by sun and shower ',\n",
       " ' doth momently to fresher beauty rise ',\n",
       " ' to us the leafless autumn is not bare ',\n",
       " \" nor winter 's rattling boughs lack lusty green \",\n",
       " \" our summer hearts make summer 's fullness where \",\n",
       " ' no leaf or bud or blossom may be seen ',\n",
       " \" for nature 's life in love 's deep life doth lie \",\n",
       " \" love ,— whose forgetfulness is beauty 's death \",\n",
       " ' whose mystic key these cells of thou and i ',\n",
       " ' into the infinite freedom openeth ',\n",
       " \" and makes the body 's dark and narrow grate \",\n",
       " \" the wide flung leaves of heaven 's palace gate \",\n",
       " '\\nsay thou , his will be done who is the good ',\n",
       " ' his will be borne who knoweth how to bear ',\n",
       " ' who also in the night had need of prayer ',\n",
       " ' both when awoke divinely longing mood ',\n",
       " ' and when the power of darkness him withstood ',\n",
       " ' for what is coming take no jot of care ',\n",
       " ' behind , before , around thee as the air ',\n",
       " \" he o'er thee like thy mother 's heart will brood \",\n",
       " ' and when thou hast wearied thy wings of prayer ',\n",
       " ' then fold them , and drop gently to thy nest ',\n",
       " ' which is thy faith ; and make thy people blest ',\n",
       " \" with what thou bring'st from that ethereal height \",\n",
       " ' which whoso looks on thee will straightway share ',\n",
       " ' he needs no eyes who is a shining light ',\n",
       " '\\nthe spirit of enjoyment and desire ',\n",
       " ' and hopes and wishes , from all living things ',\n",
       " ' went circling , like a multitude of sounds ',\n",
       " \" the budding groves appear 'd as if in haste \",\n",
       " ' to spur the steps of june ; as if their shades ',\n",
       " ' of various green were hindrances that stood ',\n",
       " ' between them and their object : yet , meanwhile ',\n",
       " ' there was such deep contentment in the air ',\n",
       " ' that every naked ash , and tardy tree ',\n",
       " \" yet leafless , seem 'd as though the countenance \",\n",
       " \" with which it look 'd on this delightful day \",\n",
       " ' were native to the summer .— up the brook ',\n",
       " \" i roam 'd in the confusion of my heart \",\n",
       " ' alive to all things and forgetting all ',\n",
       " '\\nand yet i cannot reprehend the flight ',\n",
       " \" or blame th'attempt , presuming so to soar \",\n",
       " ' the mounting venture for a high delight ',\n",
       " ' did make the honour of the fall the more ',\n",
       " ' for who gets wealth , that puts not from the shore ',\n",
       " ' danger hath honours , great designs their fame ',\n",
       " ' glory doth follow , courage goes before ',\n",
       " \" and though th'event oft answers not the same \",\n",
       " ' suffice that high attempts have never shame ',\n",
       " ' the mean observer whom base safety keeps ',\n",
       " ' lives without honour , dies without a name ',\n",
       " ' and in eternal darkness ever sleeps ',\n",
       " ' and therefore , delia , ‘ tis to me no blot ',\n",
       " ' to have attempted though attained thee not ',\n",
       " '\\nmy grief begun , fair saint , when first i saw ',\n",
       " ' love in those eyes sit ruling with disdain ',\n",
       " ' whose sweet commands did keep a world in awe ',\n",
       " ' and caused them serve your favor to obtain ',\n",
       " ' i stood as one enchanted with a frown ',\n",
       " ' yet smiled to see all creatures serve those eyes ',\n",
       " ' where each with sighs paid tribute to that crown ',\n",
       " ' and thought them grac è d by your dumb replies ',\n",
       " ' but i , ambitious , could not be content ',\n",
       " ' till that my service more than sighs made known ',\n",
       " ' and for that end my heart to you i sent ',\n",
       " ' to say and swear that , fair , it is your own ',\n",
       " ' then greater graces , licia , do impart ',\n",
       " ' not dumb replies unto a speaking heart ',\n",
       " \"\\nwe soon approach 'd it . o thou lombard spirit \",\n",
       " ' how didst thou stand , in high abstracted mood ',\n",
       " ' scarce moving with slow dignity thine eyes ',\n",
       " ' it spoke not aught , but let us onward pass ',\n",
       " ' eyeing us as a lion on his watch ',\n",
       " \" but virgil with entreaty mild advanc 'd \",\n",
       " ' requesting it to show the best ascent ',\n",
       " \" it answer to his question none return 'd \",\n",
       " ' but of our country and our kind of life ',\n",
       " ' demanded . when my courteous guide began ',\n",
       " ' “ mantua , ” the solitary shadow quick ',\n",
       " ' rose towards us from the place in which it stood ',\n",
       " \" and cry 'd , “ mantuan ! i am thy countryman \",\n",
       " \" sordello . ” each the other then embrac 'd \",\n",
       " '\\ni meet not mine by others ’ discontent ',\n",
       " ' for none compares with me in true devotion ',\n",
       " ' yet though my tears and sighs to her be spent ',\n",
       " ' her cruel heart disdains what they do motion ',\n",
       " ' yet though persisting in eternal hate ',\n",
       " ' to aggravate the cause of my complaining ',\n",
       " \" her fury ne'er confineth with a date \",\n",
       " ' i will not cease to love , for her disdaining ',\n",
       " ' such puny thoughts of unresolv è d ground ',\n",
       " ' whose inaudacity dares but base conceit ',\n",
       " ' in me and my love never shall be found ',\n",
       " ' those coward thoughts unworthy minds await ',\n",
       " ' but those that love well have not yet begun ',\n",
       " ' pers è ver ever and have never done ',\n",
       " '\\nbut hark ! the doors on jarring hinges turn ',\n",
       " \" all enter in , and the blest scene 's begun \",\n",
       " ' a thousand lights their livid flames display ',\n",
       " ' pour forth their blaze , and form a mimick day ',\n",
       " ' sudden a motley mixture fills the place ',\n",
       " ' and footmen shine as lordly as his grace ',\n",
       " ' to see the sad effect and power of change ',\n",
       " \" ladies turn 'd men , in breeches freely range \",\n",
       " \" young smooth chin 'd beaux turn priests and fryars \",\n",
       " \" and nun 's chaste habits hide our country ‘ squires \",\n",
       " ' belles , beaux , and sharpers here together play ',\n",
       " ' and wives throw their good spouses wealth away ',\n",
       " ' and when their cash runs low , and fate runs cross ',\n",
       " ' they then cornute ‘ em to retrieve their loss ',\n",
       " \"\\n“ i 'll come to that in time , if you 'll be still \",\n",
       " ' well , so the bishop left , and in six weeks ',\n",
       " ' or somewhere there , i started for montreaux ',\n",
       " ' to meet the bishop . shipped ahead my trunk ',\n",
       " ' to jaffa as the bishop did . but now ',\n",
       " ' i must tell you my dream . the night before ',\n",
       " ' i reached montreaux i had a wondrous dream ',\n",
       " ' i saw the bishop on the station platform ',\n",
       " ' his face with brandy blossoms splotched and wearing ',\n",
       " ' his gold head cane . and sure enough next day ',\n",
       " ' as i stepped from the train i saw the bishop ',\n",
       " ' his face with brandy blossoms splotched and wearing ',\n",
       " ' his gold head cane . and i thought something wrong ',\n",
       " \" and still i did n't act upon the thought \",\n",
       " '\\nis that a swan that rides upon the water ',\n",
       " ' oh no , it is that other gentle bird ',\n",
       " ' which is the patron of our noble calling ',\n",
       " ' i well remember , in my early years ',\n",
       " ' when these young hands first closed upon a goose ',\n",
       " ' i have a scar upon my thimble finger ',\n",
       " ' which chronicles the hour of young ambition ',\n",
       " ' my father was a tailor , and his father ',\n",
       " \" and my sire 's grandsire , all of them were tailors \",\n",
       " ' they had an ancient goose ,— it was an heirloom ',\n",
       " ' from some remoter tailor of our race ',\n",
       " ' it happened i did see it on a time ',\n",
       " ' when none was near , and i did deal with it ',\n",
       " ' and it did burn me ,— oh , most fearfully ',\n",
       " '\\ngreat ajax telamon first hyrtius smote ',\n",
       " ' the son of gyrtius , who to battle led ',\n",
       " ' the warlike mysians ; next antilochus ',\n",
       " \" from mermerus and phalces stripp 'd their arms \",\n",
       " ' meriones hippotion gave to death ',\n",
       " ' and morys ; teucer periphetes slew ',\n",
       " ' and prothoon ; menelaus , through the flank ',\n",
       " ' smote hyperenor ; as the grinding spear ',\n",
       " \" drain 'd all his vitals , through the gaping wound \",\n",
       " \" his spirit escap 'd , and darkness clos 'd his eyes \",\n",
       " ' but chiefest slaughter of the trojans wrought ',\n",
       " ' oileus ’ active son ; of all the greeks ',\n",
       " \" no foot so swift as his , when jove had fill 'd \",\n",
       " ' their souls with fear , to chase the flying foe ',\n",
       " '\\nitalia ! thou art fallen , though with sheen ',\n",
       " ' of battle spears thy clamorous armies stride ',\n",
       " ' from the north alps to the sicilian tide ',\n",
       " ' ay ! fallen , though the nations hail thee queen ',\n",
       " ' because rich gold in every town is seen ',\n",
       " ' and on thy sapphire lake in tossing pride ',\n",
       " ' of wind filled vans thy myriad galleys ride ',\n",
       " ' beneath one flag of red and white and green ',\n",
       " ' o fair and strong ! o strong and fair in vain ',\n",
       " \" look southward where rome 's desecrated town \",\n",
       " ' lies mourning for her god anointed king ',\n",
       " ' look heaven ward ! shall god allow this thing ',\n",
       " ' nay ! but some flame girt raphael shall come down ',\n",
       " ' and smite the spoiler with the sword of pain ',\n",
       " '\\n“ the barley harvest was nodding white ',\n",
       " ' when my children died on the rocky height ',\n",
       " ' and the reapers were singing on hill and plain ',\n",
       " ' when i came to my task of sorrow and pain ',\n",
       " ' but now the season of rain is nigh ',\n",
       " ' the sun is dim in the thickening sky ',\n",
       " ' and the clouds in sullen darkness rest ',\n",
       " ' where he hides his light at the doors of the west ',\n",
       " ' i hear the howl of the wind that brings ',\n",
       " ' the long drear storm on its heavy wings ',\n",
       " ' but the howling wind and the driving rain ',\n",
       " ' will beat on my houseless head in vain ',\n",
       " ' i shall stay , from my murdered sons to scare ',\n",
       " ' the beasts of the desert , and fowls of air ',\n",
       " '\\nwhere dim the ethereal eye , no art , though clear ',\n",
       " \" as golden star in morning 's amber springs \",\n",
       " ' can pierce the fogs of low imaginings ',\n",
       " ' painting and sculpture are a mockery mere ',\n",
       " ' where dull to deafness is the hearing ear ',\n",
       " ' vain is the poet . nought but earthly things ',\n",
       " ' have credence . when the soaring skylark sings ',\n",
       " ' how shall the stony statue strain to hear ',\n",
       " ' open the deaf ear , wake the sleeping eye ',\n",
       " ' and lo , musicians , painters , poets — all ',\n",
       " ' trooping instinctive , come without a call ',\n",
       " ' as winds that where they list blow evermore ',\n",
       " ' as waves from silent deserts roll to die ',\n",
       " ' in mighty voices on the peopled shore ',\n",
       " '\\nabout a mile behind the viny banks ',\n",
       " ' how sweet it was , upon a sloping green ',\n",
       " ' sunspread , and shaded with a branching screen ',\n",
       " ' to lie in peace half murmuring words of thanks ',\n",
       " ' to see the mountains on each other climb ',\n",
       " ' with spaces for rich meadows flowery bright ',\n",
       " ' the winding river freshening the sight ',\n",
       " ' at intervals , the trees in leafy prime ',\n",
       " ' the distant village roofs of blue and white ',\n",
       " ' with intersections of quaint fashioned beams ',\n",
       " ' all slanting crosswise , and the feudal gleams ',\n",
       " ' of ruined turrets , barren in the light ',\n",
       " ' to watch the changing clouds , like clime in clime ',\n",
       " ' oh sweet to lie and bless the luxury of time ',\n",
       " '\\nthe knave of diamonds tries his wily arts ',\n",
       " ' and wins ( oh shameful chance !) the queen of hearts ',\n",
       " \" at this , the blood the virgin 's cheek forsook \",\n",
       " \" a livid paleness spreads o'er all her look \",\n",
       " ' she sees , and trembles at th ’ approaching ill ',\n",
       " ' just in the jaws of ruin , and codille ',\n",
       " ' and now ( as oft in some distempered state ',\n",
       " ' on one nice trick depends the general fate ',\n",
       " ' an ace of hearts steps forth : the king unseen ',\n",
       " ' lurked in her hand , and mourned his captive queen ',\n",
       " ' he springs to vengeance with an eager pace ',\n",
       " ' and falls like thunder on the prostrate ace ',\n",
       " ' the nymph exulting fills with shouts the sky ',\n",
       " ' the walls , the woods , and long canals reply ',\n",
       " '\\nwhoever thinks a faultless piece to see ',\n",
       " \" thinks what ne'er was , nor is , nor e'er shall be \",\n",
       " \" in every work regard the writer 's end \",\n",
       " ' since none can compass more than they intend ',\n",
       " ' and if the means be just , the conduct true ',\n",
       " ' applause , in spite of trivial faults , is due ',\n",
       " ' as men of breeding , sometimes men of wit ',\n",
       " ' to avoid great errors , must the less commit ',\n",
       " ' neglect the rules each verbal critic lays ',\n",
       " ' for not to know some trifles is a praise ',\n",
       " ' most critics , fond of some subservient art ',\n",
       " ' still make the whole depend upon a part ',\n",
       " ' they talk of principles , but notions prize ',\n",
       " ' and all to one loved folly sacrifice ',\n",
       " \"\\nlo , the poor indian ! whose untutor 'd mind \",\n",
       " ' sees god in clouds , or hears him in the wind ',\n",
       " ' his soul , proud science never taught to stray ',\n",
       " ' far as the solar walk , or milky way ',\n",
       " ' yet simple nature to his hope has given ',\n",
       " \" behind the cloud topp 'd hill , an humbler heaven \",\n",
       " ' some safer world in depth of woods embraced ',\n",
       " ' some happier island in the watery waste ',\n",
       " ' where slaves once more their native land behold ',\n",
       " ' no fiends torment , no christians thirst for gold ',\n",
       " ' to be , contents his natural desire ',\n",
       " \" he asks no angel 's wing , no seraph 's fire \",\n",
       " ' but thinks , admitted to that equal sky ',\n",
       " ' his faithful dog shall bear him company ',\n",
       " \"\\nsir , if my unpolish't pen , that dedicates new \",\n",
       " ' the bashfull utterance of a maiden muse ',\n",
       " ' may gracefully arrive onely to you ',\n",
       " ' which for her virgin sake , do not refuse ',\n",
       " ' time , and more studious howers shall we vow ',\n",
       " ' to sing your vertues , which are now profuse ',\n",
       " ' kings haue drunke water from a louing hand ',\n",
       " \" and truth 's accepted , though we paint her poore \",\n",
       " ' the poets say , the gods that can command ',\n",
       " ' haue feasted gladly on a poore mans store ',\n",
       " ' whereby great sir , we haue to understand ',\n",
       " ' that humble riuers adde to the seashore ',\n",
       " \" liue long and happy , and with gray haires crown 'd \",\n",
       " ' reade thy youths acts , which fame shall euer sound ',\n",
       " '\\nas sheep , that step from forth their fold , by one ',\n",
       " ' or pairs , or three at once ; meanwhile the rest ',\n",
       " ' stand fearfully , bending the eye and nose ',\n",
       " ' to ground , and what the foremost does , that do ',\n",
       " \" the others , gath'ring round her , if she stops \",\n",
       " ' simple and quiet , nor the cause discern ',\n",
       " ' so saw i moving to advance the first ',\n",
       " ' who of that fortunate crew were at the head ',\n",
       " ' of modest mien and graceful in their gait ',\n",
       " ' when they before me had beheld the light ',\n",
       " ' from my right side fall broken on the ground ',\n",
       " \" so that the shadow reach 'd the cave , they stopp 'd \",\n",
       " \" and somewhat back retir 'd : the same did all \",\n",
       " \" who follow 'd , though unweeting of the cause \",\n",
       " '\\nhere paused the witch to mend her glowing fire ',\n",
       " ' while each man to his neighbour shuffled nigher ',\n",
       " ' as witch flame leapt and ever brighter grew ',\n",
       " ' till , to their horror , sudden it burned blue ',\n",
       " ' whereat each silent , fearful beholder ',\n",
       " \" felt in the gloom to touch his fellow 's shoulder \",\n",
       " ' yet , in that moment , knew an added dread ',\n",
       " ' to see the fire from blue turn ghastly red ',\n",
       " \" then , as the witch did o'er it crooning lean \",\n",
       " ' behold ! it changed again to baleful green ',\n",
       " ' whereat the witch flung bony arms on high ',\n",
       " ' as though with claw like hands she ‘ d rend the sky ',\n",
       " ' and while the lurid flames leapt ever higher ',\n",
       " ' she thus invoked the spirit of the fire ',\n",
       " '\\nwhen i consider how my light is spent ',\n",
       " ' ere half my days , in this dark world and wide ',\n",
       " ' and that one talent which is death to hide ',\n",
       " ' lodged with me useless , though my soul more bent ',\n",
       " ' to serve therewith my maker , and present ',\n",
       " ' my true account , lest he , returning , chide ',\n",
       " ' “ doth god exact day labor , light denied ',\n",
       " ' i fondly ask . but patience , to prevent ',\n",
       " ' that murmur , soon replies , “ god doth not need ',\n",
       " \" either man 's work or his own gifts . who best \",\n",
       " ' bear his mild yoke , they serve him best . his state ',\n",
       " ' is kingly : thousands at his bidding speed ',\n",
       " \" and post o'er land and ocean without rest \",\n",
       " ' they also serve who only stand and wait ',\n",
       " '\\nthe spark concealed himself ; each charm admired ',\n",
       " \" now this , now that , now t'other feature fired \",\n",
       " ' a hundred beauties caught his eager sight ',\n",
       " ' and while his bosom felt supreme delight ',\n",
       " ' he turned his thoughts advantages to take ',\n",
       " \" and of the maiden 's error something make \",\n",
       " ' assumed the character , and dress ; and air ',\n",
       " \" that should a wat'ry deity declare \",\n",
       " ' within the gliding flood his vestments dipt ',\n",
       " ' a crown of rushes on his head he slipt ',\n",
       " ' aquatick herbs and plants around he twined ',\n",
       " ' then mercury intreated to be kind ',\n",
       " ' and cupid too , the wily god of hearts ',\n",
       " ' how could the innocent resist these arts ',\n",
       " \"\\nhonora , shou 'd that cruel time arrive \",\n",
       " \" when ‘ gainst my truth thou should'st my errors poize \",\n",
       " \" scorning remembrance of our vanish 'd joys \",\n",
       " ' when for the love warm looks , in which i live ',\n",
       " ' but cold respect must greet me , that shall give ',\n",
       " ' no tender glance , no kind regretful sighs ',\n",
       " ' when thou shalt pass me with averted eyes ',\n",
       " \" feigning thou see'st me not , to sting , and grieve \",\n",
       " \" and sicken my sad heart , i cou 'd not bear \",\n",
       " ' such dire eclipse of thy soul cheering rays ',\n",
       " \" i cou 'd not learn my struggling heart to tear \",\n",
       " \" from thy lov 'd form , that thro ’ my memory strays \",\n",
       " ' nor in the pale horizon of despair ',\n",
       " \" endure the wintry and the darken 'd days \",\n",
       " '\\nhark you such sound as quivers ? kings will hear ',\n",
       " ' as kings have heard , and tremble on their thrones ',\n",
       " ' the old will feel the weight of mossy stones ',\n",
       " ' the young alone will laugh and scoff at fear ',\n",
       " ' it is the tread of armies marching near ',\n",
       " ' from scarlet lands to lands forever pale ',\n",
       " ' it is a bugle dying down the gale ',\n",
       " ' it is the sudden gushing of a tear ',\n",
       " ' and it is hands that grope at ghostly doors ',\n",
       " ' and romp of spirit children on the pave ',\n",
       " ' it is the tender sighing of the brave ',\n",
       " ' who fell , ah ! long ago , in futile wars ',\n",
       " ' it is such sound as death ; and , after all ',\n",
       " ' ‘ tis but the forest letting dead leaves fall ',\n",
       " '\\nxiv . as the pent torrent in uneasy rest ',\n",
       " ' under the griping rocks , doth ever keep ',\n",
       " ' a monstrous working as it lies asleep ',\n",
       " \" in the round hollow of some mountain 's breast \",\n",
       " ' till where it hideth in its sweltering nest ',\n",
       " ' some earthquake finds it , and its waters leap ',\n",
       " ' forth to the sunshine down the mighty steep ',\n",
       " ' so in thee once was anguished forth the quest ',\n",
       " ' whereby man sought for life power as he lay ',\n",
       " ' under his own proud heart and black despair ',\n",
       " ' wedged fast and stifled up with loads of care ',\n",
       " ' yet at dumb struggle with the tyrant clay ',\n",
       " ' thou wentest down below the roots of prayer ',\n",
       " ' and he hath cried aloud since that same day ',\n",
       " '\\nbut who , half sceptic , half afraid of wrong ',\n",
       " ' shall walk our streets , and mark the passing throng ',\n",
       " ' the brawny oaf in mould herculean cast ',\n",
       " ' the pigmy statesman trembling in his blast ',\n",
       " \" the cumb'rous citizen of portly paunch \",\n",
       " ' unwont to soar beyond the smoaking haunch ',\n",
       " ' the meagre bard behind the moving tun ',\n",
       " \" his shadow seeming lengthen 'd by the sun \",\n",
       " ' who forms scarce visible shall thus descry ',\n",
       " ' like flitting clouds athwart the mental sky ',\n",
       " ' from giant bodies then bare gleams of mind ',\n",
       " ' like mountain watch lights blinking to the wind ',\n",
       " ' nor blush to find his unperverted eye ',\n",
       " ' flash on his heart , and give his tongue the lie ',\n",
       " '\\nthree men lived yet when this dead man was young ',\n",
       " ' whose names and words endure for ever : one ',\n",
       " ' whose eyes grew dim with straining toward the sun ',\n",
       " \" and his wings weakened , and his angel 's tongue \",\n",
       " ' lost half the sweetest song was ever sung ',\n",
       " ' but like the strain half uttered earth hears none ',\n",
       " \" nor shall man hear till all men 's songs are done \",\n",
       " ' one whose clear spirit like an eagle hung ',\n",
       " ' between the mountains hallowed by his love ',\n",
       " ' and the sky stainless as his soul above ',\n",
       " ' and one the sweetest heart that ever spake ',\n",
       " ' the brightest words wherein sweet wisdom smiled ',\n",
       " ' these deathless names by this dead snake defiled ',\n",
       " ' bid memory spit upon him for their sake ',\n",
       " \"\\nfar out of bounds he 's figured — in a race \",\n",
       " ' of west end traffic pitching to his loss ',\n",
       " \" but if you 'd see him in his proper place \",\n",
       " ' making the browns for bub and grub and doss ',\n",
       " ' go east among the merchants and their men ',\n",
       " ' and where the press is noisiest , and the tides ',\n",
       " ' of trade run highest and widest , there and then ',\n",
       " ' you shall behold him , edging with equal strides ',\n",
       " ' along the kerb ; hawking in either hand ',\n",
       " ' some artful nothing made of twine and tin ',\n",
       " ' cardboard and foil and bits of rubber band ',\n",
       " \" some penn'orth of wit in fact that , with a grin \",\n",
       " ' the careful city marvels at , and buys ',\n",
       " ' for nurselings in the suburbs to despise ',\n",
       " '\\no lords our gods , it is not that ye sit ',\n",
       " ' serene above the thunder , and exempt ',\n",
       " ' from strife of tongues and casualties that tempt ',\n",
       " ' men merely found by proof of manhood fit ',\n",
       " ' for service of their fellows : this is it ',\n",
       " \" which sets you past the reach of time 's attempt \",\n",
       " ' which gives us right of justified contempt ',\n",
       " \" for commonwealths built up by mere men 's wit \",\n",
       " ' that gold unlocks not , nor may flatteries ope ',\n",
       " ' the portals of your heaven ; that none may hope ',\n",
       " ' with you to watch how life beneath you plods ',\n",
       " ' save for high service given , high duty done ',\n",
       " ' that never was your rank ignobly won ',\n",
       " ' for this we give you praise , o lords our gods ',\n",
       " '\\nthroughout the country nothing now was heard ',\n",
       " ' save talk of civil war ; yet undeterred ',\n",
       " ' was he , by what was going on around ',\n",
       " ' from his employment ; and kept gaining ground ',\n",
       " \" the village of brave soldier 's was quite full \",\n",
       " ' and they , alone , made business far from dull ',\n",
       " ' when he at first commended , he made a rule ',\n",
       " ' for which some folks then deemed him quite a fool ',\n",
       " ' to make good work and cheap , and have his pay ',\n",
       " ' for all he sold ; and this he did always ',\n",
       " ' he had been taught to look honesty ',\n",
       " ' as the best part of business policy ',\n",
       " ' and his experience fully proved the truth ',\n",
       " ' of that old maxim learned in early youth ',\n",
       " '\\ni must not think of thee ; and , tired yet strong ',\n",
       " ' i shun the thought that lurks in all delight ',\n",
       " \" the thought of thee — and in the blue heaven 's height \",\n",
       " ' and in the dearest passage of a song ',\n",
       " ' oh , just beyond the fairest thoughts that throng ',\n",
       " ' this breast the thought of thee waits , hidden yet bright ',\n",
       " ' but it must never , never come in sight ',\n",
       " ' i must stop short of thee the whole day long ',\n",
       " ' but when sleep comes to close each difficult day ',\n",
       " ' when night gives pause to the long watch i keep ',\n",
       " ' and all my bonds i needs must loose apart ',\n",
       " ' must doff my will as raiment laid away ',\n",
       " ' with the first dream that comes with the first sleep ',\n",
       " ' i run , i run , i am gathered to thy heart ',\n",
       " '\\n“ hear me , ye countless tribes , that dwelling round ',\n",
       " \" assist our cause ! you from your sev'ral homes \",\n",
       " \" not for display of numbers have i call 'd \",\n",
       " ' but that with willing hearts ye should defend ',\n",
       " ' our wives and infants from the warlike greeks ',\n",
       " \" for this i drain my people 's stores , for food \",\n",
       " ' and gifts for you , exalting your estate ',\n",
       " ' then , who will boldly onward , he may fall ',\n",
       " ' or safe escape , such is the chance of war ',\n",
       " ' but who within our valiant trojans ’ ranks ',\n",
       " ' shall but the body of patroclus bring ',\n",
       " ' despite the might of ajax ; half the spoils ',\n",
       " ' to him i give , the other half myself ',\n",
       " ' retaining ; and his praise shall equal mine ',\n",
       " '\\nmaids shout to breakfast in a merry strife ',\n",
       " ' and the cat runs to hear the whetted knife ',\n",
       " ' and dogs are ever in the way to watch ',\n",
       " ' the mouldy crust and falling bone to catch ',\n",
       " ' the wooden dishes round in haste are set ',\n",
       " ' and round the table all the boys are met ',\n",
       " ' all know their own save hodge who would be first ',\n",
       " ' but every one his master leaves the worst ',\n",
       " ' on every wooden dish , a humble claim ',\n",
       " \" two rude cut letters mark the owner 's name \",\n",
       " ' from every nook the smile of plenty calls ',\n",
       " ' and rusty flitches decorate the walls ',\n",
       " \" moore 's almanack where wonders never cease \",\n",
       " ' all smeared with candle snuff and bacon grease ',\n",
       " '\\nthe social laws from insult to protect ',\n",
       " ' to cherish peace , to cultivate respect ',\n",
       " ' the rich from wanton cruelty restrain ',\n",
       " ' to smooth the bed of penury and pain ',\n",
       " ' the hapless vagrant to his rest restore ',\n",
       " ' the maze of fraud , the haunts of theft explore ',\n",
       " ' the thoughtless maiden , when subdued by art ',\n",
       " ' to aid , and bring her rover to her heart ',\n",
       " \" wild riot 's voice with dignity to quell \",\n",
       " ' forbid unpeaceful passions to rebel ',\n",
       " ' wrest from revenge the meditated harm ',\n",
       " ' for this fair justice raised her sacred arm ',\n",
       " ' for this the rural magistrate , of yore ',\n",
       " ' thy honours , edward , to his mansion bore ',\n",
       " '\\ntake down thy stars , o god ! we look not up ',\n",
       " ' in vain thou hangest there thy changeless sign ',\n",
       " \" we lift our eyes to power 's glowing cup \",\n",
       " ' nor care if blood make strong that wizard wine ',\n",
       " ' so we but drink and feel the sorcery ',\n",
       " ' of conquest in our veins , of wits grown keen ',\n",
       " ' in strain and strife for flesh sweet sovereignty ',\n",
       " ' the fatal thrill of kingship over men ',\n",
       " ' what though the soul be from the body shrunk ',\n",
       " ' and we array the temple , but no god ',\n",
       " ' what though , the cup of golden greed once drunk ',\n",
       " ' our dust be laid in a dishonoured sod ',\n",
       " ' while thy loud hosts proclaim the end of wars ',\n",
       " ' we read no sign . o god , take down thy stars ',\n",
       " \"\\nyet , ‘ midst the blaze of courts , she fix 'd her love \",\n",
       " ' on the cool fountain , or the shady grove ',\n",
       " \" still , with the shepherd 's innocence , her mind \",\n",
       " ' to the sweet vale , and flowery mead , inclined ',\n",
       " \" and oft as spring renew 'd the plains with flowers \",\n",
       " ' breathed his soft gales , and led the fragrant hours ',\n",
       " ' with sure return she sought the sylvan scene ',\n",
       " ' the breezy mountains , and the forests green ',\n",
       " ' her maids around her moved , a duteous band ',\n",
       " ' each bore a crook , all rural , in her hand ',\n",
       " ' some simple lay , of flocks and herds , they sung ',\n",
       " ' with joy the mountain and the forest rung ',\n",
       " ' ‘ be every youth like royal abbas moved ',\n",
       " ' ‘ and every georgian maid like abra loved ',\n",
       " \"\\nbehold him trav'lling o'er th ’ extensive space \",\n",
       " ' between the realms of darkness and our race ',\n",
       " ' to pass it , scarcely he a moment took ',\n",
       " ' on florence instantly he cast a look ',\n",
       " ' delighted with the beauty of the spot ',\n",
       " ' he there resolved to fix his earthly lot ',\n",
       " ' regarding it as proper for his wiles ',\n",
       " ' a city famed for wanton freaks and guiles ',\n",
       " ' belphegor soon a noble mansion hired ',\n",
       " \" and furnished it with ev'ry thing desired \",\n",
       " ' as signor roderick he designed to pass ',\n",
       " \" his equipage was large of ev'ry class \",\n",
       " ' expense anticipating day by day ',\n",
       " ' what , in ten years , he had to throw away ',\n",
       " '\\nshe gave up beauty in her tender youth ',\n",
       " ' gave all her hope and joy and pleasant ways ',\n",
       " ' she covered up her eyes lest they should gaze ',\n",
       " ' on vanity , and chose the bitter truth ',\n",
       " ' harsh towards herself , towards others full of ruth ',\n",
       " ' servant of servants , little known to praise ',\n",
       " ' long prayers and fasts trenched on her nights and days ',\n",
       " ' she schooled herself to sights and sounds uncouth ',\n",
       " ' that with the poor and stricken she might make ',\n",
       " ' a home , until the least of all sufficed ',\n",
       " ' her wants ; her own self learned she to forsake ',\n",
       " ' counting all earthly gain but hurt and loss ',\n",
       " ' so with calm will she chose and bore the cross ',\n",
       " ' and hated all for love of jesus christ ',\n",
       " \"\\nwhome'er the hope , still blasted , still renew 'd \",\n",
       " ' of happiness lures on from toil to toil ',\n",
       " ' remember mahomet , and cease thy labour ',\n",
       " ' behold him here , in love , in war , successful ',\n",
       " ' behold him , wretched in his double triumph ',\n",
       " \" his fav'rite faithless , and his mistress base \",\n",
       " ' ambition only gave her to my arms ',\n",
       " \" by reason not convinc 'd , nor won by love \",\n",
       " ' ambition was her crime ; but meaner folly ',\n",
       " ' dooms me to loathe , at once , and dote on falsehood ',\n",
       " ' and idolize th ’ apostate i contemn ',\n",
       " ' if thou art more than the gay dream of fancy ',\n",
       " ' more than a pleasing sound , without a meaning ',\n",
       " \" o happiness ! sure thou art all aspasia 's \",\n",
       " '\\nis it her nature , or is it her will ',\n",
       " ' to be so cruell to an humbled foe ',\n",
       " ' if nature , then she may it mend with skill ',\n",
       " ' if will , then she at will may will forgoe ',\n",
       " ' but if her nature and her will be so ',\n",
       " ' that she will plague the man that loves her most ',\n",
       " \" and take delight t'encrease a wretches woe \",\n",
       " ' then all her natures goodly guifts are lost ',\n",
       " ' and that same glorious beauties ydle boast ',\n",
       " ' is but a bayt such wretches to beguile ',\n",
       " ' as , being long in her loves tempest tost ',\n",
       " ' she meanes at last to make her pitious spoyle ',\n",
       " ' o fayrest fayre ! let never it be named ',\n",
       " ' that so fayre beauty was so fowly shamed ',\n",
       " '\\nby red ripe mouth and brown , luxurious eyes ',\n",
       " ' of her i love , by all your sweetness shed ',\n",
       " ' in far , fair days , on one whose memory flies ',\n",
       " ' to faithless lights , and gracious speech gainsaid ',\n",
       " ' i pray you , when yon river path i tread ',\n",
       " ' make with the woodlands some soft compromise ',\n",
       " ' lest they should vex me into fruitless sighs ',\n",
       " \" with visions of a woman 's gleaming head \",\n",
       " ' for every green and golden hearted thing ',\n",
       " ' that gathers beauty in that shining place ',\n",
       " ' beloved of beams and wooed by wind and wing ',\n",
       " ' is rife with glimpses of her marvellous face ',\n",
       " ' and in the whispers of the lips of spring ',\n",
       " ' the music of her lute like voice i trace ',\n",
       " '\\nreignin my thoughts , fair hand , sweet eye , rare voice ',\n",
       " \" possess me whole , my heart 's triumvirate \",\n",
       " ' yet heavy heart , to make so hard a choice ',\n",
       " ' of such as spoil thy poor afflicted state ',\n",
       " ' for whilst they strive which shall be lord of all ',\n",
       " ' all my poor life by them is trodden down ',\n",
       " ' they all erect their trophies on my fall ',\n",
       " ' and yield me nought that gives them their renown ',\n",
       " ' when back i look , i sigh my freedom past ',\n",
       " ' and wail the state wherein i present stand ',\n",
       " ' and see my fortune ever like to last ',\n",
       " ' finding me reined with such a heavy hand ',\n",
       " ' what can i do but yield ? and yield i do ',\n",
       " ' and serve all three , and yet they spoil me too ',\n",
       " '\\nat last , so faire a ladie did i spie ',\n",
       " ' that thinking yet on her i burne and quake ',\n",
       " ' on hearbs and flowres she walked pensively ',\n",
       " ' milde , but yet love she proudly did forsake ',\n",
       " \" white seem 'd her robes , yet woven so they were \",\n",
       " ' as snow and golde together had been wrought ',\n",
       " ' above the wast a darke clowde shrouded her ',\n",
       " ' a stinging serpent by the heele her caught ',\n",
       " ' wherewith she languisht as the gathered floure ',\n",
       " \" and , well assur 'd , she mounted up to ioy \",\n",
       " ' alas ! on earth so nothing doth endure ',\n",
       " ' but bitter griefe and sorrowfull annoy ',\n",
       " ' which make this life wretched and miserable ',\n",
       " ' tossed with stormes of fortune variable ',\n",
       " '\\ntrust me , i have not earned your dear rebuke ',\n",
       " ' i love , as you would have me , god the most ',\n",
       " ' would lose not him , but you , must one be lost ',\n",
       " \" nor with lot 's wife cast back a faithless look \",\n",
       " ' unready to forego what i forsook ',\n",
       " ' this say i , having counted up the cost ',\n",
       " \" this , though i be the feeblest of god 's host \",\n",
       " ' the sorriest sheep christ shepherds with his crook ',\n",
       " ' yet while i love my god the most , i deem ',\n",
       " ' that i can never love you overmuch ',\n",
       " ' i love him more , so let me love you too ',\n",
       " ' yea , as i apprehend it , love is such ',\n",
       " ' i cannot love you if i love not him ',\n",
       " ' i cannot love him if i love not you ',\n",
       " '\\nlo here the impost of a faith entire ',\n",
       " ' that love doth pay , and her disdain extorts ',\n",
       " ' behold the message of a chaste desire ',\n",
       " ' that tells the world how much my grief imports ',\n",
       " \" these tributary passions , beauty 's due \",\n",
       " ' i send those eyes , the cabinets of love ',\n",
       " ' that cruelty herself might grieve to view ',\n",
       " \" th'affliction her unkind disdain doth move \",\n",
       " ' and how i live , cast down from off all mirth ',\n",
       " ' pensive , alone , only but with despair ',\n",
       " ' my joys abortive perish in their birth ',\n",
       " ' my griefs long lived and care succeeding care ',\n",
       " \" this is my state , and delia 's heart is such \",\n",
       " ' i say no more , i fear i said too much ',\n",
       " '\\nso if thou hadst been scorned in human eyes ',\n",
       " ' too bright and near to be a glory then ',\n",
       " \" if as truth 's artist , thou hadst been to men \",\n",
       " ' a setter forth of strange divinities ',\n",
       " ' to after times , thou , born in midday skies ',\n",
       " ' a sun , high up , out blazing sudden , when ',\n",
       " ' its light had had its centuries eight and ten ',\n",
       " ' to travel through the wretched void that lies ',\n",
       " ' ‘ twixt souls and truth , hadst been a love and fear ',\n",
       " \" worshipped on high from magian 's mountain crest \",\n",
       " \" and all night long symbol 'd by lamp flames clear \",\n",
       " \" thy sign , a star upon thy people 's breast \",\n",
       " ' where now a strange mysterious shape doth lie ',\n",
       " ' that once barred out the sun in noontide sky ',\n",
       " '\\nawake — arise ! all my stormy powers ',\n",
       " ' the earth , the fair earth , again is ours ',\n",
       " ' at my stern approach , pale autumn flings down ',\n",
       " ' in the dust her broken and faded crown ',\n",
       " ' at my glance the terrified mourner flies ',\n",
       " ' and the earth is filled with her doleful cries ',\n",
       " \" awake !— for the season of flowers is o'er \",\n",
       " ' my white banner unfurl on each northern shore ',\n",
       " ' ye have slumbered long in my icy chain ',\n",
       " ' ye are free to travel the land and main ',\n",
       " ' spirits of frost ! quit your mountains of snow ',\n",
       " ' will ye longer suffer the streams to flow ',\n",
       " ' up , up , and away from your rocky caves ',\n",
       " ' and herald me over the pathless waves ',\n",
       " '\\ni have heard the pigeons of the seven woods ',\n",
       " ' make their faint thunder , and the garden bees ',\n",
       " ' hum in the lime tree flowers ; and put away ',\n",
       " ' the unavailing outcries and the old bitterness ',\n",
       " ' that empty the heart . i have forgot awhile ',\n",
       " ' tara uprooted , and new commonness ',\n",
       " ' upon the throne and crying about the streets ',\n",
       " ' and hanging its paper flowers from post to post ',\n",
       " ' because it is alone of all things happy ',\n",
       " ' i am contented for i know that quiet ',\n",
       " ' wanders laughing and eating her wild heart ',\n",
       " ' among pigeons and bees , while that great archer ',\n",
       " ' who but awaits his hour to shoot , still hangs ',\n",
       " ' a cloudy quiver over parc na lee ',\n",
       " '\\nwait for the morning ! ah ! we wait indeed ',\n",
       " ' for daylight , we who toss about through stress ',\n",
       " ' of vacant armed desires and emptiness ',\n",
       " ' of all the warm , warm touches that we need ',\n",
       " ' and the warm kisses upon which we feed ',\n",
       " ' our famished lips in fancy ! may god bless ',\n",
       " ' the starved lips of us with but one caress ',\n",
       " ' warm as the yearning blood our poor hearts bleed ',\n",
       " ' a wild prayer —! bite thy pillow , praying so ',\n",
       " ' toss this side , and whirl that , and moan for dawn ',\n",
       " \" let the clock 's seconds dribble out their woe \",\n",
       " ' and time be drained of sorrow ! long ago ',\n",
       " ' we heard the crowing cock , with answer drawn ',\n",
       " ' as hoarsely sad at throat as sobs ... pray on ',\n",
       " '\\nthese wordes seyde he for the nones alle ',\n",
       " ' that with swich thing he mighte him angry maken ',\n",
       " ' and with an angre don his sorwe falle ',\n",
       " ' as for the tyme , and his corage awaken ',\n",
       " ' but wel he wist , as fer as tonges spaken ',\n",
       " ' ther nas a man of gretter hardinesse ',\n",
       " ' than he , ne more desired worthinesse ',\n",
       " ' ` what cas , ’ quod troilus , ` or what aventure ',\n",
       " ' hath gyded thee to see my languisshinge ',\n",
       " ' that am refus of euery creature ',\n",
       " ' but for the love of god , at my preyinge ',\n",
       " ' go henne a way , for certes , my deyinge ',\n",
       " ' wol thee disese , and i mot nedes deye ',\n",
       " ' ther for go wey , ther is no more to seye ',\n",
       " '\\naway they went , and gyges much admired ',\n",
       " ' still more than that : in truth his breast was fired ',\n",
       " ' for when she moved astonishment was great ',\n",
       " \" and ev'ry grace upon her seemed to wait \",\n",
       " \" emotion to suppress howe'er he tried \",\n",
       " ' since he had promised what he felt to hide ',\n",
       " ' to hold his tongue he wished , but that might raise ',\n",
       " ' suspicions of designs and mystick ways ',\n",
       " ' exaggeration was the better part ',\n",
       " ' and from the subject he would never start ',\n",
       " ' but fully praised each beauty in detail ',\n",
       " ' without appearing any thing to veil ',\n",
       " \" gods ! gyges cried , how truly , king , you 're blessed \",\n",
       " ' the skin how fair — how charming all the rest ',\n",
       " '\\nshe looked at him with almost haughty air ',\n",
       " ' to think that to reprove her he should dare ',\n",
       " ' then fearlessly as some undaunted child ',\n",
       " ' she met his eyes that searched her own for truth ',\n",
       " ' she who had scorned the tempest dark and wild ',\n",
       " ' feared not the chidings of his hasty youth ',\n",
       " ' and undismayed she moved to where he stood ',\n",
       " ' with blushing , beauteous charms of maidenhood ',\n",
       " ' and there with rapt eyes looking up to him ',\n",
       " ' she told him of those visions never dim ',\n",
       " ' of that wild spirit born amid the storm ',\n",
       " ' whose restless strength had swayed her fragile form ',\n",
       " ' before his own she laid her very soul ',\n",
       " ' that he might there its inmost thoughts unroll ',\n",
       " '\\nfair amaryllis , wilt thou never peep ',\n",
       " ' from forth the cave , and call me , and be mine ',\n",
       " ' lo , apples ten i bear thee from the steep ',\n",
       " ' these didst thou long for , and all these are thine ',\n",
       " ' ah , would i were a honey bee to sweep ',\n",
       " ' through ivy , and the bracken , and woodbine ',\n",
       " ' to watch thee waken , love , and watch thee sleep ',\n",
       " ' within thy grot below the shadowy pine ',\n",
       " ' now know i love , a cruel god is he ',\n",
       " ' the wild beast bare him in the wild wood drear ',\n",
       " ' and truly to the bone he burneth me ',\n",
       " \" but , black browed amaryllis , ne'er a tear \",\n",
       " ' nor sigh , nor blush , nor aught have i from thee ',\n",
       " ' nay , nor a kiss , a little gift and dear ',\n",
       " '\\nsurely thou seest , o spirit of light and fire ',\n",
       " ' surely thou canst not choose , o soul , but see ',\n",
       " ' the days whose dayspring was beheld of thee ',\n",
       " \" ere eyes less pure might have their hope 's desire \",\n",
       " ' beholding life in heaven again respire ',\n",
       " ' where men saw nought that was or was to be ',\n",
       " ' save only death imperial . thou and he ',\n",
       " \" who has the heart of all men 's hearts for lyre \",\n",
       " ' ye twain , being great of spirit as time is great ',\n",
       " \" and sure of sight as truth 's own heavenward eye \",\n",
       " ' beheld the forms of forces passing by ',\n",
       " ' and certitude of equal balanced fate ',\n",
       " ' whose breath forefelt makes darkness palpitate ',\n",
       " ' and knew that light should live and darkness die ',\n",
       " '\\ngazing he spoke , and , kindling at the view ',\n",
       " ' his eager arms around the goddess threw ',\n",
       " ' glad earth perceives , and from her bosom pours ',\n",
       " ' unbidden herbs and voluntary flowers ',\n",
       " ' thick new born violets a soft carpet spread ',\n",
       " ' and clustering lotos swell ’ d the rising bed ',\n",
       " ' and sudden hyacinths the turf bestrow ',\n",
       " ' and flamy crocus made the mountain glow ',\n",
       " ' there golden clouds conceal the heavenly pair ',\n",
       " ' steep ’ d in soft joys and circumfused with air ',\n",
       " ' celestial dews , descending o ’ er the ground ',\n",
       " ' perfume the mount , and breathe ambrosia round ',\n",
       " ' at length , with love and sleep ’ s soft power oppress ’ d ',\n",
       " ' the panting thunderer nods , and sinks to rest ',\n",
       " '\\nthese plaintive verse , the posts of my desire ',\n",
       " ' which haste for succour to her slow regard ',\n",
       " ' bear not report of any slender fire ',\n",
       " \" forging a grief to win a fame 's reward \",\n",
       " ' nor are my passions limned for outward hue ',\n",
       " ' for that no colours can depaint my sorrows ',\n",
       " ' delia herself , and all the world may view ',\n",
       " ' best in my face where cares have tilled deep furrows ',\n",
       " ' no bays i seek to deck my mourning brow ',\n",
       " ' o clear eyed rector of the holy hill ',\n",
       " ' my humble accents bear the olive bough ',\n",
       " ' of intercession but to move her will ',\n",
       " \" these lines i use t'unburden mine own heart \",\n",
       " ' my love affects no fame nor ‘ steems of art ',\n",
       " '\\nthrough the hushed street , along the silent plain ',\n",
       " ' the spectral future leads its mourning train ',\n",
       " ' dark with the shadows of uncounted bands ',\n",
       " \" where man 's white lips and woman 's wringing hands \",\n",
       " ' track the still burden , rolling slow before ',\n",
       " ' that love and kindness can protect no more ',\n",
       " ' the smiling babe that , called to mortal strife ',\n",
       " ' shuts its meek eyes and drops its little life ',\n",
       " ' the drooping child who prays in vain to live ',\n",
       " ' and pleads for help its parent cannot give ',\n",
       " ' the pride of beauty stricken in its flower ',\n",
       " ' the strength of manhood broken in an hour ',\n",
       " ' age in its weakness , bowed by toil and care ',\n",
       " ' traced in sad lines beneath its silvered hair ',\n",
       " '\\nthou art as tyrannous , so as thou art ',\n",
       " ' as those whose beauties proudly make them cruel ',\n",
       " \" for well thou know'st to my dear doting heart \",\n",
       " ' thou art the fairest and most precious jewel ',\n",
       " ' yet in good faith some say that thee behold ',\n",
       " ' thy face hath not the power to make love groan ',\n",
       " ' to say they err , i dare not be so bold ',\n",
       " ' although i swear it to my self alone ',\n",
       " ' and to be sure that is not false i swear ',\n",
       " ' a thousand groans but thinking on thy face ',\n",
       " \" one on another 's neck do witness bear \",\n",
       " \" thy black is fairest in my judgment 's place \",\n",
       " ' in nothing art thou black save in thy deeds ',\n",
       " ' and thence this slander as i think proceeds ',\n",
       " '\\nsince brass , nor stone , nor earth , nor boundless sea ',\n",
       " \" but sad mortality o'ersways their power \",\n",
       " ' how with this rage shall beauty hold a plea ',\n",
       " ' whose action is no stronger than a flower ',\n",
       " \" o how shall summer 's honey breath hold out \",\n",
       " \" against the wrackful siege of batt'ring days \",\n",
       " ' when rocks impregnable are not so stout ',\n",
       " ' nor gates of steel so strong but time decays ',\n",
       " ' o fearful meditation , where alack ',\n",
       " \" shall time 's best jewel from time 's chest lie hid \",\n",
       " ' or what strong hand can hold his swift foot back ',\n",
       " ' or who his spoil of beauty can forbid ',\n",
       " ' o none , unless this miracle have might ',\n",
       " ' that in black ink my love may still shine bright ',\n",
       " '\\nah me ! the fifty years since last we met ',\n",
       " ' seem to me fifty folios bound and set ',\n",
       " ' by time , the great transcriber , on his shelves ',\n",
       " ' wherein are written the histories of ourselves ',\n",
       " ' what tragedies , what comedies , are there ',\n",
       " ' what joy and grief , what rapture and despair ',\n",
       " ' what chronicles of triumph and defeat ',\n",
       " ' of struggle , and temptation , and retreat ',\n",
       " ' what records of regrets , and doubts , and fears ',\n",
       " ' what pages blotted , blistered by our tears ',\n",
       " ' what lovely landscapes on the margin shine ',\n",
       " ' what sweet , angelic faces , what divine ',\n",
       " ' and holy images of love and trust ',\n",
       " ' undimmed by age , unsoiled by damp or dust ',\n",
       " '\\ni see thine image through my tears to night ',\n",
       " ' and yet to day i saw thee smiling . how ',\n",
       " ' refer the cause ?— beloved , is it thou ',\n",
       " ' or i , who makes me sad ? the acolyte ',\n",
       " ' amid the chanted joy and thankful rite ',\n",
       " ' may so fall flat , with pale insensate brow ',\n",
       " ' on the altar stair . i hear thy voice and vow ',\n",
       " ' perplexed , uncertain , since thou art out of sight ',\n",
       " \" as he , in his swooning ears , the choir 's amen \",\n",
       " ' beloved , dost thou love ? or did i see all ',\n",
       " ' the glory as i dreamed , and fainted when ',\n",
       " ' too vehement light dilated my ideal ',\n",
       " \" for my soul 's eyes ? will that light come again \",\n",
       " ' as now these tears come — falling hot and real ',\n",
       " '\\nit one day happened , that this forward spark ',\n",
       " ' the girl we speak of , met within the park ',\n",
       " \" and to a summer house the fav'rite drew \",\n",
       " ' the course they took the princess chanced to view ',\n",
       " \" as wand'ring near ; but neither swain nor fair \",\n",
       " ' suspicion had , that any one was there ',\n",
       " ' and this gallant most confidently thought ',\n",
       " ' the girl by force , might to his terms be brought ',\n",
       " ' his wretched temper , obstacle to love ',\n",
       " \" and ev'ry bliss bestowed by heav'n above \",\n",
       " ' had oft his hopes of favours lately marred ',\n",
       " ' and fear , with those designs , had also jarred ',\n",
       " \" the girl , howe'er , would likely have been kind \",\n",
       " ' if opportunities had pleased her mind ',\n",
       " '\\nyet may we not entirely overlook ',\n",
       " ' the pleasure gathered from the rudiments ',\n",
       " ' of geometric science . though advanced ',\n",
       " ' in these inquiries , with regret i speak ',\n",
       " ' no farther than the threshold , [ g ] there i found ',\n",
       " ' both elevation and composed delight ',\n",
       " ' with indian awe and wonder , ignorance pleased ',\n",
       " ' with its own struggles , did i meditate ',\n",
       " ' on the relation those abstractions bear ',\n",
       " \" to nature 's laws , and by what process led \",\n",
       " ' those immaterial agents bowed their heads ',\n",
       " ' duly to serve the mind of earth born man ',\n",
       " ' from star to star , from kindred sphere to sphere ',\n",
       " ' from system on to system without end ',\n",
       " '\\nthen came a postscript dashed across the rest ',\n",
       " ' see that there be no traitors in your camp ',\n",
       " ' we seem a nest of traitors — none to trust ',\n",
       " ' since our arms failed — this egypt plague of men ',\n",
       " ' almost our maids were better at their homes ',\n",
       " ' than thus man girdled here : indeed i think ',\n",
       " ' our chiefest comfort is the little child ',\n",
       " ' of one unworthy mother ; which she left ',\n",
       " ' she shall not have it back : the child shall grow ',\n",
       " ' to prize the authentic mother of her mind ',\n",
       " ' i took it for an hour in mine own bed ',\n",
       " ' this morning : there the tender orphan hands ',\n",
       " ' felt at my heart , and seemed to charm from thence ',\n",
       " ' the wrath i nursed against the world : farewell ',\n",
       " '\\n` suffiseth this , my fulle freend pandare ',\n",
       " ' that i have seyd , for now wostow my wo ',\n",
       " ' and for the love of god , my colde care ',\n",
       " ' so hyd it wel , i telle it never to mo ',\n",
       " ' for harmes mighte folwen , mo than two ',\n",
       " ' if it were wist ; but be thou in gladnesse ',\n",
       " ' and lat me sterve , unknowe , of my distresse ',\n",
       " ' ` how hastow thus unkindely and longe ',\n",
       " ' hid this fro me , thou fool ? ’ quod pandarus ',\n",
       " ' ` paraunter thou might after swich oon longe ',\n",
       " ' that myn avys anoon may helpen us ',\n",
       " ' ` this were a wonder thing , ’ quod troylus ',\n",
       " ' ` thou coudest never in love thy selven wisse ',\n",
       " ' how devel maystow bringen me to blisse ',\n",
       " '\\nhard by the road , where on that little mound ',\n",
       " ' the high grass rustles to the passing breeze ',\n",
       " ' the child of misery rests her head in peace ',\n",
       " ' pause there in sadness . that unhallowed ground ',\n",
       " ' inshrines what once was isabel . sleep on ',\n",
       " ' sleep on , poor outcast ! lovely was thy cheek ',\n",
       " ' and thy mild eye was eloquent to speak ',\n",
       " ' the soul of pity . pale and woe begone ',\n",
       " ' soon did thy fair cheek fade , and thine eye weep ',\n",
       " ' the tear of anguish for the babe unborn ',\n",
       " ' the helpless heir of poverty and scorn ',\n",
       " \" she drank the draught that chill 'd her soul to sleep \",\n",
       " ' i pause and wipe the big drop from mine eye ',\n",
       " ' whilst the proud levite scowls and passes by ',\n",
       " '\\nshot up from broad rank blades that droop below ',\n",
       " ' the nodding wheat ear forms a graceful bow ',\n",
       " \" with milky kernels starting full , weigh 'd down \",\n",
       " \" ere yet the sun hath ting 'd its head with brown \",\n",
       " ' whilst thousands in a flock , for ever gay ',\n",
       " ' loud chirping sparrows welcome on the day ',\n",
       " ' and from the mazes of the leafy thorn ',\n",
       " ' drop one by one upon the bending corn ',\n",
       " ' giles with a pole assails their close retreats ',\n",
       " ' and round the grass grown dewy border beats ',\n",
       " ' on either side completely overspread ',\n",
       " \" here branches bend , there corn o'ertops his head \",\n",
       " ' green covert , hail ! for through the varying year ',\n",
       " ' no hours so sweet , no scene to him so dear ',\n",
       " '\\nhis shadow to narcissus well presented ',\n",
       " ' how fair he was by such attractive love ',\n",
       " \" so if thou would'st thyself thy beauty prove \",\n",
       " ' vulgar breath mirrors might have well contented ',\n",
       " ' and to their prayers eternally consented ',\n",
       " ' oaths , vows and sighs , if they believe might move ',\n",
       " \" but more thou forc'st , making my pen approve \",\n",
       " ' thy praise to all , least any had dissented ',\n",
       " ' when this hath wrought , thou which before wert known ',\n",
       " ' but unto some , of all art now required ',\n",
       " ' and thine eyes ’ wonders wronged , because not shown ',\n",
       " ' the world , with daily orisons desired ',\n",
       " \" thy chaste fair gifts , with learning 's breath is blown \",\n",
       " ' and thus my pen hath made thy sweets admired ',\n",
       " '\\nthe paynefull smith with force of fervent heat ',\n",
       " ' the hardest yron soone doth mollify ',\n",
       " ' that with his heavy sledge he can it beat ',\n",
       " ' and fashion to what he it list apply ',\n",
       " ' yet cannot all these flames in which i fry ',\n",
       " ' her hart , more hard then yron , soft a whit ',\n",
       " ' ne all the playnts and pray ë rs with which i ',\n",
       " \" doe beat on th'andvile of her stubberne wit \",\n",
       " ' but still , the more she fervent sees my fit ',\n",
       " ' the more she frieseth in her wilfull pryde ',\n",
       " ' and harder growes , the harder she is smit ',\n",
       " ' with all the playnts which to her be applyde ',\n",
       " ' what then remaines but i to ashes burne ',\n",
       " ' and she to stones at length all frosen turne ',\n",
       " '\\nthus prayed the sage : the eternal gave consent ',\n",
       " ' and peals of thunder shook the firmament ',\n",
       " ' presumptuous troy mistook the accepting sign ',\n",
       " ' and catch ’ d new fury at the voice divine ',\n",
       " ' as , when black tempests mix the seas and skies ',\n",
       " ' the roaring deeps in watery mountains rise ',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnets_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with clytia he no longer was received  than while he was a man of wealth believed  balls , concerts , op'ras , tournaments , and plays  expensive dresses , all engaging ways  were used to captivate this lady fair  while scarcely one around but in despair  wife , widow , maid , his fond affection sought  to gain him , ev'ry wily art was brought  but all in vain :— by passion overpow'red  the belle , whose conduct others would have soured  to him appeared a goddess full of charms  superior e'en to\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_string = ''\n",
    "for line in sonnets_dataset:\n",
    "    dataset_string = dataset_string + line\n",
    "dataset_string[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 55, 50, 36, 82, 81, 63, 72, 50, 55, 46, 82, 36, 15, 82, 65, 35,\n",
       "       82, 63, 35, 65,  7, 15, 39, 82, 17, 46, 47, 82, 39, 15, 81, 15, 55,\n",
       "       77, 15, 79, 82, 82, 50, 36, 46, 65, 82, 17, 36, 55, 63, 15, 82, 36,\n",
       "       15, 82, 17, 46, 47, 82, 46, 82, 13, 46, 65, 82, 35, 76, 82, 17, 15,\n",
       "       46, 63, 50, 36, 82, 26, 15, 63, 55, 15, 77, 15, 79, 82, 82, 26, 46,\n",
       "       63, 63, 47, 82,  3, 82, 81, 35, 65, 81, 15, 39, 50, 47, 82])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding Dataset \n",
    "chars = tuple(set(dataset_string))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in dataset_string])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n', array([53]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_string[618:619], encoded[618:619]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 14)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 55, 50, 36, 82, 81, 63, 72, 50, 55, 46, 82, 36, 15],\n",
       "       [39, 55, 13, 46, 65, 79, 82, 36, 55, 13, 82, 47, 35, 73]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 50, 36, 82, 81, 63, 72, 50, 55, 46, 82, 36, 15, 82],\n",
       "       [55, 13, 46, 65, 79, 82, 36, 55, 13, 82, 47, 35, 73, 65]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=14, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(88, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=88, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=3\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 3.0620... Val Loss: 2.9893\n",
      "Epoch: 1/10... Step: 20... Loss: 2.9145... Val Loss: 2.8913\n",
      "Epoch: 1/10... Step: 30... Loss: 2.9466... Val Loss: 2.8730\n",
      "Epoch: 1/10... Step: 40... Loss: 2.9529... Val Loss: 2.8711\n",
      "Epoch: 1/10... Step: 50... Loss: 2.9036... Val Loss: 2.8697\n",
      "Epoch: 1/10... Step: 60... Loss: 2.8860... Val Loss: 2.8748\n",
      "Epoch: 1/10... Step: 70... Loss: 2.9109... Val Loss: 2.8709\n",
      "Epoch: 1/10... Step: 80... Loss: 2.8297... Val Loss: 2.8676\n",
      "Epoch: 1/10... Step: 90... Loss: 2.8946... Val Loss: 2.8680\n",
      "Epoch: 1/10... Step: 100... Loss: 2.8449... Val Loss: 2.8670\n",
      "Epoch: 1/10... Step: 110... Loss: 2.8378... Val Loss: 2.8639\n",
      "Epoch: 1/10... Step: 120... Loss: 2.8866... Val Loss: 2.8654\n",
      "Epoch: 1/10... Step: 130... Loss: 2.8444... Val Loss: 2.8678\n",
      "Epoch: 1/10... Step: 140... Loss: 2.8898... Val Loss: 2.8651\n",
      "Epoch: 1/10... Step: 150... Loss: 2.8986... Val Loss: 2.8642\n",
      "Epoch: 1/10... Step: 160... Loss: 2.8027... Val Loss: 2.8629\n",
      "Epoch: 1/10... Step: 170... Loss: 2.8563... Val Loss: 2.8638\n",
      "Epoch: 1/10... Step: 180... Loss: 2.8232... Val Loss: 2.8612\n",
      "Epoch: 1/10... Step: 190... Loss: 2.8009... Val Loss: 2.8601\n",
      "Epoch: 1/10... Step: 200... Loss: 2.8746... Val Loss: 2.8508\n",
      "Epoch: 1/10... Step: 210... Loss: 2.8395... Val Loss: 2.8289\n",
      "Epoch: 1/10... Step: 220... Loss: 2.8676... Val Loss: 2.7970\n",
      "Epoch: 1/10... Step: 230... Loss: 2.7940... Val Loss: 2.7468\n",
      "Epoch: 1/10... Step: 240... Loss: 2.7285... Val Loss: 2.7060\n",
      "Epoch: 1/10... Step: 250... Loss: 2.7024... Val Loss: 2.6848\n",
      "Epoch: 1/10... Step: 260... Loss: 2.6810... Val Loss: 2.6707\n",
      "Epoch: 1/10... Step: 270... Loss: 2.7349... Val Loss: 2.6413\n",
      "Epoch: 1/10... Step: 280... Loss: 2.6628... Val Loss: 2.6176\n",
      "Epoch: 1/10... Step: 290... Loss: 2.6191... Val Loss: 2.5960\n",
      "Epoch: 1/10... Step: 300... Loss: 2.6479... Val Loss: 2.5811\n",
      "Epoch: 1/10... Step: 310... Loss: 2.6355... Val Loss: 2.5566\n",
      "Epoch: 1/10... Step: 320... Loss: 2.5279... Val Loss: 2.5236\n",
      "Epoch: 1/10... Step: 330... Loss: 2.5435... Val Loss: 2.5215\n",
      "Epoch: 1/10... Step: 340... Loss: 2.5329... Val Loss: 2.4640\n",
      "Epoch: 1/10... Step: 350... Loss: 2.4334... Val Loss: 2.4387\n",
      "Epoch: 1/10... Step: 360... Loss: 2.4861... Val Loss: 2.4164\n",
      "Epoch: 1/10... Step: 370... Loss: 2.4132... Val Loss: 2.3895\n",
      "Epoch: 1/10... Step: 380... Loss: 2.2605... Val Loss: 2.3832\n",
      "Epoch: 1/10... Step: 390... Loss: 2.4559... Val Loss: 2.3719\n",
      "Epoch: 1/10... Step: 400... Loss: 2.4103... Val Loss: 2.3536\n",
      "Epoch: 1/10... Step: 410... Loss: 2.3876... Val Loss: 2.3323\n",
      "Epoch: 1/10... Step: 420... Loss: 2.3054... Val Loss: 2.3092\n",
      "Epoch: 1/10... Step: 430... Loss: 2.2262... Val Loss: 2.3016\n",
      "Epoch: 1/10... Step: 440... Loss: 2.4574... Val Loss: 2.2804\n",
      "Epoch: 1/10... Step: 450... Loss: 2.3475... Val Loss: 2.2657\n",
      "Epoch: 1/10... Step: 460... Loss: 2.2517... Val Loss: 2.2582\n",
      "Epoch: 1/10... Step: 470... Loss: 2.3520... Val Loss: 2.2433\n",
      "Epoch: 1/10... Step: 480... Loss: 2.2105... Val Loss: 2.2301\n",
      "Epoch: 1/10... Step: 490... Loss: 2.4313... Val Loss: 2.2370\n",
      "Epoch: 1/10... Step: 500... Loss: 2.3647... Val Loss: 2.2078\n",
      "Epoch: 1/10... Step: 510... Loss: 2.2842... Val Loss: 2.2034\n",
      "Epoch: 1/10... Step: 520... Loss: 2.2075... Val Loss: 2.1879\n",
      "Epoch: 1/10... Step: 530... Loss: 2.2289... Val Loss: 2.1868\n",
      "Epoch: 1/10... Step: 540... Loss: 2.2240... Val Loss: 2.1941\n",
      "Epoch: 1/10... Step: 550... Loss: 2.2280... Val Loss: 2.1740\n",
      "Epoch: 1/10... Step: 560... Loss: 2.1800... Val Loss: 2.1651\n",
      "Epoch: 1/10... Step: 570... Loss: 2.1156... Val Loss: 2.1552\n",
      "Epoch: 1/10... Step: 580... Loss: 2.1944... Val Loss: 2.1557\n",
      "Epoch: 1/10... Step: 590... Loss: 2.1697... Val Loss: 2.1328\n",
      "Epoch: 1/10... Step: 600... Loss: 2.1147... Val Loss: 2.1282\n",
      "Epoch: 1/10... Step: 610... Loss: 2.1846... Val Loss: 2.1172\n",
      "Epoch: 1/10... Step: 620... Loss: 2.2143... Val Loss: 2.1111\n",
      "Epoch: 1/10... Step: 630... Loss: 2.1108... Val Loss: 2.1112\n",
      "Epoch: 1/10... Step: 640... Loss: 2.1079... Val Loss: 2.0950\n",
      "Epoch: 1/10... Step: 650... Loss: 2.0881... Val Loss: 2.0936\n",
      "Epoch: 1/10... Step: 660... Loss: 2.1546... Val Loss: 2.0840\n",
      "Epoch: 1/10... Step: 670... Loss: 2.0971... Val Loss: 2.0739\n",
      "Epoch: 1/10... Step: 680... Loss: 2.0528... Val Loss: 2.0655\n",
      "Epoch: 1/10... Step: 690... Loss: 2.0885... Val Loss: 2.0592\n",
      "Epoch: 1/10... Step: 700... Loss: 2.0818... Val Loss: 2.0492\n",
      "Epoch: 1/10... Step: 710... Loss: 2.1529... Val Loss: 2.0437\n",
      "Epoch: 1/10... Step: 720... Loss: 2.0820... Val Loss: 2.0359\n",
      "Epoch: 1/10... Step: 730... Loss: 2.0326... Val Loss: 2.0391\n",
      "Epoch: 1/10... Step: 740... Loss: 1.9744... Val Loss: 2.0240\n",
      "Epoch: 1/10... Step: 750... Loss: 2.1579... Val Loss: 2.0174\n",
      "Epoch: 1/10... Step: 760... Loss: 2.1947... Val Loss: 2.0144\n",
      "Epoch: 1/10... Step: 770... Loss: 2.1134... Val Loss: 2.0138\n",
      "Epoch: 1/10... Step: 780... Loss: 2.1243... Val Loss: 2.0110\n",
      "Epoch: 1/10... Step: 790... Loss: 2.0467... Val Loss: 2.0066\n",
      "Epoch: 1/10... Step: 800... Loss: 2.0244... Val Loss: 1.9984\n",
      "Epoch: 1/10... Step: 810... Loss: 2.0812... Val Loss: 1.9934\n",
      "Epoch: 1/10... Step: 820... Loss: 2.0695... Val Loss: 1.9863\n",
      "Epoch: 1/10... Step: 830... Loss: 2.0550... Val Loss: 1.9803\n",
      "Epoch: 1/10... Step: 840... Loss: 2.1131... Val Loss: 1.9686\n",
      "Epoch: 1/10... Step: 850... Loss: 1.8982... Val Loss: 1.9687\n",
      "Epoch: 1/10... Step: 860... Loss: 2.0982... Val Loss: 1.9602\n",
      "Epoch: 1/10... Step: 870... Loss: 1.9660... Val Loss: 1.9579\n",
      "Epoch: 1/10... Step: 880... Loss: 2.1083... Val Loss: 1.9541\n",
      "Epoch: 1/10... Step: 890... Loss: 2.1128... Val Loss: 1.9545\n",
      "Epoch: 1/10... Step: 900... Loss: 1.9717... Val Loss: 1.9504\n",
      "Epoch: 1/10... Step: 910... Loss: 2.1467... Val Loss: 1.9480\n",
      "Epoch: 1/10... Step: 920... Loss: 2.0589... Val Loss: 1.9405\n",
      "Epoch: 1/10... Step: 930... Loss: 1.9359... Val Loss: 1.9409\n",
      "Epoch: 1/10... Step: 940... Loss: 1.9982... Val Loss: 1.9386\n",
      "Epoch: 1/10... Step: 950... Loss: 1.9496... Val Loss: 1.9331\n",
      "Epoch: 1/10... Step: 960... Loss: 1.9908... Val Loss: 1.9323\n",
      "Epoch: 1/10... Step: 970... Loss: 1.9757... Val Loss: 1.9237\n",
      "Epoch: 1/10... Step: 980... Loss: 2.0255... Val Loss: 1.9237\n",
      "Epoch: 1/10... Step: 990... Loss: 2.0483... Val Loss: 1.9182\n",
      "Epoch: 1/10... Step: 1000... Loss: 1.9890... Val Loss: 1.9160\n",
      "Epoch: 1/10... Step: 1010... Loss: 2.0307... Val Loss: 1.9095\n",
      "Epoch: 1/10... Step: 1020... Loss: 1.9301... Val Loss: 1.9093\n",
      "Epoch: 1/10... Step: 1030... Loss: 1.9816... Val Loss: 1.9067\n",
      "Epoch: 1/10... Step: 1040... Loss: 1.9455... Val Loss: 1.8973\n",
      "Epoch: 1/10... Step: 1050... Loss: 1.9001... Val Loss: 1.8988\n",
      "Epoch: 1/10... Step: 1060... Loss: 1.9019... Val Loss: 1.8891\n",
      "Epoch: 1/10... Step: 1070... Loss: 1.8420... Val Loss: 1.8892\n",
      "Epoch: 1/10... Step: 1080... Loss: 1.9054... Val Loss: 1.8852\n",
      "Epoch: 1/10... Step: 1090... Loss: 2.0489... Val Loss: 1.8796\n",
      "Epoch: 1/10... Step: 1100... Loss: 2.0827... Val Loss: 1.8770\n",
      "Epoch: 1/10... Step: 1110... Loss: 1.9816... Val Loss: 1.8791\n",
      "Epoch: 1/10... Step: 1120... Loss: 1.8698... Val Loss: 1.8750\n",
      "Epoch: 1/10... Step: 1130... Loss: 2.0114... Val Loss: 1.8714\n",
      "Epoch: 1/10... Step: 1140... Loss: 1.8807... Val Loss: 1.8674\n",
      "Epoch: 1/10... Step: 1150... Loss: 1.9538... Val Loss: 1.8628\n",
      "Epoch: 1/10... Step: 1160... Loss: 1.9935... Val Loss: 1.8618\n",
      "Epoch: 1/10... Step: 1170... Loss: 1.8850... Val Loss: 1.8589\n",
      "Epoch: 1/10... Step: 1180... Loss: 1.9506... Val Loss: 1.8599\n",
      "Epoch: 1/10... Step: 1190... Loss: 1.9149... Val Loss: 1.8602\n",
      "Epoch: 1/10... Step: 1200... Loss: 1.8505... Val Loss: 1.8516\n",
      "Epoch: 1/10... Step: 1210... Loss: 1.8864... Val Loss: 1.8527\n",
      "Epoch: 1/10... Step: 1220... Loss: 2.0500... Val Loss: 1.8477\n",
      "Epoch: 1/10... Step: 1230... Loss: 1.9570... Val Loss: 1.8476\n",
      "Epoch: 1/10... Step: 1240... Loss: 1.9265... Val Loss: 1.8423\n",
      "Epoch: 1/10... Step: 1250... Loss: 1.9777... Val Loss: 1.8404\n",
      "Epoch: 1/10... Step: 1260... Loss: 1.8805... Val Loss: 1.8427\n",
      "Epoch: 1/10... Step: 1270... Loss: 1.8591... Val Loss: 1.8387\n",
      "Epoch: 1/10... Step: 1280... Loss: 1.8176... Val Loss: 1.8364\n",
      "Epoch: 1/10... Step: 1290... Loss: 1.9442... Val Loss: 1.8295\n",
      "Epoch: 1/10... Step: 1300... Loss: 1.8461... Val Loss: 1.8307\n",
      "Epoch: 1/10... Step: 1310... Loss: 1.9272... Val Loss: 1.8275\n",
      "Epoch: 1/10... Step: 1320... Loss: 1.7818... Val Loss: 1.8206\n",
      "Epoch: 1/10... Step: 1330... Loss: 2.0085... Val Loss: 1.8171\n",
      "Epoch: 1/10... Step: 1340... Loss: 1.8608... Val Loss: 1.8171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 1350... Loss: 1.8461... Val Loss: 1.8103\n",
      "Epoch: 1/10... Step: 1360... Loss: 1.9154... Val Loss: 1.8101\n",
      "Epoch: 1/10... Step: 1370... Loss: 1.9052... Val Loss: 1.8042\n",
      "Epoch: 1/10... Step: 1380... Loss: 1.9153... Val Loss: 1.8066\n",
      "Epoch: 1/10... Step: 1390... Loss: 1.8854... Val Loss: 1.8038\n",
      "Epoch: 1/10... Step: 1400... Loss: 1.7867... Val Loss: 1.8126\n",
      "Epoch: 1/10... Step: 1410... Loss: 1.8080... Val Loss: 1.8022\n",
      "Epoch: 1/10... Step: 1420... Loss: 1.8485... Val Loss: 1.7989\n",
      "Epoch: 1/10... Step: 1430... Loss: 1.8677... Val Loss: 1.7942\n",
      "Epoch: 1/10... Step: 1440... Loss: 1.8153... Val Loss: 1.7938\n",
      "Epoch: 1/10... Step: 1450... Loss: 1.8307... Val Loss: 1.7921\n",
      "Epoch: 1/10... Step: 1460... Loss: 1.8426... Val Loss: 1.7882\n",
      "Epoch: 1/10... Step: 1470... Loss: 1.7530... Val Loss: 1.7859\n",
      "Epoch: 1/10... Step: 1480... Loss: 1.8433... Val Loss: 1.7818\n",
      "Epoch: 1/10... Step: 1490... Loss: 1.8143... Val Loss: 1.7826\n",
      "Epoch: 1/10... Step: 1500... Loss: 1.8948... Val Loss: 1.7824\n",
      "Epoch: 1/10... Step: 1510... Loss: 1.8740... Val Loss: 1.7799\n",
      "Epoch: 1/10... Step: 1520... Loss: 1.8871... Val Loss: 1.7789\n",
      "Epoch: 1/10... Step: 1530... Loss: 1.8130... Val Loss: 1.7776\n",
      "Epoch: 1/10... Step: 1540... Loss: 1.6725... Val Loss: 1.7770\n",
      "Epoch: 1/10... Step: 1550... Loss: 1.9064... Val Loss: 1.7729\n",
      "Epoch: 1/10... Step: 1560... Loss: 1.8598... Val Loss: 1.7696\n",
      "Epoch: 1/10... Step: 1570... Loss: 1.7415... Val Loss: 1.7685\n",
      "Epoch: 1/10... Step: 1580... Loss: 1.7860... Val Loss: 1.7678\n",
      "Epoch: 1/10... Step: 1590... Loss: 1.8526... Val Loss: 1.7704\n",
      "Epoch: 1/10... Step: 1600... Loss: 1.8476... Val Loss: 1.7647\n",
      "Epoch: 1/10... Step: 1610... Loss: 1.8370... Val Loss: 1.7608\n",
      "Epoch: 1/10... Step: 1620... Loss: 1.7920... Val Loss: 1.7536\n",
      "Epoch: 1/10... Step: 1630... Loss: 1.7716... Val Loss: 1.7544\n",
      "Epoch: 1/10... Step: 1640... Loss: 1.8629... Val Loss: 1.7531\n",
      "Epoch: 1/10... Step: 1650... Loss: 1.8491... Val Loss: 1.7516\n",
      "Epoch: 1/10... Step: 1660... Loss: 1.7503... Val Loss: 1.7523\n",
      "Epoch: 1/10... Step: 1670... Loss: 1.8565... Val Loss: 1.7459\n",
      "Epoch: 1/10... Step: 1680... Loss: 1.8398... Val Loss: 1.7466\n",
      "Epoch: 1/10... Step: 1690... Loss: 1.6962... Val Loss: 1.7466\n",
      "Epoch: 1/10... Step: 1700... Loss: 1.9221... Val Loss: 1.7448\n",
      "Epoch: 1/10... Step: 1710... Loss: 1.8308... Val Loss: 1.7431\n",
      "Epoch: 1/10... Step: 1720... Loss: 1.8497... Val Loss: 1.7394\n",
      "Epoch: 1/10... Step: 1730... Loss: 1.8471... Val Loss: 1.7417\n",
      "Epoch: 1/10... Step: 1740... Loss: 1.7972... Val Loss: 1.7430\n",
      "Epoch: 1/10... Step: 1750... Loss: 1.9194... Val Loss: 1.7397\n",
      "Epoch: 1/10... Step: 1760... Loss: 1.8710... Val Loss: 1.7336\n",
      "Epoch: 1/10... Step: 1770... Loss: 1.8334... Val Loss: 1.7326\n",
      "Epoch: 1/10... Step: 1780... Loss: 1.8019... Val Loss: 1.7309\n",
      "Epoch: 1/10... Step: 1790... Loss: 1.8376... Val Loss: 1.7309\n",
      "Epoch: 1/10... Step: 1800... Loss: 1.7813... Val Loss: 1.7333\n",
      "Epoch: 1/10... Step: 1810... Loss: 1.7700... Val Loss: 1.7287\n",
      "Epoch: 1/10... Step: 1820... Loss: 1.7760... Val Loss: 1.7275\n",
      "Epoch: 1/10... Step: 1830... Loss: 1.8163... Val Loss: 1.7245\n",
      "Epoch: 1/10... Step: 1840... Loss: 1.9203... Val Loss: 1.7194\n",
      "Epoch: 1/10... Step: 1850... Loss: 1.7402... Val Loss: 1.7224\n",
      "Epoch: 1/10... Step: 1860... Loss: 1.8043... Val Loss: 1.7161\n",
      "Epoch: 1/10... Step: 1870... Loss: 1.8544... Val Loss: 1.7155\n",
      "Epoch: 1/10... Step: 1880... Loss: 1.7451... Val Loss: 1.7193\n",
      "Epoch: 1/10... Step: 1890... Loss: 1.7498... Val Loss: 1.7183\n",
      "Epoch: 1/10... Step: 1900... Loss: 1.8331... Val Loss: 1.7150\n",
      "Epoch: 1/10... Step: 1910... Loss: 1.8283... Val Loss: 1.7113\n",
      "Epoch: 1/10... Step: 1920... Loss: 1.7573... Val Loss: 1.7120\n",
      "Epoch: 1/10... Step: 1930... Loss: 1.7642... Val Loss: 1.7086\n",
      "Epoch: 1/10... Step: 1940... Loss: 1.8054... Val Loss: 1.7048\n",
      "Epoch: 1/10... Step: 1950... Loss: 1.7044... Val Loss: 1.7024\n",
      "Epoch: 1/10... Step: 1960... Loss: 1.6888... Val Loss: 1.6991\n",
      "Epoch: 1/10... Step: 1970... Loss: 1.6770... Val Loss: 1.7004\n",
      "Epoch: 1/10... Step: 1980... Loss: 1.9293... Val Loss: 1.7045\n",
      "Epoch: 1/10... Step: 1990... Loss: 1.7229... Val Loss: 1.7006\n",
      "Epoch: 1/10... Step: 2000... Loss: 1.8100... Val Loss: 1.6996\n",
      "Epoch: 1/10... Step: 2010... Loss: 1.8935... Val Loss: 1.6988\n",
      "Epoch: 1/10... Step: 2020... Loss: 1.7953... Val Loss: 1.7005\n",
      "Epoch: 1/10... Step: 2030... Loss: 1.7697... Val Loss: 1.6949\n",
      "Epoch: 1/10... Step: 2040... Loss: 1.7402... Val Loss: 1.6967\n",
      "Epoch: 1/10... Step: 2050... Loss: 1.7449... Val Loss: 1.6910\n",
      "Epoch: 1/10... Step: 2060... Loss: 1.7767... Val Loss: 1.6923\n",
      "Epoch: 1/10... Step: 2070... Loss: 1.7824... Val Loss: 1.6889\n",
      "Epoch: 1/10... Step: 2080... Loss: 1.7272... Val Loss: 1.6916\n",
      "Epoch: 1/10... Step: 2090... Loss: 1.7195... Val Loss: 1.6888\n",
      "Epoch: 1/10... Step: 2100... Loss: 1.6932... Val Loss: 1.6869\n",
      "Epoch: 1/10... Step: 2110... Loss: 1.6713... Val Loss: 1.6857\n",
      "Epoch: 1/10... Step: 2120... Loss: 1.7495... Val Loss: 1.6842\n",
      "Epoch: 1/10... Step: 2130... Loss: 1.8769... Val Loss: 1.6850\n",
      "Epoch: 1/10... Step: 2140... Loss: 1.7929... Val Loss: 1.6818\n",
      "Epoch: 1/10... Step: 2150... Loss: 1.7327... Val Loss: 1.6831\n",
      "Epoch: 1/10... Step: 2160... Loss: 1.7065... Val Loss: 1.6817\n",
      "Epoch: 1/10... Step: 2170... Loss: 1.7778... Val Loss: 1.6806\n",
      "Epoch: 1/10... Step: 2180... Loss: 1.7547... Val Loss: 1.6772\n",
      "Epoch: 1/10... Step: 2190... Loss: 1.7455... Val Loss: 1.6754\n",
      "Epoch: 1/10... Step: 2200... Loss: 1.7328... Val Loss: 1.6773\n",
      "Epoch: 1/10... Step: 2210... Loss: 1.8819... Val Loss: 1.6743\n",
      "Epoch: 1/10... Step: 2220... Loss: 1.8049... Val Loss: 1.6741\n",
      "Epoch: 1/10... Step: 2230... Loss: 1.8470... Val Loss: 1.6749\n",
      "Epoch: 1/10... Step: 2240... Loss: 1.7563... Val Loss: 1.6721\n",
      "Epoch: 1/10... Step: 2250... Loss: 1.9004... Val Loss: 1.6714\n",
      "Epoch: 1/10... Step: 2260... Loss: 1.7539... Val Loss: 1.6698\n",
      "Epoch: 1/10... Step: 2270... Loss: 1.8093... Val Loss: 1.6718\n",
      "Epoch: 1/10... Step: 2280... Loss: 1.8419... Val Loss: 1.6705\n",
      "Epoch: 1/10... Step: 2290... Loss: 1.7358... Val Loss: 1.6663\n",
      "Epoch: 1/10... Step: 2300... Loss: 1.7123... Val Loss: 1.6656\n",
      "Epoch: 1/10... Step: 2310... Loss: 1.8064... Val Loss: 1.6699\n",
      "Epoch: 1/10... Step: 2320... Loss: 1.6783... Val Loss: 1.6611\n",
      "Epoch: 1/10... Step: 2330... Loss: 1.7288... Val Loss: 1.6637\n",
      "Epoch: 1/10... Step: 2340... Loss: 1.7501... Val Loss: 1.6622\n",
      "Epoch: 1/10... Step: 2350... Loss: 1.7944... Val Loss: 1.6641\n",
      "Epoch: 1/10... Step: 2360... Loss: 1.8473... Val Loss: 1.6609\n",
      "Epoch: 1/10... Step: 2370... Loss: 1.6692... Val Loss: 1.6652\n",
      "Epoch: 1/10... Step: 2380... Loss: 1.8978... Val Loss: 1.6630\n",
      "Epoch: 1/10... Step: 2390... Loss: 1.8190... Val Loss: 1.6622\n",
      "Epoch: 1/10... Step: 2400... Loss: 1.7743... Val Loss: 1.6594\n",
      "Epoch: 1/10... Step: 2410... Loss: 1.6751... Val Loss: 1.6584\n",
      "Epoch: 1/10... Step: 2420... Loss: 1.6870... Val Loss: 1.6532\n",
      "Epoch: 1/10... Step: 2430... Loss: 1.8539... Val Loss: 1.6525\n",
      "Epoch: 1/10... Step: 2440... Loss: 1.7434... Val Loss: 1.6483\n",
      "Epoch: 1/10... Step: 2450... Loss: 1.6423... Val Loss: 1.6523\n",
      "Epoch: 1/10... Step: 2460... Loss: 1.7333... Val Loss: 1.6549\n",
      "Epoch: 1/10... Step: 2470... Loss: 1.7164... Val Loss: 1.6543\n",
      "Epoch: 1/10... Step: 2480... Loss: 1.6699... Val Loss: 1.6520\n",
      "Epoch: 1/10... Step: 2490... Loss: 1.6341... Val Loss: 1.6470\n",
      "Epoch: 1/10... Step: 2500... Loss: 1.7724... Val Loss: 1.6462\n",
      "Epoch: 1/10... Step: 2510... Loss: 1.6577... Val Loss: 1.6503\n",
      "Epoch: 1/10... Step: 2520... Loss: 1.7337... Val Loss: 1.6466\n",
      "Epoch: 1/10... Step: 2530... Loss: 1.8580... Val Loss: 1.6400\n",
      "Epoch: 1/10... Step: 2540... Loss: 1.7905... Val Loss: 1.6405\n",
      "Epoch: 1/10... Step: 2550... Loss: 1.7288... Val Loss: 1.6385\n",
      "Epoch: 1/10... Step: 2560... Loss: 1.8126... Val Loss: 1.6425\n",
      "Epoch: 1/10... Step: 2570... Loss: 1.7786... Val Loss: 1.6418\n",
      "Epoch: 1/10... Step: 2580... Loss: 1.6794... Val Loss: 1.6415\n",
      "Epoch: 1/10... Step: 2590... Loss: 1.7134... Val Loss: 1.6398\n",
      "Epoch: 1/10... Step: 2600... Loss: 1.6721... Val Loss: 1.6359\n",
      "Epoch: 1/10... Step: 2610... Loss: 1.6948... Val Loss: 1.6364\n",
      "Epoch: 1/10... Step: 2620... Loss: 1.7518... Val Loss: 1.6392\n",
      "Epoch: 1/10... Step: 2630... Loss: 1.8559... Val Loss: 1.6385\n",
      "Epoch: 1/10... Step: 2640... Loss: 1.7539... Val Loss: 1.6371\n",
      "Epoch: 1/10... Step: 2650... Loss: 1.8581... Val Loss: 1.6320\n",
      "Epoch: 1/10... Step: 2660... Loss: 1.6808... Val Loss: 1.6308\n",
      "Epoch: 1/10... Step: 2670... Loss: 1.7186... Val Loss: 1.6285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 2680... Loss: 1.7232... Val Loss: 1.6309\n",
      "Epoch: 1/10... Step: 2690... Loss: 1.8763... Val Loss: 1.6335\n",
      "Epoch: 1/10... Step: 2700... Loss: 1.6934... Val Loss: 1.6329\n",
      "Epoch: 1/10... Step: 2710... Loss: 1.7913... Val Loss: 1.6320\n",
      "Epoch: 1/10... Step: 2720... Loss: 1.6125... Val Loss: 1.6298\n",
      "Epoch: 1/10... Step: 2730... Loss: 1.6754... Val Loss: 1.6300\n",
      "Epoch: 1/10... Step: 2740... Loss: 1.6196... Val Loss: 1.6261\n",
      "Epoch: 1/10... Step: 2750... Loss: 1.8452... Val Loss: 1.6263\n",
      "Epoch: 1/10... Step: 2760... Loss: 1.7670... Val Loss: 1.6264\n",
      "Epoch: 1/10... Step: 2770... Loss: 1.7199... Val Loss: 1.6238\n",
      "Epoch: 1/10... Step: 2780... Loss: 1.7011... Val Loss: 1.6248\n",
      "Epoch: 1/10... Step: 2790... Loss: 1.7454... Val Loss: 1.6240\n",
      "Epoch: 1/10... Step: 2800... Loss: 1.7019... Val Loss: 1.6244\n",
      "Epoch: 1/10... Step: 2810... Loss: 1.8112... Val Loss: 1.6206\n",
      "Epoch: 1/10... Step: 2820... Loss: 1.7702... Val Loss: 1.6192\n",
      "Epoch: 1/10... Step: 2830... Loss: 1.6636... Val Loss: 1.6168\n",
      "Epoch: 1/10... Step: 2840... Loss: 1.7017... Val Loss: 1.6193\n",
      "Epoch: 1/10... Step: 2850... Loss: 1.6935... Val Loss: 1.6190\n",
      "Epoch: 1/10... Step: 2860... Loss: 1.7392... Val Loss: 1.6154\n",
      "Epoch: 1/10... Step: 2870... Loss: 1.5590... Val Loss: 1.6161\n",
      "Epoch: 1/10... Step: 2880... Loss: 1.7011... Val Loss: 1.6132\n",
      "Epoch: 1/10... Step: 2890... Loss: 1.6046... Val Loss: 1.6167\n",
      "Epoch: 1/10... Step: 2900... Loss: 1.6939... Val Loss: 1.6161\n",
      "Epoch: 1/10... Step: 2910... Loss: 1.7064... Val Loss: 1.6123\n",
      "Epoch: 1/10... Step: 2920... Loss: 1.7282... Val Loss: 1.6122\n",
      "Epoch: 1/10... Step: 2930... Loss: 1.6957... Val Loss: 1.6116\n",
      "Epoch: 1/10... Step: 2940... Loss: 1.6167... Val Loss: 1.6117\n",
      "Epoch: 1/10... Step: 2950... Loss: 1.6902... Val Loss: 1.6164\n",
      "Epoch: 1/10... Step: 2960... Loss: 1.7462... Val Loss: 1.6168\n",
      "Epoch: 1/10... Step: 2970... Loss: 1.7759... Val Loss: 1.6136\n",
      "Epoch: 1/10... Step: 2980... Loss: 1.8198... Val Loss: 1.6139\n",
      "Epoch: 1/10... Step: 2990... Loss: 1.6753... Val Loss: 1.6098\n",
      "Epoch: 1/10... Step: 3000... Loss: 1.7159... Val Loss: 1.6075\n",
      "Epoch: 1/10... Step: 3010... Loss: 1.6587... Val Loss: 1.6083\n",
      "Epoch: 1/10... Step: 3020... Loss: 1.6330... Val Loss: 1.6091\n",
      "Epoch: 1/10... Step: 3030... Loss: 1.6966... Val Loss: 1.6083\n",
      "Epoch: 1/10... Step: 3040... Loss: 1.6436... Val Loss: 1.6074\n",
      "Epoch: 1/10... Step: 3050... Loss: 1.7499... Val Loss: 1.6033\n",
      "Epoch: 1/10... Step: 3060... Loss: 1.6098... Val Loss: 1.6044\n",
      "Epoch: 1/10... Step: 3070... Loss: 1.5663... Val Loss: 1.6062\n",
      "Epoch: 1/10... Step: 3080... Loss: 1.5777... Val Loss: 1.6045\n",
      "Epoch: 1/10... Step: 3090... Loss: 1.7220... Val Loss: 1.5998\n",
      "Epoch: 1/10... Step: 3100... Loss: 1.5846... Val Loss: 1.5993\n",
      "Epoch: 1/10... Step: 3110... Loss: 1.8125... Val Loss: 1.6008\n",
      "Epoch: 1/10... Step: 3120... Loss: 1.8227... Val Loss: 1.6034\n",
      "Epoch: 1/10... Step: 3130... Loss: 1.7239... Val Loss: 1.6073\n",
      "Epoch: 1/10... Step: 3140... Loss: 1.7402... Val Loss: 1.5991\n",
      "Epoch: 1/10... Step: 3150... Loss: 1.6461... Val Loss: 1.6024\n",
      "Epoch: 1/10... Step: 3160... Loss: 1.6021... Val Loss: 1.5998\n",
      "Epoch: 1/10... Step: 3170... Loss: 1.5637... Val Loss: 1.6035\n",
      "Epoch: 1/10... Step: 3180... Loss: 1.6437... Val Loss: 1.6033\n",
      "Epoch: 1/10... Step: 3190... Loss: 1.6065... Val Loss: 1.5980\n",
      "Epoch: 1/10... Step: 3200... Loss: 1.6310... Val Loss: 1.5963\n",
      "Epoch: 1/10... Step: 3210... Loss: 1.6278... Val Loss: 1.5963\n",
      "Epoch: 1/10... Step: 3220... Loss: 1.7214... Val Loss: 1.5982\n",
      "Epoch: 1/10... Step: 3230... Loss: 1.6816... Val Loss: 1.5972\n",
      "Epoch: 1/10... Step: 3240... Loss: 1.7283... Val Loss: 1.5970\n",
      "Epoch: 1/10... Step: 3250... Loss: 1.7299... Val Loss: 1.5966\n",
      "Epoch: 1/10... Step: 3260... Loss: 1.6094... Val Loss: 1.5951\n",
      "Epoch: 1/10... Step: 3270... Loss: 1.6772... Val Loss: 1.5921\n",
      "Epoch: 1/10... Step: 3280... Loss: 1.6847... Val Loss: 1.5897\n",
      "Epoch: 1/10... Step: 3290... Loss: 1.6071... Val Loss: 1.5875\n",
      "Epoch: 1/10... Step: 3300... Loss: 1.7606... Val Loss: 1.5886\n",
      "Epoch: 1/10... Step: 3310... Loss: 1.7044... Val Loss: 1.5889\n",
      "Epoch: 1/10... Step: 3320... Loss: 1.7229... Val Loss: 1.5876\n",
      "Epoch: 1/10... Step: 3330... Loss: 1.7011... Val Loss: 1.5872\n",
      "Epoch: 1/10... Step: 3340... Loss: 1.7075... Val Loss: 1.5868\n",
      "Epoch: 1/10... Step: 3350... Loss: 1.7318... Val Loss: 1.5847\n",
      "Epoch: 1/10... Step: 3360... Loss: 1.5951... Val Loss: 1.5881\n",
      "Epoch: 1/10... Step: 3370... Loss: 1.6638... Val Loss: 1.5842\n",
      "Epoch: 1/10... Step: 3380... Loss: 1.7692... Val Loss: 1.5816\n",
      "Epoch: 2/10... Step: 3390... Loss: 1.7009... Val Loss: 1.5894\n",
      "Epoch: 2/10... Step: 3400... Loss: 1.6936... Val Loss: 1.5853\n",
      "Epoch: 2/10... Step: 3410... Loss: 1.6461... Val Loss: 1.5859\n",
      "Epoch: 2/10... Step: 3420... Loss: 1.6315... Val Loss: 1.5825\n",
      "Epoch: 2/10... Step: 3430... Loss: 1.6492... Val Loss: 1.5826\n",
      "Epoch: 2/10... Step: 3440... Loss: 1.6282... Val Loss: 1.5820\n",
      "Epoch: 2/10... Step: 3450... Loss: 1.6190... Val Loss: 1.5832\n",
      "Epoch: 2/10... Step: 3460... Loss: 1.6788... Val Loss: 1.5790\n",
      "Epoch: 2/10... Step: 3470... Loss: 1.7523... Val Loss: 1.5827\n",
      "Epoch: 2/10... Step: 3480... Loss: 1.6887... Val Loss: 1.5769\n",
      "Epoch: 2/10... Step: 3490... Loss: 1.7569... Val Loss: 1.5806\n",
      "Epoch: 2/10... Step: 3500... Loss: 1.6732... Val Loss: 1.5794\n",
      "Epoch: 2/10... Step: 3510... Loss: 1.7207... Val Loss: 1.5794\n",
      "Epoch: 2/10... Step: 3520... Loss: 1.5957... Val Loss: 1.5779\n",
      "Epoch: 2/10... Step: 3530... Loss: 1.5445... Val Loss: 1.5783\n",
      "Epoch: 2/10... Step: 3540... Loss: 1.6075... Val Loss: 1.5719\n",
      "Epoch: 2/10... Step: 3550... Loss: 1.6912... Val Loss: 1.5726\n",
      "Epoch: 2/10... Step: 3560... Loss: 1.7867... Val Loss: 1.5739\n",
      "Epoch: 2/10... Step: 3570... Loss: 1.6288... Val Loss: 1.5732\n",
      "Epoch: 2/10... Step: 3580... Loss: 1.7290... Val Loss: 1.5801\n",
      "Epoch: 2/10... Step: 3590... Loss: 1.7331... Val Loss: 1.5758\n",
      "Epoch: 2/10... Step: 3600... Loss: 1.6478... Val Loss: 1.5735\n",
      "Epoch: 2/10... Step: 3610... Loss: 1.5038... Val Loss: 1.5718\n",
      "Epoch: 2/10... Step: 3620... Loss: 1.6553... Val Loss: 1.5703\n",
      "Epoch: 2/10... Step: 3630... Loss: 1.5544... Val Loss: 1.5709\n",
      "Epoch: 2/10... Step: 3640... Loss: 1.7022... Val Loss: 1.5698\n",
      "Epoch: 2/10... Step: 3650... Loss: 1.7644... Val Loss: 1.5675\n",
      "Epoch: 2/10... Step: 3660... Loss: 1.6397... Val Loss: 1.5691\n",
      "Epoch: 2/10... Step: 3670... Loss: 1.6190... Val Loss: 1.5678\n",
      "Epoch: 2/10... Step: 3680... Loss: 1.7545... Val Loss: 1.5681\n",
      "Epoch: 2/10... Step: 3690... Loss: 1.7122... Val Loss: 1.5694\n",
      "Epoch: 2/10... Step: 3700... Loss: 1.6452... Val Loss: 1.5681\n",
      "Epoch: 2/10... Step: 3710... Loss: 1.7072... Val Loss: 1.5685\n",
      "Epoch: 2/10... Step: 3720... Loss: 1.6291... Val Loss: 1.5705\n",
      "Epoch: 2/10... Step: 3730... Loss: 1.7454... Val Loss: 1.5691\n",
      "Epoch: 2/10... Step: 3740... Loss: 1.5879... Val Loss: 1.5644\n",
      "Epoch: 2/10... Step: 3750... Loss: 1.6939... Val Loss: 1.5719\n",
      "Epoch: 2/10... Step: 3760... Loss: 1.7270... Val Loss: 1.5659\n",
      "Epoch: 2/10... Step: 3770... Loss: 1.5981... Val Loss: 1.5656\n",
      "Epoch: 2/10... Step: 3780... Loss: 1.5754... Val Loss: 1.5638\n",
      "Epoch: 2/10... Step: 3790... Loss: 1.7654... Val Loss: 1.5637\n",
      "Epoch: 2/10... Step: 3800... Loss: 1.6165... Val Loss: 1.5619\n",
      "Epoch: 2/10... Step: 3810... Loss: 1.6779... Val Loss: 1.5626\n",
      "Epoch: 2/10... Step: 3820... Loss: 1.6824... Val Loss: 1.5603\n",
      "Epoch: 2/10... Step: 3830... Loss: 1.7692... Val Loss: 1.5638\n",
      "Epoch: 2/10... Step: 3840... Loss: 1.5978... Val Loss: 1.5608\n",
      "Epoch: 2/10... Step: 3850... Loss: 1.6368... Val Loss: 1.5585\n",
      "Epoch: 2/10... Step: 3860... Loss: 1.6481... Val Loss: 1.5588\n",
      "Epoch: 2/10... Step: 3870... Loss: 1.6216... Val Loss: 1.5618\n",
      "Epoch: 2/10... Step: 3880... Loss: 1.6613... Val Loss: 1.5604\n",
      "Epoch: 2/10... Step: 3890... Loss: 1.6783... Val Loss: 1.5592\n",
      "Epoch: 2/10... Step: 3900... Loss: 1.5466... Val Loss: 1.5575\n",
      "Epoch: 2/10... Step: 3910... Loss: 1.6099... Val Loss: 1.5579\n",
      "Epoch: 2/10... Step: 3920... Loss: 1.5267... Val Loss: 1.5582\n",
      "Epoch: 2/10... Step: 3930... Loss: 1.6525... Val Loss: 1.5573\n",
      "Epoch: 2/10... Step: 3940... Loss: 1.6620... Val Loss: 1.5576\n",
      "Epoch: 2/10... Step: 3950... Loss: 1.5962... Val Loss: 1.5578\n",
      "Epoch: 2/10... Step: 3960... Loss: 1.6871... Val Loss: 1.5566\n",
      "Epoch: 2/10... Step: 3970... Loss: 1.5317... Val Loss: 1.5550\n",
      "Epoch: 2/10... Step: 3980... Loss: 1.7360... Val Loss: 1.5553\n",
      "Epoch: 2/10... Step: 3990... Loss: 1.7300... Val Loss: 1.5542\n",
      "Epoch: 2/10... Step: 4000... Loss: 1.6461... Val Loss: 1.5551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10... Step: 4010... Loss: 1.7123... Val Loss: 1.5568\n",
      "Epoch: 2/10... Step: 4020... Loss: 1.5681... Val Loss: 1.5537\n",
      "Epoch: 2/10... Step: 4030... Loss: 1.5485... Val Loss: 1.5557\n",
      "Epoch: 2/10... Step: 4040... Loss: 1.6141... Val Loss: 1.5537\n",
      "Epoch: 2/10... Step: 4050... Loss: 1.6236... Val Loss: 1.5488\n",
      "Epoch: 2/10... Step: 4060... Loss: 1.6527... Val Loss: 1.5494\n",
      "Epoch: 2/10... Step: 4070... Loss: 1.5489... Val Loss: 1.5501\n",
      "Epoch: 2/10... Step: 4080... Loss: 1.4768... Val Loss: 1.5502\n",
      "Epoch: 2/10... Step: 4090... Loss: 1.6284... Val Loss: 1.5510\n",
      "Epoch: 2/10... Step: 4100... Loss: 1.6803... Val Loss: 1.5479\n",
      "Epoch: 2/10... Step: 4110... Loss: 1.7180... Val Loss: 1.5485\n",
      "Epoch: 2/10... Step: 4120... Loss: 1.6773... Val Loss: 1.5563\n",
      "Epoch: 2/10... Step: 4130... Loss: 1.6969... Val Loss: 1.5513\n",
      "Epoch: 2/10... Step: 4140... Loss: 1.6974... Val Loss: 1.5481\n",
      "Epoch: 2/10... Step: 4150... Loss: 1.8375... Val Loss: 1.5474\n",
      "Epoch: 2/10... Step: 4160... Loss: 1.8070... Val Loss: 1.5488\n",
      "Epoch: 2/10... Step: 4170... Loss: 1.7644... Val Loss: 1.5476\n",
      "Epoch: 2/10... Step: 4180... Loss: 1.6742... Val Loss: 1.5456\n",
      "Epoch: 2/10... Step: 4190... Loss: 1.6671... Val Loss: 1.5437\n",
      "Epoch: 2/10... Step: 4200... Loss: 1.6247... Val Loss: 1.5422\n",
      "Epoch: 2/10... Step: 4210... Loss: 1.6820... Val Loss: 1.5460\n",
      "Epoch: 2/10... Step: 4220... Loss: 1.6363... Val Loss: 1.5443\n",
      "Epoch: 2/10... Step: 4230... Loss: 1.5964... Val Loss: 1.5404\n",
      "Epoch: 2/10... Step: 4240... Loss: 1.5665... Val Loss: 1.5429\n",
      "Epoch: 2/10... Step: 4250... Loss: 1.5890... Val Loss: 1.5450\n",
      "Epoch: 2/10... Step: 4260... Loss: 1.7098... Val Loss: 1.5417\n",
      "Epoch: 2/10... Step: 4270... Loss: 1.5726... Val Loss: 1.5417\n",
      "Epoch: 2/10... Step: 4280... Loss: 1.5815... Val Loss: 1.5425\n",
      "Epoch: 2/10... Step: 4290... Loss: 1.5677... Val Loss: 1.5437\n",
      "Epoch: 2/10... Step: 4300... Loss: 1.5297... Val Loss: 1.5408\n",
      "Epoch: 2/10... Step: 4310... Loss: 1.6523... Val Loss: 1.5401\n",
      "Epoch: 2/10... Step: 4320... Loss: 1.6576... Val Loss: 1.5469\n",
      "Epoch: 2/10... Step: 4330... Loss: 1.5911... Val Loss: 1.5409\n",
      "Epoch: 2/10... Step: 4340... Loss: 1.6211... Val Loss: 1.5400\n",
      "Epoch: 2/10... Step: 4350... Loss: 1.7535... Val Loss: 1.5407\n",
      "Epoch: 2/10... Step: 4360... Loss: 1.5597... Val Loss: 1.5395\n",
      "Epoch: 2/10... Step: 4370... Loss: 1.5301... Val Loss: 1.5366\n",
      "Epoch: 2/10... Step: 4380... Loss: 1.6361... Val Loss: 1.5344\n",
      "Epoch: 2/10... Step: 4390... Loss: 1.6639... Val Loss: 1.5398\n",
      "Epoch: 2/10... Step: 4400... Loss: 1.5591... Val Loss: 1.5375\n",
      "Epoch: 2/10... Step: 4410... Loss: 1.5374... Val Loss: 1.5386\n",
      "Epoch: 2/10... Step: 4420... Loss: 1.6847... Val Loss: 1.5364\n",
      "Epoch: 2/10... Step: 4430... Loss: 1.6826... Val Loss: 1.5376\n",
      "Epoch: 2/10... Step: 4440... Loss: 1.5881... Val Loss: 1.5363\n",
      "Epoch: 2/10... Step: 4450... Loss: 1.6901... Val Loss: 1.5365\n",
      "Epoch: 2/10... Step: 4460... Loss: 1.5413... Val Loss: 1.5344\n",
      "Epoch: 2/10... Step: 4470... Loss: 1.5789... Val Loss: 1.5323\n",
      "Epoch: 2/10... Step: 4480... Loss: 1.7269... Val Loss: 1.5364\n",
      "Epoch: 2/10... Step: 4490... Loss: 1.6711... Val Loss: 1.5349\n",
      "Epoch: 2/10... Step: 4500... Loss: 1.6126... Val Loss: 1.5394\n",
      "Epoch: 2/10... Step: 4510... Loss: 1.6671... Val Loss: 1.5362\n",
      "Epoch: 2/10... Step: 4520... Loss: 1.7103... Val Loss: 1.5407\n",
      "Epoch: 2/10... Step: 4530... Loss: 1.6219... Val Loss: 1.5337\n",
      "Epoch: 2/10... Step: 4540... Loss: 1.6202... Val Loss: 1.5331\n",
      "Epoch: 2/10... Step: 4550... Loss: 1.5769... Val Loss: 1.5339\n",
      "Epoch: 2/10... Step: 4560... Loss: 1.5857... Val Loss: 1.5332\n",
      "Epoch: 2/10... Step: 4570... Loss: 1.6123... Val Loss: 1.5315\n",
      "Epoch: 2/10... Step: 4580... Loss: 1.6493... Val Loss: 1.5330\n",
      "Epoch: 2/10... Step: 4590... Loss: 1.6111... Val Loss: 1.5314\n",
      "Epoch: 2/10... Step: 4600... Loss: 1.6561... Val Loss: 1.5319\n",
      "Epoch: 2/10... Step: 4610... Loss: 1.6892... Val Loss: 1.5313\n",
      "Epoch: 2/10... Step: 4620... Loss: 1.6637... Val Loss: 1.5282\n",
      "Epoch: 2/10... Step: 4630... Loss: 1.6461... Val Loss: 1.5302\n",
      "Epoch: 2/10... Step: 4640... Loss: 1.7233... Val Loss: 1.5335\n",
      "Epoch: 2/10... Step: 4650... Loss: 1.6163... Val Loss: 1.5298\n",
      "Epoch: 2/10... Step: 4660... Loss: 1.5966... Val Loss: 1.5302\n",
      "Epoch: 2/10... Step: 4670... Loss: 1.5919... Val Loss: 1.5333\n",
      "Epoch: 2/10... Step: 4680... Loss: 1.5903... Val Loss: 1.5280\n",
      "Epoch: 2/10... Step: 4690... Loss: 1.6854... Val Loss: 1.5295\n",
      "Epoch: 2/10... Step: 4700... Loss: 1.6655... Val Loss: 1.5251\n",
      "Epoch: 2/10... Step: 4710... Loss: 1.5730... Val Loss: 1.5250\n",
      "Epoch: 2/10... Step: 4720... Loss: 1.5899... Val Loss: 1.5295\n",
      "Epoch: 2/10... Step: 4730... Loss: 1.7232... Val Loss: 1.5231\n",
      "Epoch: 2/10... Step: 4740... Loss: 1.5933... Val Loss: 1.5241\n",
      "Epoch: 2/10... Step: 4750... Loss: 1.6056... Val Loss: 1.5249\n",
      "Epoch: 2/10... Step: 4760... Loss: 1.6436... Val Loss: 1.5240\n",
      "Epoch: 2/10... Step: 4770... Loss: 1.7755... Val Loss: 1.5246\n",
      "Epoch: 2/10... Step: 4780... Loss: 1.7104... Val Loss: 1.5257\n",
      "Epoch: 2/10... Step: 4790... Loss: 1.6979... Val Loss: 1.5264\n",
      "Epoch: 2/10... Step: 4800... Loss: 1.6685... Val Loss: 1.5291\n",
      "Epoch: 2/10... Step: 4810... Loss: 1.6045... Val Loss: 1.5247\n",
      "Epoch: 2/10... Step: 4820... Loss: 1.5783... Val Loss: 1.5231\n",
      "Epoch: 2/10... Step: 4830... Loss: 1.5043... Val Loss: 1.5239\n",
      "Epoch: 2/10... Step: 4840... Loss: 1.5674... Val Loss: 1.5214\n",
      "Epoch: 2/10... Step: 4850... Loss: 1.5757... Val Loss: 1.5243\n",
      "Epoch: 2/10... Step: 4860... Loss: 1.6277... Val Loss: 1.5219\n",
      "Epoch: 2/10... Step: 4870... Loss: 1.5631... Val Loss: 1.5193\n",
      "Epoch: 2/10... Step: 4880... Loss: 1.5966... Val Loss: 1.5199\n",
      "Epoch: 2/10... Step: 4890... Loss: 1.6747... Val Loss: 1.5190\n",
      "Epoch: 2/10... Step: 4900... Loss: 1.6976... Val Loss: 1.5234\n",
      "Epoch: 2/10... Step: 4910... Loss: 1.5775... Val Loss: 1.5184\n",
      "Epoch: 2/10... Step: 4920... Loss: 1.5636... Val Loss: 1.5168\n",
      "Epoch: 2/10... Step: 4930... Loss: 1.6406... Val Loss: 1.5169\n",
      "Epoch: 2/10... Step: 4940... Loss: 1.5976... Val Loss: 1.5187\n",
      "Epoch: 2/10... Step: 4950... Loss: 1.7422... Val Loss: 1.5204\n",
      "Epoch: 2/10... Step: 4960... Loss: 1.5699... Val Loss: 1.5239\n",
      "Epoch: 2/10... Step: 4970... Loss: 1.6167... Val Loss: 1.5201\n",
      "Epoch: 2/10... Step: 4980... Loss: 1.6114... Val Loss: 1.5180\n",
      "Epoch: 2/10... Step: 4990... Loss: 1.6622... Val Loss: 1.5158\n",
      "Epoch: 2/10... Step: 5000... Loss: 1.5331... Val Loss: 1.5138\n",
      "Epoch: 2/10... Step: 5010... Loss: 1.6597... Val Loss: 1.5136\n",
      "Epoch: 2/10... Step: 5020... Loss: 1.5813... Val Loss: 1.5128\n",
      "Epoch: 2/10... Step: 5030... Loss: 1.5667... Val Loss: 1.5104\n",
      "Epoch: 2/10... Step: 5040... Loss: 1.5397... Val Loss: 1.5163\n",
      "Epoch: 2/10... Step: 5050... Loss: 1.5044... Val Loss: 1.5167\n",
      "Epoch: 2/10... Step: 5060... Loss: 1.6616... Val Loss: 1.5180\n",
      "Epoch: 2/10... Step: 5070... Loss: 1.7375... Val Loss: 1.5135\n",
      "Epoch: 2/10... Step: 5080... Loss: 1.7080... Val Loss: 1.5096\n",
      "Epoch: 2/10... Step: 5090... Loss: 1.5857... Val Loss: 1.5132\n",
      "Epoch: 2/10... Step: 5100... Loss: 1.6645... Val Loss: 1.5096\n",
      "Epoch: 2/10... Step: 5110... Loss: 1.5682... Val Loss: 1.5111\n",
      "Epoch: 2/10... Step: 5120... Loss: 1.6064... Val Loss: 1.5115\n",
      "Epoch: 2/10... Step: 5130... Loss: 1.6492... Val Loss: 1.5108\n",
      "Epoch: 2/10... Step: 5140... Loss: 1.7394... Val Loss: 1.5069\n",
      "Epoch: 2/10... Step: 5150... Loss: 1.6218... Val Loss: 1.5062\n",
      "Epoch: 2/10... Step: 5160... Loss: 1.6229... Val Loss: 1.5080\n",
      "Epoch: 2/10... Step: 5170... Loss: 1.5607... Val Loss: 1.5049\n",
      "Epoch: 2/10... Step: 5180... Loss: 1.5934... Val Loss: 1.5069\n",
      "Epoch: 2/10... Step: 5190... Loss: 1.5698... Val Loss: 1.5067\n",
      "Epoch: 2/10... Step: 5200... Loss: 1.5487... Val Loss: 1.5052\n",
      "Epoch: 2/10... Step: 5210... Loss: 1.6624... Val Loss: 1.5060\n",
      "Epoch: 2/10... Step: 5220... Loss: 1.5612... Val Loss: 1.5037\n",
      "Epoch: 2/10... Step: 5230... Loss: 1.6441... Val Loss: 1.5045\n",
      "Epoch: 2/10... Step: 5240... Loss: 1.5981... Val Loss: 1.5082\n",
      "Epoch: 2/10... Step: 5250... Loss: 1.6057... Val Loss: 1.5054\n",
      "Epoch: 2/10... Step: 5260... Loss: 1.4563... Val Loss: 1.5084\n",
      "Epoch: 2/10... Step: 5270... Loss: 1.5301... Val Loss: 1.5062\n",
      "Epoch: 2/10... Step: 5280... Loss: 1.6423... Val Loss: 1.5059\n",
      "Epoch: 2/10... Step: 5290... Loss: 1.7558... Val Loss: 1.5096\n",
      "Epoch: 2/10... Step: 5300... Loss: 1.6231... Val Loss: 1.5056\n",
      "Epoch: 2/10... Step: 5310... Loss: 1.7362... Val Loss: 1.5041\n",
      "Epoch: 2/10... Step: 5320... Loss: 1.6281... Val Loss: 1.5080\n",
      "Epoch: 2/10... Step: 5330... Loss: 1.5796... Val Loss: 1.5003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10... Step: 5340... Loss: 1.5021... Val Loss: 1.5019\n",
      "Epoch: 2/10... Step: 5350... Loss: 1.6344... Val Loss: 1.5018\n",
      "Epoch: 2/10... Step: 5360... Loss: 1.3942... Val Loss: 1.5024\n",
      "Epoch: 2/10... Step: 5370... Loss: 1.5887... Val Loss: 1.5042\n",
      "Epoch: 2/10... Step: 5380... Loss: 1.5467... Val Loss: 1.5027\n",
      "Epoch: 2/10... Step: 5390... Loss: 1.5983... Val Loss: 1.5008\n",
      "Epoch: 2/10... Step: 5400... Loss: 1.5408... Val Loss: 1.5034\n",
      "Epoch: 2/10... Step: 5410... Loss: 1.6340... Val Loss: 1.5033\n",
      "Epoch: 2/10... Step: 5420... Loss: 1.4741... Val Loss: 1.5026\n",
      "Epoch: 2/10... Step: 5430... Loss: 1.5227... Val Loss: 1.5017\n",
      "Epoch: 2/10... Step: 5440... Loss: 1.5329... Val Loss: 1.5032\n",
      "Epoch: 2/10... Step: 5450... Loss: 1.6273... Val Loss: 1.4983\n",
      "Epoch: 2/10... Step: 5460... Loss: 1.5541... Val Loss: 1.5000\n",
      "Epoch: 2/10... Step: 5470... Loss: 1.4985... Val Loss: 1.4995\n",
      "Epoch: 2/10... Step: 5480... Loss: 1.5526... Val Loss: 1.4988\n",
      "Epoch: 2/10... Step: 5490... Loss: 1.5650... Val Loss: 1.5021\n",
      "Epoch: 2/10... Step: 5500... Loss: 1.6637... Val Loss: 1.4978\n",
      "Epoch: 2/10... Step: 5510... Loss: 1.6295... Val Loss: 1.4990\n",
      "Epoch: 2/10... Step: 5520... Loss: 1.6955... Val Loss: 1.5036\n",
      "Epoch: 2/10... Step: 5530... Loss: 1.6181... Val Loss: 1.4986\n",
      "Epoch: 2/10... Step: 5540... Loss: 1.6372... Val Loss: 1.4970\n",
      "Epoch: 2/10... Step: 5550... Loss: 1.5558... Val Loss: 1.4988\n",
      "Epoch: 2/10... Step: 5560... Loss: 1.5734... Val Loss: 1.5001\n",
      "Epoch: 2/10... Step: 5570... Loss: 1.5941... Val Loss: 1.4982\n",
      "Epoch: 2/10... Step: 5580... Loss: 1.5165... Val Loss: 1.4978\n",
      "Epoch: 2/10... Step: 5590... Loss: 1.5848... Val Loss: 1.4990\n",
      "Epoch: 2/10... Step: 5600... Loss: 1.5557... Val Loss: 1.4964\n",
      "Epoch: 2/10... Step: 5610... Loss: 1.5647... Val Loss: 1.4972\n",
      "Epoch: 2/10... Step: 5620... Loss: 1.5453... Val Loss: 1.5017\n",
      "Epoch: 2/10... Step: 5630... Loss: 1.6024... Val Loss: 1.4978\n",
      "Epoch: 2/10... Step: 5640... Loss: 1.5063... Val Loss: 1.4971\n",
      "Epoch: 2/10... Step: 5650... Loss: 1.6617... Val Loss: 1.4952\n",
      "Epoch: 2/10... Step: 5660... Loss: 1.6442... Val Loss: 1.4953\n",
      "Epoch: 2/10... Step: 5670... Loss: 1.6393... Val Loss: 1.4966\n",
      "Epoch: 2/10... Step: 5680... Loss: 1.5768... Val Loss: 1.4953\n",
      "Epoch: 2/10... Step: 5690... Loss: 1.6305... Val Loss: 1.4965\n",
      "Epoch: 2/10... Step: 5700... Loss: 1.5664... Val Loss: 1.4953\n",
      "Epoch: 2/10... Step: 5710... Loss: 1.5519... Val Loss: 1.4932\n",
      "Epoch: 2/10... Step: 5720... Loss: 1.5851... Val Loss: 1.4929\n",
      "Epoch: 2/10... Step: 5730... Loss: 1.6449... Val Loss: 1.4934\n",
      "Epoch: 2/10... Step: 5740... Loss: 1.7293... Val Loss: 1.4950\n",
      "Epoch: 2/10... Step: 5750... Loss: 1.6194... Val Loss: 1.4969\n",
      "Epoch: 2/10... Step: 5760... Loss: 1.7078... Val Loss: 1.5006\n",
      "Epoch: 2/10... Step: 5770... Loss: 1.6145... Val Loss: 1.4985\n",
      "Epoch: 2/10... Step: 5780... Loss: 1.6629... Val Loss: 1.4968\n",
      "Epoch: 2/10... Step: 5790... Loss: 1.7540... Val Loss: 1.4978\n",
      "Epoch: 2/10... Step: 5800... Loss: 1.5475... Val Loss: 1.4974\n",
      "Epoch: 2/10... Step: 5810... Loss: 1.5826... Val Loss: 1.4936\n",
      "Epoch: 2/10... Step: 5820... Loss: 1.6831... Val Loss: 1.4943\n",
      "Epoch: 2/10... Step: 5830... Loss: 1.6010... Val Loss: 1.4962\n",
      "Epoch: 2/10... Step: 5840... Loss: 1.7055... Val Loss: 1.4943\n",
      "Epoch: 2/10... Step: 5850... Loss: 1.5199... Val Loss: 1.4936\n",
      "Epoch: 2/10... Step: 5860... Loss: 1.5417... Val Loss: 1.4934\n",
      "Epoch: 2/10... Step: 5870... Loss: 1.5077... Val Loss: 1.4924\n",
      "Epoch: 2/10... Step: 5880... Loss: 1.6516... Val Loss: 1.4909\n",
      "Epoch: 2/10... Step: 5890... Loss: 1.4966... Val Loss: 1.4888\n",
      "Epoch: 2/10... Step: 5900... Loss: 1.5172... Val Loss: 1.4912\n",
      "Epoch: 2/10... Step: 5910... Loss: 1.6014... Val Loss: 1.4907\n",
      "Epoch: 2/10... Step: 5920... Loss: 1.6131... Val Loss: 1.4885\n",
      "Epoch: 2/10... Step: 5930... Loss: 1.5427... Val Loss: 1.4880\n",
      "Epoch: 2/10... Step: 5940... Loss: 1.6046... Val Loss: 1.4912\n",
      "Epoch: 2/10... Step: 5950... Loss: 1.6469... Val Loss: 1.4899\n",
      "Epoch: 2/10... Step: 5960... Loss: 1.5829... Val Loss: 1.4873\n",
      "Epoch: 2/10... Step: 5970... Loss: 1.5090... Val Loss: 1.4873\n",
      "Epoch: 2/10... Step: 5980... Loss: 1.6641... Val Loss: 1.4868\n",
      "Epoch: 2/10... Step: 5990... Loss: 1.6086... Val Loss: 1.4876\n",
      "Epoch: 2/10... Step: 6000... Loss: 1.6445... Val Loss: 1.4876\n",
      "Epoch: 2/10... Step: 6010... Loss: 1.5620... Val Loss: 1.4864\n",
      "Epoch: 2/10... Step: 6020... Loss: 1.6905... Val Loss: 1.4863\n",
      "Epoch: 2/10... Step: 6030... Loss: 1.5463... Val Loss: 1.4873\n",
      "Epoch: 2/10... Step: 6040... Loss: 1.5719... Val Loss: 1.4865\n",
      "Epoch: 2/10... Step: 6050... Loss: 1.5841... Val Loss: 1.4871\n",
      "Epoch: 2/10... Step: 6060... Loss: 1.4673... Val Loss: 1.4869\n",
      "Epoch: 2/10... Step: 6070... Loss: 1.5118... Val Loss: 1.4867\n",
      "Epoch: 2/10... Step: 6080... Loss: 1.5674... Val Loss: 1.4870\n",
      "Epoch: 2/10... Step: 6090... Loss: 1.5651... Val Loss: 1.4862\n",
      "Epoch: 2/10... Step: 6100... Loss: 1.6137... Val Loss: 1.4891\n",
      "Epoch: 2/10... Step: 6110... Loss: 1.5885... Val Loss: 1.4857\n",
      "Epoch: 2/10... Step: 6120... Loss: 1.5206... Val Loss: 1.4845\n",
      "Epoch: 2/10... Step: 6130... Loss: 1.4777... Val Loss: 1.4904\n",
      "Epoch: 2/10... Step: 6140... Loss: 1.6882... Val Loss: 1.4897\n",
      "Epoch: 2/10... Step: 6150... Loss: 1.6152... Val Loss: 1.4866\n",
      "Epoch: 2/10... Step: 6160... Loss: 1.5190... Val Loss: 1.4853\n",
      "Epoch: 2/10... Step: 6170... Loss: 1.6270... Val Loss: 1.4954\n",
      "Epoch: 2/10... Step: 6180... Loss: 1.4851... Val Loss: 1.4894\n",
      "Epoch: 2/10... Step: 6190... Loss: 1.5851... Val Loss: 1.4866\n",
      "Epoch: 2/10... Step: 6200... Loss: 1.6317... Val Loss: 1.4865\n",
      "Epoch: 2/10... Step: 6210... Loss: 1.5070... Val Loss: 1.4858\n",
      "Epoch: 2/10... Step: 6220... Loss: 1.6775... Val Loss: 1.4831\n",
      "Epoch: 2/10... Step: 6230... Loss: 1.7077... Val Loss: 1.4855\n",
      "Epoch: 2/10... Step: 6240... Loss: 1.5283... Val Loss: 1.4825\n",
      "Epoch: 2/10... Step: 6250... Loss: 1.5258... Val Loss: 1.4830\n",
      "Epoch: 2/10... Step: 6260... Loss: 1.5990... Val Loss: 1.4810\n",
      "Epoch: 2/10... Step: 6270... Loss: 1.6118... Val Loss: 1.4838\n",
      "Epoch: 2/10... Step: 6280... Loss: 1.5534... Val Loss: 1.4800\n",
      "Epoch: 2/10... Step: 6290... Loss: 1.5634... Val Loss: 1.4817\n",
      "Epoch: 2/10... Step: 6300... Loss: 1.5897... Val Loss: 1.4789\n",
      "Epoch: 2/10... Step: 6310... Loss: 1.6593... Val Loss: 1.4823\n",
      "Epoch: 2/10... Step: 6320... Loss: 1.6098... Val Loss: 1.4846\n",
      "Epoch: 2/10... Step: 6330... Loss: 1.6719... Val Loss: 1.4832\n",
      "Epoch: 2/10... Step: 6340... Loss: 1.6508... Val Loss: 1.4847\n",
      "Epoch: 2/10... Step: 6350... Loss: 1.5060... Val Loss: 1.4834\n",
      "Epoch: 2/10... Step: 6360... Loss: 1.5153... Val Loss: 1.4836\n",
      "Epoch: 2/10... Step: 6370... Loss: 1.4830... Val Loss: 1.4810\n",
      "Epoch: 2/10... Step: 6380... Loss: 1.5257... Val Loss: 1.4802\n",
      "Epoch: 2/10... Step: 6390... Loss: 1.5984... Val Loss: 1.4807\n",
      "Epoch: 2/10... Step: 6400... Loss: 1.5512... Val Loss: 1.4799\n",
      "Epoch: 2/10... Step: 6410... Loss: 1.4459... Val Loss: 1.4825\n",
      "Epoch: 2/10... Step: 6420... Loss: 1.5638... Val Loss: 1.4790\n",
      "Epoch: 2/10... Step: 6430... Loss: 1.6063... Val Loss: 1.4782\n",
      "Epoch: 2/10... Step: 6440... Loss: 1.4650... Val Loss: 1.4850\n",
      "Epoch: 2/10... Step: 6450... Loss: 1.5312... Val Loss: 1.4829\n",
      "Epoch: 2/10... Step: 6460... Loss: 1.6022... Val Loss: 1.4807\n",
      "Epoch: 2/10... Step: 6470... Loss: 1.6099... Val Loss: 1.4817\n",
      "Epoch: 2/10... Step: 6480... Loss: 1.4796... Val Loss: 1.4799\n",
      "Epoch: 2/10... Step: 6490... Loss: 1.5316... Val Loss: 1.4794\n",
      "Epoch: 2/10... Step: 6500... Loss: 1.6216... Val Loss: 1.4813\n",
      "Epoch: 2/10... Step: 6510... Loss: 1.5441... Val Loss: 1.4798\n",
      "Epoch: 2/10... Step: 6520... Loss: 1.5886... Val Loss: 1.4802\n",
      "Epoch: 2/10... Step: 6530... Loss: 1.7761... Val Loss: 1.4776\n",
      "Epoch: 2/10... Step: 6540... Loss: 1.5067... Val Loss: 1.4781\n",
      "Epoch: 2/10... Step: 6550... Loss: 1.4858... Val Loss: 1.4790\n",
      "Epoch: 2/10... Step: 6560... Loss: 1.5852... Val Loss: 1.4784\n",
      "Epoch: 2/10... Step: 6570... Loss: 1.6319... Val Loss: 1.4769\n",
      "Epoch: 2/10... Step: 6580... Loss: 1.5030... Val Loss: 1.4744\n",
      "Epoch: 2/10... Step: 6590... Loss: 1.5584... Val Loss: 1.4768\n",
      "Epoch: 2/10... Step: 6600... Loss: 1.5598... Val Loss: 1.4791\n",
      "Epoch: 2/10... Step: 6610... Loss: 1.5670... Val Loss: 1.4782\n",
      "Epoch: 2/10... Step: 6620... Loss: 1.5967... Val Loss: 1.4778\n",
      "Epoch: 2/10... Step: 6630... Loss: 1.5447... Val Loss: 1.4775\n",
      "Epoch: 2/10... Step: 6640... Loss: 1.5804... Val Loss: 1.4788\n",
      "Epoch: 2/10... Step: 6650... Loss: 1.5329... Val Loss: 1.4765\n",
      "Epoch: 2/10... Step: 6660... Loss: 1.5568... Val Loss: 1.4755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10... Step: 6670... Loss: 1.6054... Val Loss: 1.4731\n",
      "Epoch: 2/10... Step: 6680... Loss: 1.6301... Val Loss: 1.4723\n",
      "Epoch: 2/10... Step: 6690... Loss: 1.4860... Val Loss: 1.4756\n",
      "Epoch: 2/10... Step: 6700... Loss: 1.4978... Val Loss: 1.4750\n",
      "Epoch: 2/10... Step: 6710... Loss: 1.5789... Val Loss: 1.4748\n",
      "Epoch: 2/10... Step: 6720... Loss: 1.5178... Val Loss: 1.4728\n",
      "Epoch: 2/10... Step: 6730... Loss: 1.5433... Val Loss: 1.4715\n",
      "Epoch: 2/10... Step: 6740... Loss: 1.5261... Val Loss: 1.4720\n",
      "Epoch: 2/10... Step: 6750... Loss: 1.5535... Val Loss: 1.4708\n",
      "Epoch: 2/10... Step: 6760... Loss: 1.5700... Val Loss: 1.4700\n",
      "Epoch: 3/10... Step: 6770... Loss: 1.5131... Val Loss: 1.4793\n",
      "Epoch: 3/10... Step: 6780... Loss: 1.6649... Val Loss: 1.4720\n",
      "Epoch: 3/10... Step: 6790... Loss: 1.5302... Val Loss: 1.4690\n",
      "Epoch: 3/10... Step: 6800... Loss: 1.4902... Val Loss: 1.4789\n",
      "Epoch: 3/10... Step: 6810... Loss: 1.5998... Val Loss: 1.4697\n",
      "Epoch: 3/10... Step: 6820... Loss: 1.4534... Val Loss: 1.4694\n",
      "Epoch: 3/10... Step: 6830... Loss: 1.5092... Val Loss: 1.4756\n",
      "Epoch: 3/10... Step: 6840... Loss: 1.5916... Val Loss: 1.4726\n",
      "Epoch: 3/10... Step: 6850... Loss: 1.5193... Val Loss: 1.4711\n",
      "Epoch: 3/10... Step: 6860... Loss: 1.5024... Val Loss: 1.4748\n",
      "Epoch: 3/10... Step: 6870... Loss: 1.5129... Val Loss: 1.4683\n",
      "Epoch: 3/10... Step: 6880... Loss: 1.4725... Val Loss: 1.4701\n",
      "Epoch: 3/10... Step: 6890... Loss: 1.5962... Val Loss: 1.4771\n",
      "Epoch: 3/10... Step: 6900... Loss: 1.4403... Val Loss: 1.4749\n",
      "Epoch: 3/10... Step: 6910... Loss: 1.5263... Val Loss: 1.4702\n",
      "Epoch: 3/10... Step: 6920... Loss: 1.5696... Val Loss: 1.4696\n",
      "Epoch: 3/10... Step: 6930... Loss: 1.6307... Val Loss: 1.4686\n",
      "Epoch: 3/10... Step: 6940... Loss: 1.6081... Val Loss: 1.4679\n",
      "Epoch: 3/10... Step: 6950... Loss: 1.6202... Val Loss: 1.4677\n",
      "Epoch: 3/10... Step: 6960... Loss: 1.4840... Val Loss: 1.4730\n",
      "Epoch: 3/10... Step: 6970... Loss: 1.6462... Val Loss: 1.4716\n",
      "Epoch: 3/10... Step: 6980... Loss: 1.6212... Val Loss: 1.4712\n",
      "Epoch: 3/10... Step: 6990... Loss: 1.4548... Val Loss: 1.4673\n",
      "Epoch: 3/10... Step: 7000... Loss: 1.5320... Val Loss: 1.4658\n",
      "Epoch: 3/10... Step: 7010... Loss: 1.5612... Val Loss: 1.4672\n",
      "Epoch: 3/10... Step: 7020... Loss: 1.4886... Val Loss: 1.4671\n",
      "Epoch: 3/10... Step: 7030... Loss: 1.4886... Val Loss: 1.4656\n",
      "Epoch: 3/10... Step: 7040... Loss: 1.5360... Val Loss: 1.4651\n",
      "Epoch: 3/10... Step: 7050... Loss: 1.5920... Val Loss: 1.4682\n",
      "Epoch: 3/10... Step: 7060... Loss: 1.6063... Val Loss: 1.4665\n",
      "Epoch: 3/10... Step: 7070... Loss: 1.5486... Val Loss: 1.4658\n",
      "Epoch: 3/10... Step: 7080... Loss: 1.6165... Val Loss: 1.4646\n",
      "Epoch: 3/10... Step: 7090... Loss: 1.6664... Val Loss: 1.4641\n",
      "Epoch: 3/10... Step: 7100... Loss: 1.5488... Val Loss: 1.4657\n",
      "Epoch: 3/10... Step: 7110... Loss: 1.6077... Val Loss: 1.4659\n",
      "Epoch: 3/10... Step: 7120... Loss: 1.6200... Val Loss: 1.4676\n",
      "Epoch: 3/10... Step: 7130... Loss: 1.5746... Val Loss: 1.4658\n",
      "Epoch: 3/10... Step: 7140... Loss: 1.6458... Val Loss: 1.4650\n",
      "Epoch: 3/10... Step: 7150... Loss: 1.4476... Val Loss: 1.4668\n",
      "Epoch: 3/10... Step: 7160... Loss: 1.6441... Val Loss: 1.4654\n",
      "Epoch: 3/10... Step: 7170... Loss: 1.6180... Val Loss: 1.4645\n",
      "Epoch: 3/10... Step: 7180... Loss: 1.5501... Val Loss: 1.4628\n",
      "Epoch: 3/10... Step: 7190... Loss: 1.5052... Val Loss: 1.4622\n",
      "Epoch: 3/10... Step: 7200... Loss: 1.5453... Val Loss: 1.4636\n",
      "Epoch: 3/10... Step: 7210... Loss: 1.6341... Val Loss: 1.4650\n",
      "Epoch: 3/10... Step: 7220... Loss: 1.5947... Val Loss: 1.4650\n",
      "Epoch: 3/10... Step: 7230... Loss: 1.5974... Val Loss: 1.4637\n",
      "Epoch: 3/10... Step: 7240... Loss: 1.5273... Val Loss: 1.4619\n",
      "Epoch: 3/10... Step: 7250... Loss: 1.4079... Val Loss: 1.4628\n",
      "Epoch: 3/10... Step: 7260... Loss: 1.5523... Val Loss: 1.4635\n",
      "Epoch: 3/10... Step: 7270... Loss: 1.4960... Val Loss: 1.4628\n",
      "Epoch: 3/10... Step: 7280... Loss: 1.5904... Val Loss: 1.4638\n",
      "Epoch: 3/10... Step: 7290... Loss: 1.5082... Val Loss: 1.4604\n",
      "Epoch: 3/10... Step: 7300... Loss: 1.4861... Val Loss: 1.4645\n",
      "Epoch: 3/10... Step: 7310... Loss: 1.4866... Val Loss: 1.4616\n",
      "Epoch: 3/10... Step: 7320... Loss: 1.5136... Val Loss: 1.4651\n",
      "Epoch: 3/10... Step: 7330... Loss: 1.6440... Val Loss: 1.4659\n",
      "Epoch: 3/10... Step: 7340... Loss: 1.6624... Val Loss: 1.4631\n",
      "Epoch: 3/10... Step: 7350... Loss: 1.6137... Val Loss: 1.4607\n",
      "Epoch: 3/10... Step: 7360... Loss: 1.4624... Val Loss: 1.4598\n",
      "Epoch: 3/10... Step: 7370... Loss: 1.4812... Val Loss: 1.4584\n",
      "Epoch: 3/10... Step: 7380... Loss: 1.5172... Val Loss: 1.4619\n",
      "Epoch: 3/10... Step: 7390... Loss: 1.5511... Val Loss: 1.4644\n",
      "Epoch: 3/10... Step: 7400... Loss: 1.4848... Val Loss: 1.4623\n",
      "Epoch: 3/10... Step: 7410... Loss: 1.4548... Val Loss: 1.4621\n",
      "Epoch: 3/10... Step: 7420... Loss: 1.5032... Val Loss: 1.4595\n",
      "Epoch: 3/10... Step: 7430... Loss: 1.4914... Val Loss: 1.4568\n",
      "Epoch: 3/10... Step: 7440... Loss: 1.5194... Val Loss: 1.4623\n",
      "Epoch: 3/10... Step: 7450... Loss: 1.4915... Val Loss: 1.4575\n",
      "Epoch: 3/10... Step: 7460... Loss: 1.5737... Val Loss: 1.4583\n",
      "Epoch: 3/10... Step: 7470... Loss: 1.5769... Val Loss: 1.4603\n",
      "Epoch: 3/10... Step: 7480... Loss: 1.4953... Val Loss: 1.4587\n",
      "Epoch: 3/10... Step: 7490... Loss: 1.5506... Val Loss: 1.4588\n",
      "Epoch: 3/10... Step: 7500... Loss: 1.5961... Val Loss: 1.4609\n",
      "Epoch: 3/10... Step: 7510... Loss: 1.5942... Val Loss: 1.4595\n",
      "Epoch: 3/10... Step: 7520... Loss: 1.5326... Val Loss: 1.4599\n",
      "Epoch: 3/10... Step: 7530... Loss: 1.6036... Val Loss: 1.4585\n",
      "Epoch: 3/10... Step: 7540... Loss: 1.5573... Val Loss: 1.4570\n",
      "Epoch: 3/10... Step: 7550... Loss: 1.5457... Val Loss: 1.4580\n",
      "Epoch: 3/10... Step: 7560... Loss: 1.5677... Val Loss: 1.4560\n",
      "Epoch: 3/10... Step: 7570... Loss: 1.5210... Val Loss: 1.4558\n",
      "Epoch: 3/10... Step: 7580... Loss: 1.4608... Val Loss: 1.4537\n",
      "Epoch: 3/10... Step: 7590... Loss: 1.4455... Val Loss: 1.4567\n",
      "Epoch: 3/10... Step: 7600... Loss: 1.4863... Val Loss: 1.4574\n",
      "Epoch: 3/10... Step: 7610... Loss: 1.5941... Val Loss: 1.4560\n",
      "Epoch: 3/10... Step: 7620... Loss: 1.5274... Val Loss: 1.4557\n",
      "Epoch: 3/10... Step: 7630... Loss: 1.5782... Val Loss: 1.4541\n",
      "Epoch: 3/10... Step: 7640... Loss: 1.4484... Val Loss: 1.4551\n",
      "Epoch: 3/10... Step: 7650... Loss: 1.6025... Val Loss: 1.4559\n",
      "Epoch: 3/10... Step: 7660... Loss: 1.6105... Val Loss: 1.4580\n",
      "Epoch: 3/10... Step: 7670... Loss: 1.6554... Val Loss: 1.4583\n",
      "Epoch: 3/10... Step: 7680... Loss: 1.5401... Val Loss: 1.4566\n",
      "Epoch: 3/10... Step: 7690... Loss: 1.4633... Val Loss: 1.4540\n",
      "Epoch: 3/10... Step: 7700... Loss: 1.5685... Val Loss: 1.4611\n",
      "Epoch: 3/10... Step: 7710... Loss: 1.6207... Val Loss: 1.4569\n",
      "Epoch: 3/10... Step: 7720... Loss: 1.5223... Val Loss: 1.4554\n",
      "Epoch: 3/10... Step: 7730... Loss: 1.6009... Val Loss: 1.4562\n",
      "Epoch: 3/10... Step: 7740... Loss: 1.4560... Val Loss: 1.4558\n",
      "Epoch: 3/10... Step: 7750... Loss: 1.5346... Val Loss: 1.4565\n",
      "Epoch: 3/10... Step: 7760... Loss: 1.5057... Val Loss: 1.4550\n",
      "Epoch: 3/10... Step: 7770... Loss: 1.5819... Val Loss: 1.4549\n",
      "Epoch: 3/10... Step: 7780... Loss: 1.5725... Val Loss: 1.4522\n",
      "Epoch: 3/10... Step: 7790... Loss: 1.5541... Val Loss: 1.4508\n",
      "Epoch: 3/10... Step: 7800... Loss: 1.6681... Val Loss: 1.4514\n",
      "Epoch: 3/10... Step: 7810... Loss: 1.5193... Val Loss: 1.4508\n",
      "Epoch: 3/10... Step: 7820... Loss: 1.6155... Val Loss: 1.4494\n",
      "Epoch: 3/10... Step: 7830... Loss: 1.4320... Val Loss: 1.4551\n",
      "Epoch: 3/10... Step: 7840... Loss: 1.4668... Val Loss: 1.4518\n",
      "Epoch: 3/10... Step: 7850... Loss: 1.5233... Val Loss: 1.4507\n",
      "Epoch: 3/10... Step: 7860... Loss: 1.6399... Val Loss: 1.4528\n",
      "Epoch: 3/10... Step: 7870... Loss: 1.5888... Val Loss: 1.4537\n",
      "Epoch: 3/10... Step: 7880... Loss: 1.5468... Val Loss: 1.4542\n",
      "Epoch: 3/10... Step: 7890... Loss: 1.6181... Val Loss: 1.4499\n",
      "Epoch: 3/10... Step: 7900... Loss: 1.5219... Val Loss: 1.4480\n",
      "Epoch: 3/10... Step: 7910... Loss: 1.5126... Val Loss: 1.4581\n",
      "Epoch: 3/10... Step: 7920... Loss: 1.5910... Val Loss: 1.4512\n",
      "Epoch: 3/10... Step: 7930... Loss: 1.5139... Val Loss: 1.4501\n",
      "Epoch: 3/10... Step: 7940... Loss: 1.5199... Val Loss: 1.4502\n",
      "Epoch: 3/10... Step: 7950... Loss: 1.5411... Val Loss: 1.4502\n",
      "Epoch: 3/10... Step: 7960... Loss: 1.5635... Val Loss: 1.4523\n",
      "Epoch: 3/10... Step: 7970... Loss: 1.6561... Val Loss: 1.4513\n",
      "Epoch: 3/10... Step: 7980... Loss: 1.5256... Val Loss: 1.4523\n",
      "Epoch: 3/10... Step: 7990... Loss: 1.5006... Val Loss: 1.4533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10... Step: 8000... Loss: 1.5143... Val Loss: 1.4487\n",
      "Epoch: 3/10... Step: 8010... Loss: 1.5051... Val Loss: 1.4500\n",
      "Epoch: 3/10... Step: 8020... Loss: 1.5025... Val Loss: 1.4507\n",
      "Epoch: 3/10... Step: 8030... Loss: 1.5250... Val Loss: 1.4514\n",
      "Epoch: 3/10... Step: 8040... Loss: 1.4767... Val Loss: 1.4561\n",
      "Epoch: 3/10... Step: 8050... Loss: 1.5323... Val Loss: 1.4504\n",
      "Epoch: 3/10... Step: 8060... Loss: 1.6297... Val Loss: 1.4495\n",
      "Epoch: 3/10... Step: 8070... Loss: 1.5439... Val Loss: 1.4491\n",
      "Epoch: 3/10... Step: 8080... Loss: 1.4663... Val Loss: 1.4480\n",
      "Epoch: 3/10... Step: 8090... Loss: 1.6308... Val Loss: 1.4461\n",
      "Epoch: 3/10... Step: 8100... Loss: 1.5293... Val Loss: 1.4473\n",
      "Epoch: 3/10... Step: 8110... Loss: 1.4182... Val Loss: 1.4453\n",
      "Epoch: 3/10... Step: 8120... Loss: 1.5410... Val Loss: 1.4474\n",
      "Epoch: 3/10... Step: 8130... Loss: 1.4488... Val Loss: 1.4471\n",
      "Epoch: 3/10... Step: 8140... Loss: 1.4923... Val Loss: 1.4483\n",
      "Epoch: 3/10... Step: 8150... Loss: 1.5996... Val Loss: 1.4480\n",
      "Epoch: 3/10... Step: 8160... Loss: 1.5154... Val Loss: 1.4459\n",
      "Epoch: 3/10... Step: 8170... Loss: 1.4765... Val Loss: 1.4520\n",
      "Epoch: 3/10... Step: 8180... Loss: 1.5067... Val Loss: 1.4515\n",
      "Epoch: 3/10... Step: 8190... Loss: 1.6172... Val Loss: 1.4491\n",
      "Epoch: 3/10... Step: 8200... Loss: 1.5106... Val Loss: 1.4515\n",
      "Epoch: 3/10... Step: 8210... Loss: 1.4917... Val Loss: 1.4488\n",
      "Epoch: 3/10... Step: 8220... Loss: 1.5215... Val Loss: 1.4473\n",
      "Epoch: 3/10... Step: 8230... Loss: 1.5848... Val Loss: 1.4515\n",
      "Epoch: 3/10... Step: 8240... Loss: 1.6018... Val Loss: 1.4476\n",
      "Epoch: 3/10... Step: 8250... Loss: 1.5447... Val Loss: 1.4464\n",
      "Epoch: 3/10... Step: 8260... Loss: 1.5308... Val Loss: 1.4528\n",
      "Epoch: 3/10... Step: 8270... Loss: 1.5511... Val Loss: 1.4497\n",
      "Epoch: 3/10... Step: 8280... Loss: 1.5109... Val Loss: 1.4512\n",
      "Epoch: 3/10... Step: 8290... Loss: 1.4558... Val Loss: 1.4486\n",
      "Epoch: 3/10... Step: 8300... Loss: 1.4738... Val Loss: 1.4482\n",
      "Epoch: 3/10... Step: 8310... Loss: 1.5453... Val Loss: 1.4485\n",
      "Epoch: 3/10... Step: 8320... Loss: 1.4987... Val Loss: 1.4477\n",
      "Epoch: 3/10... Step: 8330... Loss: 1.4420... Val Loss: 1.4482\n",
      "Epoch: 3/10... Step: 8340... Loss: 1.5095... Val Loss: 1.4473\n",
      "Epoch: 3/10... Step: 8350... Loss: 1.6676... Val Loss: 1.4458\n",
      "Epoch: 3/10... Step: 8360... Loss: 1.5557... Val Loss: 1.4450\n",
      "Epoch: 3/10... Step: 8370... Loss: 1.6263... Val Loss: 1.4442\n",
      "Epoch: 3/10... Step: 8380... Loss: 1.5100... Val Loss: 1.4426\n",
      "Epoch: 3/10... Step: 8390... Loss: 1.5424... Val Loss: 1.4456\n",
      "Epoch: 3/10... Step: 8400... Loss: 1.4937... Val Loss: 1.4444\n",
      "Epoch: 3/10... Step: 8410... Loss: 1.5256... Val Loss: 1.4422\n",
      "Epoch: 3/10... Step: 8420... Loss: 1.5184... Val Loss: 1.4444\n",
      "Epoch: 3/10... Step: 8430... Loss: 1.4529... Val Loss: 1.4455\n",
      "Epoch: 3/10... Step: 8440... Loss: 1.5686... Val Loss: 1.4506\n",
      "Epoch: 3/10... Step: 8450... Loss: 1.6249... Val Loss: 1.4423\n",
      "Epoch: 3/10... Step: 8460... Loss: 1.5468... Val Loss: 1.4412\n",
      "Epoch: 3/10... Step: 8470... Loss: 1.4875... Val Loss: 1.4402\n",
      "Epoch: 3/10... Step: 8480... Loss: 1.5122... Val Loss: 1.4403\n",
      "Epoch: 3/10... Step: 8490... Loss: 1.4613... Val Loss: 1.4398\n",
      "Epoch: 3/10... Step: 8500... Loss: 1.5007... Val Loss: 1.4425\n",
      "Epoch: 3/10... Step: 8510... Loss: 1.6400... Val Loss: 1.4406\n",
      "Epoch: 3/10... Step: 8520... Loss: 1.5266... Val Loss: 1.4388\n",
      "Epoch: 3/10... Step: 8530... Loss: 1.5428... Val Loss: 1.4371\n",
      "Epoch: 3/10... Step: 8540... Loss: 1.4667... Val Loss: 1.4363\n",
      "Epoch: 3/10... Step: 8550... Loss: 1.5435... Val Loss: 1.4378\n",
      "Epoch: 3/10... Step: 8560... Loss: 1.5382... Val Loss: 1.4396\n",
      "Epoch: 3/10... Step: 8570... Loss: 1.4681... Val Loss: 1.4375\n",
      "Epoch: 3/10... Step: 8580... Loss: 1.5939... Val Loss: 1.4358\n",
      "Epoch: 3/10... Step: 8590... Loss: 1.5643... Val Loss: 1.4365\n",
      "Epoch: 3/10... Step: 8600... Loss: 1.5523... Val Loss: 1.4360\n",
      "Epoch: 3/10... Step: 8610... Loss: 1.4535... Val Loss: 1.4368\n",
      "Epoch: 3/10... Step: 8620... Loss: 1.5013... Val Loss: 1.4380\n",
      "Epoch: 3/10... Step: 8630... Loss: 1.4797... Val Loss: 1.4382\n",
      "Epoch: 3/10... Step: 8640... Loss: 1.4052... Val Loss: 1.4401\n",
      "Epoch: 3/10... Step: 8650... Loss: 1.4473... Val Loss: 1.4411\n",
      "Epoch: 3/10... Step: 8660... Loss: 1.5638... Val Loss: 1.4399\n",
      "Epoch: 3/10... Step: 8670... Loss: 1.6637... Val Loss: 1.4399\n",
      "Epoch: 3/10... Step: 8680... Loss: 1.4947... Val Loss: 1.4407\n",
      "Epoch: 3/10... Step: 8690... Loss: 1.5539... Val Loss: 1.4394\n",
      "Epoch: 3/10... Step: 8700... Loss: 1.4767... Val Loss: 1.4399\n",
      "Epoch: 3/10... Step: 8710... Loss: 1.5315... Val Loss: 1.4328\n",
      "Epoch: 3/10... Step: 8720... Loss: 1.3892... Val Loss: 1.4337\n",
      "Epoch: 3/10... Step: 8730... Loss: 1.5655... Val Loss: 1.4376\n",
      "Epoch: 3/10... Step: 8740... Loss: 1.5510... Val Loss: 1.4353\n",
      "Epoch: 3/10... Step: 8750... Loss: 1.5242... Val Loss: 1.4360\n",
      "Epoch: 3/10... Step: 8760... Loss: 1.6363... Val Loss: 1.4357\n",
      "Epoch: 3/10... Step: 8770... Loss: 1.4974... Val Loss: 1.4356\n",
      "Epoch: 3/10... Step: 8780... Loss: 1.4970... Val Loss: 1.4400\n",
      "Epoch: 3/10... Step: 8790... Loss: 1.4837... Val Loss: 1.4366\n",
      "Epoch: 3/10... Step: 8800... Loss: 1.5481... Val Loss: 1.4357\n",
      "Epoch: 3/10... Step: 8810... Loss: 1.6059... Val Loss: 1.4354\n",
      "Epoch: 3/10... Step: 8820... Loss: 1.4963... Val Loss: 1.4372\n",
      "Epoch: 3/10... Step: 8830... Loss: 1.5067... Val Loss: 1.4373\n",
      "Epoch: 3/10... Step: 8840... Loss: 1.6051... Val Loss: 1.4324\n",
      "Epoch: 3/10... Step: 8850... Loss: 1.5630... Val Loss: 1.4338\n",
      "Epoch: 3/10... Step: 8860... Loss: 1.5156... Val Loss: 1.4350\n",
      "Epoch: 3/10... Step: 8870... Loss: 1.4947... Val Loss: 1.4365\n",
      "Epoch: 3/10... Step: 8880... Loss: 1.4959... Val Loss: 1.4328\n",
      "Epoch: 3/10... Step: 8890... Loss: 1.6104... Val Loss: 1.4346\n",
      "Epoch: 3/10... Step: 8900... Loss: 1.6303... Val Loss: 1.4375\n",
      "Epoch: 3/10... Step: 8910... Loss: 1.6729... Val Loss: 1.4328\n",
      "Epoch: 3/10... Step: 8920... Loss: 1.6271... Val Loss: 1.4348\n",
      "Epoch: 3/10... Step: 8930... Loss: 1.4855... Val Loss: 1.4339\n",
      "Epoch: 3/10... Step: 8940... Loss: 1.5667... Val Loss: 1.4347\n",
      "Epoch: 3/10... Step: 8950... Loss: 1.4860... Val Loss: 1.4352\n",
      "Epoch: 3/10... Step: 8960... Loss: 1.5454... Val Loss: 1.4310\n",
      "Epoch: 3/10... Step: 8970... Loss: 1.5707... Val Loss: 1.4336\n",
      "Epoch: 3/10... Step: 8980... Loss: 1.4646... Val Loss: 1.4302\n",
      "Epoch: 3/10... Step: 8990... Loss: 1.6052... Val Loss: 1.4304\n",
      "Epoch: 3/10... Step: 9000... Loss: 1.4508... Val Loss: 1.4342\n",
      "Epoch: 3/10... Step: 9010... Loss: 1.6217... Val Loss: 1.4315\n",
      "Epoch: 3/10... Step: 9020... Loss: 1.5326... Val Loss: 1.4350\n",
      "Epoch: 3/10... Step: 9030... Loss: 1.6140... Val Loss: 1.4314\n",
      "Epoch: 3/10... Step: 9040... Loss: 1.3795... Val Loss: 1.4326\n",
      "Epoch: 3/10... Step: 9050... Loss: 1.5054... Val Loss: 1.4328\n",
      "Epoch: 3/10... Step: 9060... Loss: 1.5355... Val Loss: 1.4318\n",
      "Epoch: 3/10... Step: 9070... Loss: 1.5985... Val Loss: 1.4324\n",
      "Epoch: 3/10... Step: 9080... Loss: 1.4559... Val Loss: 1.4324\n",
      "Epoch: 3/10... Step: 9090... Loss: 1.5275... Val Loss: 1.4288\n",
      "Epoch: 3/10... Step: 9100... Loss: 1.6262... Val Loss: 1.4316\n",
      "Epoch: 3/10... Step: 9110... Loss: 1.4525... Val Loss: 1.4296\n",
      "Epoch: 3/10... Step: 9120... Loss: 1.7141... Val Loss: 1.4302\n",
      "Epoch: 3/10... Step: 9130... Loss: 1.6113... Val Loss: 1.4349\n",
      "Epoch: 3/10... Step: 9140... Loss: 1.6402... Val Loss: 1.4351\n",
      "Epoch: 3/10... Step: 9150... Loss: 1.5782... Val Loss: 1.4336\n",
      "Epoch: 3/10... Step: 9160... Loss: 1.5516... Val Loss: 1.4340\n",
      "Epoch: 3/10... Step: 9170... Loss: 1.5090... Val Loss: 1.4333\n",
      "Epoch: 3/10... Step: 9180... Loss: 1.4937... Val Loss: 1.4311\n",
      "Epoch: 3/10... Step: 9190... Loss: 1.5811... Val Loss: 1.4283\n",
      "Epoch: 3/10... Step: 9200... Loss: 1.5545... Val Loss: 1.4293\n",
      "Epoch: 3/10... Step: 9210... Loss: 1.5675... Val Loss: 1.4279\n",
      "Epoch: 3/10... Step: 9220... Loss: 1.6068... Val Loss: 1.4294\n",
      "Epoch: 3/10... Step: 9230... Loss: 1.5395... Val Loss: 1.4298\n",
      "Epoch: 3/10... Step: 9240... Loss: 1.6520... Val Loss: 1.4332\n",
      "Epoch: 3/10... Step: 9250... Loss: 1.5268... Val Loss: 1.4300\n",
      "Epoch: 3/10... Step: 9260... Loss: 1.4837... Val Loss: 1.4299\n",
      "Epoch: 3/10... Step: 9270... Loss: 1.4655... Val Loss: 1.4290\n",
      "Epoch: 3/10... Step: 9280... Loss: 1.4150... Val Loss: 1.4308\n",
      "Epoch: 3/10... Step: 9290... Loss: 1.5018... Val Loss: 1.4319\n",
      "Epoch: 3/10... Step: 9300... Loss: 1.5250... Val Loss: 1.4274\n",
      "Epoch: 3/10... Step: 9310... Loss: 1.4706... Val Loss: 1.4259\n",
      "Epoch: 3/10... Step: 9320... Loss: 1.5850... Val Loss: 1.4293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10... Step: 9330... Loss: 1.5744... Val Loss: 1.4291\n",
      "Epoch: 3/10... Step: 9340... Loss: 1.4358... Val Loss: 1.4269\n",
      "Epoch: 3/10... Step: 9350... Loss: 1.4629... Val Loss: 1.4270\n",
      "Epoch: 3/10... Step: 9360... Loss: 1.4714... Val Loss: 1.4267\n",
      "Epoch: 3/10... Step: 9370... Loss: 1.4555... Val Loss: 1.4278\n",
      "Epoch: 3/10... Step: 9380... Loss: 1.5161... Val Loss: 1.4291\n",
      "Epoch: 3/10... Step: 9390... Loss: 1.5744... Val Loss: 1.4269\n",
      "Epoch: 3/10... Step: 9400... Loss: 1.5339... Val Loss: 1.4280\n",
      "Epoch: 3/10... Step: 9410... Loss: 1.6016... Val Loss: 1.4279\n",
      "Epoch: 3/10... Step: 9420... Loss: 1.5043... Val Loss: 1.4279\n",
      "Epoch: 3/10... Step: 9430... Loss: 1.6088... Val Loss: 1.4266\n",
      "Epoch: 3/10... Step: 9440... Loss: 1.4056... Val Loss: 1.4263\n",
      "Epoch: 3/10... Step: 9450... Loss: 1.5751... Val Loss: 1.4268\n",
      "Epoch: 3/10... Step: 9460... Loss: 1.4874... Val Loss: 1.4273\n",
      "Epoch: 3/10... Step: 9470... Loss: 1.4195... Val Loss: 1.4283\n",
      "Epoch: 3/10... Step: 9480... Loss: 1.5590... Val Loss: 1.4283\n",
      "Epoch: 3/10... Step: 9490... Loss: 1.5113... Val Loss: 1.4283\n",
      "Epoch: 3/10... Step: 9500... Loss: 1.5286... Val Loss: 1.4290\n",
      "Epoch: 3/10... Step: 9510... Loss: 1.5109... Val Loss: 1.4311\n",
      "Epoch: 3/10... Step: 9520... Loss: 1.5333... Val Loss: 1.4294\n",
      "Epoch: 3/10... Step: 9530... Loss: 1.6198... Val Loss: 1.4271\n",
      "Epoch: 3/10... Step: 9540... Loss: 1.4105... Val Loss: 1.4264\n",
      "Epoch: 3/10... Step: 9550... Loss: 1.5614... Val Loss: 1.4258\n",
      "Epoch: 3/10... Step: 9560... Loss: 1.5620... Val Loss: 1.4256\n",
      "Epoch: 3/10... Step: 9570... Loss: 1.5745... Val Loss: 1.4250\n",
      "Epoch: 3/10... Step: 9580... Loss: 1.4968... Val Loss: 1.4241\n",
      "Epoch: 3/10... Step: 9590... Loss: 1.4621... Val Loss: 1.4231\n",
      "Epoch: 3/10... Step: 9600... Loss: 1.4632... Val Loss: 1.4255\n",
      "Epoch: 3/10... Step: 9610... Loss: 1.5600... Val Loss: 1.4267\n",
      "Epoch: 3/10... Step: 9620... Loss: 1.5092... Val Loss: 1.4261\n",
      "Epoch: 3/10... Step: 9630... Loss: 1.5434... Val Loss: 1.4264\n",
      "Epoch: 3/10... Step: 9640... Loss: 1.4796... Val Loss: 1.4256\n",
      "Epoch: 3/10... Step: 9650... Loss: 1.5701... Val Loss: 1.4265\n",
      "Epoch: 3/10... Step: 9660... Loss: 1.5230... Val Loss: 1.4233\n",
      "Epoch: 3/10... Step: 9670... Loss: 1.4740... Val Loss: 1.4251\n",
      "Epoch: 3/10... Step: 9680... Loss: 1.6030... Val Loss: 1.4250\n",
      "Epoch: 3/10... Step: 9690... Loss: 1.4899... Val Loss: 1.4236\n",
      "Epoch: 3/10... Step: 9700... Loss: 1.4826... Val Loss: 1.4236\n",
      "Epoch: 3/10... Step: 9710... Loss: 1.4718... Val Loss: 1.4267\n",
      "Epoch: 3/10... Step: 9720... Loss: 1.5118... Val Loss: 1.4278\n",
      "Epoch: 3/10... Step: 9730... Loss: 1.4734... Val Loss: 1.4267\n",
      "Epoch: 3/10... Step: 9740... Loss: 1.5632... Val Loss: 1.4278\n",
      "Epoch: 3/10... Step: 9750... Loss: 1.4057... Val Loss: 1.4278\n",
      "Epoch: 3/10... Step: 9760... Loss: 1.5271... Val Loss: 1.4265\n",
      "Epoch: 3/10... Step: 9770... Loss: 1.4148... Val Loss: 1.4267\n",
      "Epoch: 3/10... Step: 9780... Loss: 1.4849... Val Loss: 1.4287\n",
      "Epoch: 3/10... Step: 9790... Loss: 1.4636... Val Loss: 1.4291\n",
      "Epoch: 3/10... Step: 9800... Loss: 1.4401... Val Loss: 1.4267\n",
      "Epoch: 3/10... Step: 9810... Loss: 1.4202... Val Loss: 1.4253\n",
      "Epoch: 3/10... Step: 9820... Loss: 1.4634... Val Loss: 1.4258\n",
      "Epoch: 3/10... Step: 9830... Loss: 1.4656... Val Loss: 1.4245\n",
      "Epoch: 3/10... Step: 9840... Loss: 1.3747... Val Loss: 1.4263\n",
      "Epoch: 3/10... Step: 9850... Loss: 1.5374... Val Loss: 1.4296\n",
      "Epoch: 3/10... Step: 9860... Loss: 1.6204... Val Loss: 1.4243\n",
      "Epoch: 3/10... Step: 9870... Loss: 1.4176... Val Loss: 1.4245\n",
      "Epoch: 3/10... Step: 9880... Loss: 1.5738... Val Loss: 1.4284\n",
      "Epoch: 3/10... Step: 9890... Loss: 1.5132... Val Loss: 1.4260\n",
      "Epoch: 3/10... Step: 9900... Loss: 1.5276... Val Loss: 1.4240\n",
      "Epoch: 3/10... Step: 9910... Loss: 1.5767... Val Loss: 1.4237\n",
      "Epoch: 3/10... Step: 9920... Loss: 1.4474... Val Loss: 1.4239\n",
      "Epoch: 3/10... Step: 9930... Loss: 1.5843... Val Loss: 1.4259\n",
      "Epoch: 3/10... Step: 9940... Loss: 1.4453... Val Loss: 1.4270\n",
      "Epoch: 3/10... Step: 9950... Loss: 1.5585... Val Loss: 1.4260\n",
      "Epoch: 3/10... Step: 9960... Loss: 1.5921... Val Loss: 1.4226\n",
      "Epoch: 3/10... Step: 9970... Loss: 1.5393... Val Loss: 1.4225\n",
      "Epoch: 3/10... Step: 9980... Loss: 1.5411... Val Loss: 1.4270\n",
      "Epoch: 3/10... Step: 9990... Loss: 1.3887... Val Loss: 1.4252\n",
      "Epoch: 3/10... Step: 10000... Loss: 1.5832... Val Loss: 1.4241\n",
      "Epoch: 3/10... Step: 10010... Loss: 1.4399... Val Loss: 1.4252\n",
      "Epoch: 3/10... Step: 10020... Loss: 1.5341... Val Loss: 1.4235\n",
      "Epoch: 3/10... Step: 10030... Loss: 1.4228... Val Loss: 1.4237\n",
      "Epoch: 3/10... Step: 10040... Loss: 1.4633... Val Loss: 1.4274\n",
      "Epoch: 3/10... Step: 10050... Loss: 1.4265... Val Loss: 1.4228\n",
      "Epoch: 3/10... Step: 10060... Loss: 1.5770... Val Loss: 1.4215\n",
      "Epoch: 3/10... Step: 10070... Loss: 1.3460... Val Loss: 1.4203\n",
      "Epoch: 3/10... Step: 10080... Loss: 1.4673... Val Loss: 1.4230\n",
      "Epoch: 3/10... Step: 10090... Loss: 1.4622... Val Loss: 1.4252\n",
      "Epoch: 3/10... Step: 10100... Loss: 1.5442... Val Loss: 1.4228\n",
      "Epoch: 3/10... Step: 10110... Loss: 1.5543... Val Loss: 1.4209\n",
      "Epoch: 3/10... Step: 10120... Loss: 1.4493... Val Loss: 1.4232\n",
      "Epoch: 3/10... Step: 10130... Loss: 1.4171... Val Loss: 1.4227\n",
      "Epoch: 3/10... Step: 10140... Loss: 1.5585... Val Loss: 1.4221\n",
      "Epoch: 4/10... Step: 10150... Loss: 1.6064... Val Loss: 1.4230\n",
      "Epoch: 4/10... Step: 10160... Loss: 1.4286... Val Loss: 1.4206\n",
      "Epoch: 4/10... Step: 10170... Loss: 1.4146... Val Loss: 1.4204\n",
      "Epoch: 4/10... Step: 10180... Loss: 1.4303... Val Loss: 1.4225\n",
      "Epoch: 4/10... Step: 10190... Loss: 1.4267... Val Loss: 1.4195\n",
      "Epoch: 4/10... Step: 10200... Loss: 1.5563... Val Loss: 1.4213\n",
      "Epoch: 4/10... Step: 10210... Loss: 1.5812... Val Loss: 1.4213\n",
      "Epoch: 4/10... Step: 10220... Loss: 1.4871... Val Loss: 1.4189\n",
      "Epoch: 4/10... Step: 10230... Loss: 1.5521... Val Loss: 1.4204\n",
      "Epoch: 4/10... Step: 10240... Loss: 1.4922... Val Loss: 1.4195\n",
      "Epoch: 4/10... Step: 10250... Loss: 1.5025... Val Loss: 1.4174\n",
      "Epoch: 4/10... Step: 10260... Loss: 1.5020... Val Loss: 1.4198\n",
      "Epoch: 4/10... Step: 10270... Loss: 1.5085... Val Loss: 1.4242\n",
      "Epoch: 4/10... Step: 10280... Loss: 1.5189... Val Loss: 1.4247\n",
      "Epoch: 4/10... Step: 10290... Loss: 1.5384... Val Loss: 1.4198\n",
      "Epoch: 4/10... Step: 10300... Loss: 1.5105... Val Loss: 1.4185\n",
      "Epoch: 4/10... Step: 10310... Loss: 1.5081... Val Loss: 1.4184\n",
      "Epoch: 4/10... Step: 10320... Loss: 1.4793... Val Loss: 1.4210\n",
      "Epoch: 4/10... Step: 10330... Loss: 1.5229... Val Loss: 1.4219\n",
      "Epoch: 4/10... Step: 10340... Loss: 1.5643... Val Loss: 1.4225\n",
      "Epoch: 4/10... Step: 10350... Loss: 1.5860... Val Loss: 1.4240\n",
      "Epoch: 4/10... Step: 10360... Loss: 1.5945... Val Loss: 1.4184\n",
      "Epoch: 4/10... Step: 10370... Loss: 1.5179... Val Loss: 1.4173\n",
      "Epoch: 4/10... Step: 10380... Loss: 1.4263... Val Loss: 1.4176\n",
      "Epoch: 4/10... Step: 10390... Loss: 1.4987... Val Loss: 1.4183\n",
      "Epoch: 4/10... Step: 10400... Loss: 1.4851... Val Loss: 1.4187\n",
      "Epoch: 4/10... Step: 10410... Loss: 1.4847... Val Loss: 1.4177\n",
      "Epoch: 4/10... Step: 10420... Loss: 1.4948... Val Loss: 1.4179\n",
      "Epoch: 4/10... Step: 10430... Loss: 1.5352... Val Loss: 1.4196\n",
      "Epoch: 4/10... Step: 10440... Loss: 1.5364... Val Loss: 1.4180\n",
      "Epoch: 4/10... Step: 10450... Loss: 1.6179... Val Loss: 1.4174\n",
      "Epoch: 4/10... Step: 10460... Loss: 1.5299... Val Loss: 1.4182\n",
      "Epoch: 4/10... Step: 10470... Loss: 1.5977... Val Loss: 1.4178\n",
      "Epoch: 4/10... Step: 10480... Loss: 1.4528... Val Loss: 1.4185\n",
      "Epoch: 4/10... Step: 10490... Loss: 1.4777... Val Loss: 1.4179\n",
      "Epoch: 4/10... Step: 10500... Loss: 1.4863... Val Loss: 1.4168\n",
      "Epoch: 4/10... Step: 10510... Loss: 1.4733... Val Loss: 1.4149\n",
      "Epoch: 4/10... Step: 10520... Loss: 1.5678... Val Loss: 1.4193\n",
      "Epoch: 4/10... Step: 10530... Loss: 1.5662... Val Loss: 1.4170\n",
      "Epoch: 4/10... Step: 10540... Loss: 1.4912... Val Loss: 1.4164\n",
      "Epoch: 4/10... Step: 10550... Loss: 1.4622... Val Loss: 1.4234\n",
      "Epoch: 4/10... Step: 10560... Loss: 1.5652... Val Loss: 1.4178\n",
      "Epoch: 4/10... Step: 10570... Loss: 1.4646... Val Loss: 1.4158\n",
      "Epoch: 4/10... Step: 10580... Loss: 1.5606... Val Loss: 1.4190\n",
      "Epoch: 4/10... Step: 10590... Loss: 1.5064... Val Loss: 1.4156\n",
      "Epoch: 4/10... Step: 10600... Loss: 1.4978... Val Loss: 1.4147\n",
      "Epoch: 4/10... Step: 10610... Loss: 1.5475... Val Loss: 1.4132\n",
      "Epoch: 4/10... Step: 10620... Loss: 1.5932... Val Loss: 1.4157\n",
      "Epoch: 4/10... Step: 10630... Loss: 1.5848... Val Loss: 1.4169\n",
      "Epoch: 4/10... Step: 10640... Loss: 1.5273... Val Loss: 1.4157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10... Step: 10650... Loss: 1.5187... Val Loss: 1.4126\n",
      "Epoch: 4/10... Step: 10660... Loss: 1.5187... Val Loss: 1.4141\n",
      "Epoch: 4/10... Step: 10670... Loss: 1.4458... Val Loss: 1.4128\n",
      "Epoch: 4/10... Step: 10680... Loss: 1.4622... Val Loss: 1.4135\n",
      "Epoch: 4/10... Step: 10690... Loss: 1.4954... Val Loss: 1.4136\n",
      "Epoch: 4/10... Step: 10700... Loss: 1.4441... Val Loss: 1.4126\n",
      "Epoch: 4/10... Step: 10710... Loss: 1.4463... Val Loss: 1.4139\n",
      "Epoch: 4/10... Step: 10720... Loss: 1.5003... Val Loss: 1.4184\n",
      "Epoch: 4/10... Step: 10730... Loss: 1.3825... Val Loss: 1.4141\n",
      "Epoch: 4/10... Step: 10740... Loss: 1.4054... Val Loss: 1.4110\n",
      "Epoch: 4/10... Step: 10750... Loss: 1.3612... Val Loss: 1.4111\n",
      "Epoch: 4/10... Step: 10760... Loss: 1.4429... Val Loss: 1.4157\n",
      "Epoch: 4/10... Step: 10770... Loss: 1.5107... Val Loss: 1.4185\n",
      "Epoch: 4/10... Step: 10780... Loss: 1.6036... Val Loss: 1.4138\n",
      "Epoch: 4/10... Step: 10790... Loss: 1.6095... Val Loss: 1.4125\n",
      "Epoch: 4/10... Step: 10800... Loss: 1.5607... Val Loss: 1.4129\n",
      "Epoch: 4/10... Step: 10810... Loss: 1.4414... Val Loss: 1.4099\n",
      "Epoch: 4/10... Step: 10820... Loss: 1.4397... Val Loss: 1.4090\n",
      "Epoch: 4/10... Step: 10830... Loss: 1.5113... Val Loss: 1.4154\n",
      "Epoch: 4/10... Step: 10840... Loss: 1.3865... Val Loss: 1.4140\n",
      "Epoch: 4/10... Step: 10850... Loss: 1.3706... Val Loss: 1.4147\n",
      "Epoch: 4/10... Step: 10860... Loss: 1.4426... Val Loss: 1.4132\n",
      "Epoch: 4/10... Step: 10870... Loss: 1.5518... Val Loss: 1.4124\n",
      "Epoch: 4/10... Step: 10880... Loss: 1.5743... Val Loss: 1.4135\n",
      "Epoch: 4/10... Step: 10890... Loss: 1.6203... Val Loss: 1.4139\n",
      "Epoch: 4/10... Step: 10900... Loss: 1.5471... Val Loss: 1.4143\n",
      "Epoch: 4/10... Step: 10910... Loss: 1.6243... Val Loss: 1.4135\n",
      "Epoch: 4/10... Step: 10920... Loss: 1.4439... Val Loss: 1.4138\n",
      "Epoch: 4/10... Step: 10930... Loss: 1.5842... Val Loss: 1.4113\n",
      "Epoch: 4/10... Step: 10940... Loss: 1.4556... Val Loss: 1.4099\n",
      "Epoch: 4/10... Step: 10950... Loss: 1.4974... Val Loss: 1.4118\n",
      "Epoch: 4/10... Step: 10960... Loss: 1.4394... Val Loss: 1.4103\n",
      "Epoch: 4/10... Step: 10970... Loss: 1.4457... Val Loss: 1.4155\n",
      "Epoch: 4/10... Step: 10980... Loss: 1.4660... Val Loss: 1.4153\n",
      "Epoch: 4/10... Step: 10990... Loss: 1.5136... Val Loss: 1.4135\n",
      "Epoch: 4/10... Step: 11000... Loss: 1.4735... Val Loss: 1.4127\n",
      "Epoch: 4/10... Step: 11010... Loss: 1.4501... Val Loss: 1.4118\n",
      "Epoch: 4/10... Step: 11020... Loss: 1.4708... Val Loss: 1.4106\n",
      "Epoch: 4/10... Step: 11030... Loss: 1.5353... Val Loss: 1.4106\n",
      "Epoch: 4/10... Step: 11040... Loss: 1.4727... Val Loss: 1.4141\n",
      "Epoch: 4/10... Step: 11050... Loss: 1.5706... Val Loss: 1.4124\n",
      "Epoch: 4/10... Step: 11060... Loss: 1.4804... Val Loss: 1.4119\n",
      "Epoch: 4/10... Step: 11070... Loss: 1.5542... Val Loss: 1.4107\n",
      "Epoch: 4/10... Step: 11080... Loss: 1.5206... Val Loss: 1.4183\n",
      "Epoch: 4/10... Step: 11090... Loss: 1.4681... Val Loss: 1.4123\n",
      "Epoch: 4/10... Step: 11100... Loss: 1.5778... Val Loss: 1.4125\n",
      "Epoch: 4/10... Step: 11110... Loss: 1.4258... Val Loss: 1.4120\n",
      "Epoch: 4/10... Step: 11120... Loss: 1.4705... Val Loss: 1.4129\n",
      "Epoch: 4/10... Step: 11130... Loss: 1.5724... Val Loss: 1.4128\n",
      "Epoch: 4/10... Step: 11140... Loss: 1.5673... Val Loss: 1.4120\n",
      "Epoch: 4/10... Step: 11150... Loss: 1.5467... Val Loss: 1.4117\n",
      "Epoch: 4/10... Step: 11160... Loss: 1.4625... Val Loss: 1.4125\n",
      "Epoch: 4/10... Step: 11170... Loss: 1.4941... Val Loss: 1.4109\n",
      "Epoch: 4/10... Step: 11180... Loss: 1.4750... Val Loss: 1.4133\n",
      "Epoch: 4/10... Step: 11190... Loss: 1.4902... Val Loss: 1.4105\n",
      "Epoch: 4/10... Step: 11200... Loss: 1.5841... Val Loss: 1.4098\n",
      "Epoch: 4/10... Step: 11210... Loss: 1.4109... Val Loss: 1.4114\n",
      "Epoch: 4/10... Step: 11220... Loss: 1.4648... Val Loss: 1.4112\n",
      "Epoch: 4/10... Step: 11230... Loss: 1.5178... Val Loss: 1.4131\n",
      "Epoch: 4/10... Step: 11240... Loss: 1.5353... Val Loss: 1.4097\n",
      "Epoch: 4/10... Step: 11250... Loss: 1.5476... Val Loss: 1.4181\n",
      "Epoch: 4/10... Step: 11260... Loss: 1.5894... Val Loss: 1.4141\n",
      "Epoch: 4/10... Step: 11270... Loss: 1.5396... Val Loss: 1.4109\n",
      "Epoch: 4/10... Step: 11280... Loss: 1.5818... Val Loss: 1.4084\n",
      "Epoch: 4/10... Step: 11290... Loss: 1.4709... Val Loss: 1.4148\n",
      "Epoch: 4/10... Step: 11300... Loss: 1.4213... Val Loss: 1.4134\n",
      "Epoch: 4/10... Step: 11310... Loss: 1.4559... Val Loss: 1.4096\n",
      "Epoch: 4/10... Step: 11320... Loss: 1.4782... Val Loss: 1.4094\n",
      "Epoch: 4/10... Step: 11330... Loss: 1.5627... Val Loss: 1.4091\n",
      "Epoch: 4/10... Step: 11340... Loss: 1.4706... Val Loss: 1.4104\n",
      "Epoch: 4/10... Step: 11350... Loss: 1.5713... Val Loss: 1.4141\n",
      "Epoch: 4/10... Step: 11360... Loss: 1.5192... Val Loss: 1.4101\n",
      "Epoch: 4/10... Step: 11370... Loss: 1.5087... Val Loss: 1.4100\n",
      "Epoch: 4/10... Step: 11380... Loss: 1.5597... Val Loss: 1.4096\n",
      "Epoch: 4/10... Step: 11390... Loss: 1.5778... Val Loss: 1.4090\n",
      "Epoch: 4/10... Step: 11400... Loss: 1.5526... Val Loss: 1.4091\n",
      "Epoch: 4/10... Step: 11410... Loss: 1.5333... Val Loss: 1.4098\n",
      "Epoch: 4/10... Step: 11420... Loss: 1.5669... Val Loss: 1.4120\n",
      "Epoch: 4/10... Step: 11430... Loss: 1.4650... Val Loss: 1.4107\n",
      "Epoch: 4/10... Step: 11440... Loss: 1.4166... Val Loss: 1.4085\n",
      "Epoch: 4/10... Step: 11450... Loss: 1.6201... Val Loss: 1.4077\n",
      "Epoch: 4/10... Step: 11460... Loss: 1.4533... Val Loss: 1.4081\n",
      "Epoch: 4/10... Step: 11470... Loss: 1.5755... Val Loss: 1.4065\n",
      "Epoch: 4/10... Step: 11480... Loss: 1.4531... Val Loss: 1.4068\n",
      "Epoch: 4/10... Step: 11490... Loss: 1.5976... Val Loss: 1.4068\n",
      "Epoch: 4/10... Step: 11500... Loss: 1.4943... Val Loss: 1.4058\n",
      "Epoch: 4/10... Step: 11510... Loss: 1.5275... Val Loss: 1.4059\n",
      "Epoch: 4/10... Step: 11520... Loss: 1.5409... Val Loss: 1.4071\n",
      "Epoch: 4/10... Step: 11530... Loss: 1.4787... Val Loss: 1.4085\n",
      "Epoch: 4/10... Step: 11540... Loss: 1.5943... Val Loss: 1.4088\n",
      "Epoch: 4/10... Step: 11550... Loss: 1.5158... Val Loss: 1.4132\n",
      "Epoch: 4/10... Step: 11560... Loss: 1.4958... Val Loss: 1.4166\n",
      "Epoch: 4/10... Step: 11570... Loss: 1.5769... Val Loss: 1.4145\n",
      "Epoch: 4/10... Step: 11580... Loss: 1.4274... Val Loss: 1.4096\n",
      "Epoch: 4/10... Step: 11590... Loss: 1.4592... Val Loss: 1.4101\n",
      "Epoch: 4/10... Step: 11600... Loss: 1.5910... Val Loss: 1.4119\n",
      "Epoch: 4/10... Step: 11610... Loss: 1.5009... Val Loss: 1.4129\n",
      "Epoch: 4/10... Step: 11620... Loss: 1.5825... Val Loss: 1.4114\n",
      "Epoch: 4/10... Step: 11630... Loss: 1.5018... Val Loss: 1.4099\n",
      "Epoch: 4/10... Step: 11640... Loss: 1.4301... Val Loss: 1.4094\n",
      "Epoch: 4/10... Step: 11650... Loss: 1.5029... Val Loss: 1.4070\n",
      "Epoch: 4/10... Step: 11660... Loss: 1.4510... Val Loss: 1.4076\n",
      "Epoch: 4/10... Step: 11670... Loss: 1.5668... Val Loss: 1.4074\n",
      "Epoch: 4/10... Step: 11680... Loss: 1.4593... Val Loss: 1.4072\n",
      "Epoch: 4/10... Step: 11690... Loss: 1.4527... Val Loss: 1.4069\n",
      "Epoch: 4/10... Step: 11700... Loss: 1.5496... Val Loss: 1.4070\n",
      "Epoch: 4/10... Step: 11710... Loss: 1.4743... Val Loss: 1.4075\n",
      "Epoch: 4/10... Step: 11720... Loss: 1.4324... Val Loss: 1.4072\n",
      "Epoch: 4/10... Step: 11730... Loss: 1.7435... Val Loss: 1.4073\n",
      "Epoch: 4/10... Step: 11740... Loss: 1.4668... Val Loss: 1.4044\n",
      "Epoch: 4/10... Step: 11750... Loss: 1.5393... Val Loss: 1.4055\n",
      "Epoch: 4/10... Step: 11760... Loss: 1.4821... Val Loss: 1.4057\n",
      "Epoch: 4/10... Step: 11770... Loss: 1.3590... Val Loss: 1.4045\n",
      "Epoch: 4/10... Step: 11780... Loss: 1.5052... Val Loss: 1.4049\n",
      "Epoch: 4/10... Step: 11790... Loss: 1.5942... Val Loss: 1.4067\n",
      "Epoch: 4/10... Step: 11800... Loss: 1.4900... Val Loss: 1.4040\n",
      "Epoch: 4/10... Step: 11810... Loss: 1.4159... Val Loss: 1.4028\n",
      "Epoch: 4/10... Step: 11820... Loss: 1.5906... Val Loss: 1.4078\n",
      "Epoch: 4/10... Step: 11830... Loss: 1.5374... Val Loss: 1.4069\n",
      "Epoch: 4/10... Step: 11840... Loss: 1.6934... Val Loss: 1.4040\n",
      "Epoch: 4/10... Step: 11850... Loss: 1.5161... Val Loss: 1.4038\n",
      "Epoch: 4/10... Step: 11860... Loss: 1.5174... Val Loss: 1.4049\n",
      "Epoch: 4/10... Step: 11870... Loss: 1.5360... Val Loss: 1.4024\n",
      "Epoch: 4/10... Step: 11880... Loss: 1.5034... Val Loss: 1.4037\n",
      "Epoch: 4/10... Step: 11890... Loss: 1.5577... Val Loss: 1.4098\n",
      "Epoch: 4/10... Step: 11900... Loss: 1.4996... Val Loss: 1.4038\n",
      "Epoch: 4/10... Step: 11910... Loss: 1.4945... Val Loss: 1.4025\n",
      "Epoch: 4/10... Step: 11920... Loss: 1.5286... Val Loss: 1.4013\n",
      "Epoch: 4/10... Step: 11930... Loss: 1.4450... Val Loss: 1.4001\n",
      "Epoch: 4/10... Step: 11940... Loss: 1.5093... Val Loss: 1.3999\n",
      "Epoch: 4/10... Step: 11950... Loss: 1.4544... Val Loss: 1.3996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10... Step: 11960... Loss: 1.5691... Val Loss: 1.4001\n",
      "Epoch: 4/10... Step: 11970... Loss: 1.4742... Val Loss: 1.4003\n",
      "Epoch: 4/10... Step: 11980... Loss: 1.4286... Val Loss: 1.4001\n",
      "Epoch: 4/10... Step: 11990... Loss: 1.5425... Val Loss: 1.4018\n",
      "Epoch: 4/10... Step: 12000... Loss: 1.5291... Val Loss: 1.4052\n",
      "Epoch: 4/10... Step: 12010... Loss: 1.6226... Val Loss: 1.4024\n",
      "Epoch: 4/10... Step: 12020... Loss: 1.4131... Val Loss: 1.4021\n",
      "Epoch: 4/10... Step: 12030... Loss: 1.5091... Val Loss: 1.4041\n",
      "Epoch: 4/10... Step: 12040... Loss: 1.4234... Val Loss: 1.4063\n",
      "Epoch: 4/10... Step: 12050... Loss: 1.5065... Val Loss: 1.4040\n",
      "Epoch: 4/10... Step: 12060... Loss: 1.5257... Val Loss: 1.4033\n",
      "Epoch: 4/10... Step: 12070... Loss: 1.5323... Val Loss: 1.4030\n",
      "Epoch: 4/10... Step: 12080... Loss: 1.3849... Val Loss: 1.4024\n",
      "Epoch: 4/10... Step: 12090... Loss: 1.4256... Val Loss: 1.4006\n",
      "Epoch: 4/10... Step: 12100... Loss: 1.5230... Val Loss: 1.3995\n",
      "Epoch: 4/10... Step: 12110... Loss: 1.4261... Val Loss: 1.3992\n",
      "Epoch: 4/10... Step: 12120... Loss: 1.4392... Val Loss: 1.3984\n",
      "Epoch: 4/10... Step: 12130... Loss: 1.5119... Val Loss: 1.3984\n",
      "Epoch: 4/10... Step: 12140... Loss: 1.5693... Val Loss: 1.3992\n",
      "Epoch: 4/10... Step: 12150... Loss: 1.5083... Val Loss: 1.3992\n",
      "Epoch: 4/10... Step: 12160... Loss: 1.6547... Val Loss: 1.4001\n",
      "Epoch: 4/10... Step: 12170... Loss: 1.4846... Val Loss: 1.4024\n",
      "Epoch: 4/10... Step: 12180... Loss: 1.5571... Val Loss: 1.4031\n",
      "Epoch: 4/10... Step: 12190... Loss: 1.5461... Val Loss: 1.3999\n",
      "Epoch: 4/10... Step: 12200... Loss: 1.4281... Val Loss: 1.4017\n",
      "Epoch: 4/10... Step: 12210... Loss: 1.5412... Val Loss: 1.4024\n",
      "Epoch: 4/10... Step: 12220... Loss: 1.5538... Val Loss: 1.3995\n",
      "Epoch: 4/10... Step: 12230... Loss: 1.4877... Val Loss: 1.4020\n",
      "Epoch: 4/10... Step: 12240... Loss: 1.5059... Val Loss: 1.4003\n",
      "Epoch: 4/10... Step: 12250... Loss: 1.4953... Val Loss: 1.4012\n",
      "Epoch: 4/10... Step: 12260... Loss: 1.5249... Val Loss: 1.4006\n",
      "Epoch: 4/10... Step: 12270... Loss: 1.6190... Val Loss: 1.4011\n",
      "Epoch: 4/10... Step: 12280... Loss: 1.4619... Val Loss: 1.4034\n",
      "Epoch: 4/10... Step: 12290... Loss: 1.5106... Val Loss: 1.4006\n",
      "Epoch: 4/10... Step: 12300... Loss: 1.6085... Val Loss: 1.4026\n",
      "Epoch: 4/10... Step: 12310... Loss: 1.4973... Val Loss: 1.3998\n",
      "Epoch: 4/10... Step: 12320... Loss: 1.4345... Val Loss: 1.4009\n",
      "Epoch: 4/10... Step: 12330... Loss: 1.4292... Val Loss: 1.4019\n",
      "Epoch: 4/10... Step: 12340... Loss: 1.4091... Val Loss: 1.4012\n",
      "Epoch: 4/10... Step: 12350... Loss: 1.4076... Val Loss: 1.4002\n",
      "Epoch: 4/10... Step: 12360... Loss: 1.5195... Val Loss: 1.4000\n",
      "Epoch: 4/10... Step: 12370... Loss: 1.4441... Val Loss: 1.3986\n",
      "Epoch: 4/10... Step: 12380... Loss: 1.5576... Val Loss: 1.3997\n",
      "Epoch: 4/10... Step: 12390... Loss: 1.5557... Val Loss: 1.3991\n",
      "Epoch: 4/10... Step: 12400... Loss: 1.4558... Val Loss: 1.3999\n",
      "Epoch: 4/10... Step: 12410... Loss: 1.5518... Val Loss: 1.3989\n",
      "Epoch: 4/10... Step: 12420... Loss: 1.6191... Val Loss: 1.3982\n",
      "Epoch: 4/10... Step: 12430... Loss: 1.6604... Val Loss: 1.3956\n",
      "Epoch: 4/10... Step: 12440... Loss: 1.4552... Val Loss: 1.3972\n",
      "Epoch: 4/10... Step: 12450... Loss: 1.5239... Val Loss: 1.3986\n",
      "Epoch: 4/10... Step: 12460... Loss: 1.4054... Val Loss: 1.3978\n",
      "Epoch: 4/10... Step: 12470... Loss: 1.4541... Val Loss: 1.3979\n",
      "Epoch: 4/10... Step: 12480... Loss: 1.5627... Val Loss: 1.3982\n",
      "Epoch: 4/10... Step: 12490... Loss: 1.5435... Val Loss: 1.3974\n",
      "Epoch: 4/10... Step: 12500... Loss: 1.4862... Val Loss: 1.3953\n",
      "Epoch: 4/10... Step: 12510... Loss: 1.5630... Val Loss: 1.3955\n",
      "Epoch: 4/10... Step: 12520... Loss: 1.5429... Val Loss: 1.3979\n",
      "Epoch: 4/10... Step: 12530... Loss: 1.5837... Val Loss: 1.3980\n",
      "Epoch: 4/10... Step: 12540... Loss: 1.4852... Val Loss: 1.3988\n",
      "Epoch: 4/10... Step: 12550... Loss: 1.4486... Val Loss: 1.4006\n",
      "Epoch: 4/10... Step: 12560... Loss: 1.4831... Val Loss: 1.3979\n",
      "Epoch: 4/10... Step: 12570... Loss: 1.4942... Val Loss: 1.3967\n",
      "Epoch: 4/10... Step: 12580... Loss: 1.5925... Val Loss: 1.3960\n",
      "Epoch: 4/10... Step: 12590... Loss: 1.4641... Val Loss: 1.3970\n",
      "Epoch: 4/10... Step: 12600... Loss: 1.5639... Val Loss: 1.3969\n",
      "Epoch: 4/10... Step: 12610... Loss: 1.4558... Val Loss: 1.3964\n",
      "Epoch: 4/10... Step: 12620... Loss: 1.5032... Val Loss: 1.3982\n",
      "Epoch: 4/10... Step: 12630... Loss: 1.4627... Val Loss: 1.3965\n",
      "Epoch: 4/10... Step: 12640... Loss: 1.4971... Val Loss: 1.3981\n",
      "Epoch: 4/10... Step: 12650... Loss: 1.5335... Val Loss: 1.3982\n",
      "Epoch: 4/10... Step: 12660... Loss: 1.4550... Val Loss: 1.3988\n",
      "Epoch: 4/10... Step: 12670... Loss: 1.5099... Val Loss: 1.4018\n",
      "Epoch: 4/10... Step: 12680... Loss: 1.4638... Val Loss: 1.3961\n",
      "Epoch: 4/10... Step: 12690... Loss: 1.3370... Val Loss: 1.3950\n",
      "Epoch: 4/10... Step: 12700... Loss: 1.5456... Val Loss: 1.3972\n",
      "Epoch: 4/10... Step: 12710... Loss: 1.4912... Val Loss: 1.3977\n",
      "Epoch: 4/10... Step: 12720... Loss: 1.5739... Val Loss: 1.3981\n",
      "Epoch: 4/10... Step: 12730... Loss: 1.5088... Val Loss: 1.3966\n",
      "Epoch: 4/10... Step: 12740... Loss: 1.5434... Val Loss: 1.3970\n",
      "Epoch: 4/10... Step: 12750... Loss: 1.3688... Val Loss: 1.3986\n",
      "Epoch: 4/10... Step: 12760... Loss: 1.4737... Val Loss: 1.3993\n",
      "Epoch: 4/10... Step: 12770... Loss: 1.4486... Val Loss: 1.3986\n",
      "Epoch: 4/10... Step: 12780... Loss: 1.4323... Val Loss: 1.3967\n",
      "Epoch: 4/10... Step: 12790... Loss: 1.4791... Val Loss: 1.3964\n",
      "Epoch: 4/10... Step: 12800... Loss: 1.3893... Val Loss: 1.3957\n",
      "Epoch: 4/10... Step: 12810... Loss: 1.4874... Val Loss: 1.3973\n",
      "Epoch: 4/10... Step: 12820... Loss: 1.5989... Val Loss: 1.3981\n",
      "Epoch: 4/10... Step: 12830... Loss: 1.5389... Val Loss: 1.3967\n",
      "Epoch: 4/10... Step: 12840... Loss: 1.4991... Val Loss: 1.3977\n",
      "Epoch: 4/10... Step: 12850... Loss: 1.5699... Val Loss: 1.3955\n",
      "Epoch: 4/10... Step: 12860... Loss: 1.5308... Val Loss: 1.3996\n",
      "Epoch: 4/10... Step: 12870... Loss: 1.5472... Val Loss: 1.3974\n",
      "Epoch: 4/10... Step: 12880... Loss: 1.5140... Val Loss: 1.3965\n",
      "Epoch: 4/10... Step: 12890... Loss: 1.3683... Val Loss: 1.3962\n",
      "Epoch: 4/10... Step: 12900... Loss: 1.3670... Val Loss: 1.3955\n",
      "Epoch: 4/10... Step: 12910... Loss: 1.5039... Val Loss: 1.3954\n",
      "Epoch: 4/10... Step: 12920... Loss: 1.5828... Val Loss: 1.3950\n",
      "Epoch: 4/10... Step: 12930... Loss: 1.3920... Val Loss: 1.3983\n",
      "Epoch: 4/10... Step: 12940... Loss: 1.4469... Val Loss: 1.3961\n",
      "Epoch: 4/10... Step: 12950... Loss: 1.4466... Val Loss: 1.3953\n",
      "Epoch: 4/10... Step: 12960... Loss: 1.5791... Val Loss: 1.3943\n",
      "Epoch: 4/10... Step: 12970... Loss: 1.4039... Val Loss: 1.3937\n",
      "Epoch: 4/10... Step: 12980... Loss: 1.4254... Val Loss: 1.3954\n",
      "Epoch: 4/10... Step: 12990... Loss: 1.4713... Val Loss: 1.3981\n",
      "Epoch: 4/10... Step: 13000... Loss: 1.4466... Val Loss: 1.3969\n",
      "Epoch: 4/10... Step: 13010... Loss: 1.4814... Val Loss: 1.3950\n",
      "Epoch: 4/10... Step: 13020... Loss: 1.4700... Val Loss: 1.3942\n",
      "Epoch: 4/10... Step: 13030... Loss: 1.3770... Val Loss: 1.3952\n",
      "Epoch: 4/10... Step: 13040... Loss: 1.5564... Val Loss: 1.3967\n",
      "Epoch: 4/10... Step: 13050... Loss: 1.4877... Val Loss: 1.3961\n",
      "Epoch: 4/10... Step: 13060... Loss: 1.5437... Val Loss: 1.3942\n",
      "Epoch: 4/10... Step: 13070... Loss: 1.5189... Val Loss: 1.3928\n",
      "Epoch: 4/10... Step: 13080... Loss: 1.5853... Val Loss: 1.3949\n",
      "Epoch: 4/10... Step: 13090... Loss: 1.5320... Val Loss: 1.3949\n",
      "Epoch: 4/10... Step: 13100... Loss: 1.6019... Val Loss: 1.3961\n",
      "Epoch: 4/10... Step: 13110... Loss: 1.5639... Val Loss: 1.3981\n",
      "Epoch: 4/10... Step: 13120... Loss: 1.4866... Val Loss: 1.3978\n",
      "Epoch: 4/10... Step: 13130... Loss: 1.3527... Val Loss: 1.3967\n",
      "Epoch: 4/10... Step: 13140... Loss: 1.5226... Val Loss: 1.3979\n",
      "Epoch: 4/10... Step: 13150... Loss: 1.4934... Val Loss: 1.3973\n",
      "Epoch: 4/10... Step: 13160... Loss: 1.3787... Val Loss: 1.3968\n",
      "Epoch: 4/10... Step: 13170... Loss: 1.4707... Val Loss: 1.3973\n",
      "Epoch: 4/10... Step: 13180... Loss: 1.4850... Val Loss: 1.3963\n",
      "Epoch: 4/10... Step: 13190... Loss: 1.3486... Val Loss: 1.3966\n",
      "Epoch: 4/10... Step: 13200... Loss: 1.5094... Val Loss: 1.3953\n",
      "Epoch: 4/10... Step: 13210... Loss: 1.4583... Val Loss: 1.3979\n",
      "Epoch: 4/10... Step: 13220... Loss: 1.4882... Val Loss: 1.3974\n",
      "Epoch: 4/10... Step: 13230... Loss: 1.5435... Val Loss: 1.3956\n",
      "Epoch: 4/10... Step: 13240... Loss: 1.5210... Val Loss: 1.3956\n",
      "Epoch: 4/10... Step: 13250... Loss: 1.5036... Val Loss: 1.3974\n",
      "Epoch: 4/10... Step: 13260... Loss: 1.4515... Val Loss: 1.3968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10... Step: 13270... Loss: 1.4879... Val Loss: 1.3978\n",
      "Epoch: 4/10... Step: 13280... Loss: 1.4712... Val Loss: 1.3951\n",
      "Epoch: 4/10... Step: 13290... Loss: 1.5007... Val Loss: 1.3953\n",
      "Epoch: 4/10... Step: 13300... Loss: 1.4803... Val Loss: 1.3951\n",
      "Epoch: 4/10... Step: 13310... Loss: 1.4645... Val Loss: 1.3954\n",
      "Epoch: 4/10... Step: 13320... Loss: 1.4361... Val Loss: 1.3977\n",
      "Epoch: 4/10... Step: 13330... Loss: 1.4862... Val Loss: 1.3950\n",
      "Epoch: 4/10... Step: 13340... Loss: 1.4050... Val Loss: 1.3939\n",
      "Epoch: 4/10... Step: 13350... Loss: 1.5126... Val Loss: 1.3952\n",
      "Epoch: 4/10... Step: 13360... Loss: 1.5097... Val Loss: 1.3965\n",
      "Epoch: 4/10... Step: 13370... Loss: 1.4669... Val Loss: 1.3959\n",
      "Epoch: 4/10... Step: 13380... Loss: 1.5134... Val Loss: 1.3959\n",
      "Epoch: 4/10... Step: 13390... Loss: 1.4605... Val Loss: 1.3948\n",
      "Epoch: 4/10... Step: 13400... Loss: 1.4710... Val Loss: 1.3942\n",
      "Epoch: 4/10... Step: 13410... Loss: 1.4507... Val Loss: 1.3974\n",
      "Epoch: 4/10... Step: 13420... Loss: 1.4972... Val Loss: 1.3968\n",
      "Epoch: 4/10... Step: 13430... Loss: 1.4991... Val Loss: 1.3952\n",
      "Epoch: 4/10... Step: 13440... Loss: 1.5047... Val Loss: 1.3953\n",
      "Epoch: 4/10... Step: 13450... Loss: 1.4424... Val Loss: 1.3952\n",
      "Epoch: 4/10... Step: 13460... Loss: 1.4208... Val Loss: 1.3969\n",
      "Epoch: 4/10... Step: 13470... Loss: 1.5873... Val Loss: 1.4003\n",
      "Epoch: 4/10... Step: 13480... Loss: 1.5060... Val Loss: 1.3991\n",
      "Epoch: 4/10... Step: 13490... Loss: 1.5353... Val Loss: 1.3957\n",
      "Epoch: 4/10... Step: 13500... Loss: 1.3678... Val Loss: 1.3967\n",
      "Epoch: 4/10... Step: 13510... Loss: 1.5391... Val Loss: 1.3952\n",
      "Epoch: 4/10... Step: 13520... Loss: 1.5108... Val Loss: 1.3945\n",
      "Epoch: 5/10... Step: 13530... Loss: 1.4625... Val Loss: 1.3995\n",
      "Epoch: 5/10... Step: 13540... Loss: 1.4273... Val Loss: 1.3928\n",
      "Epoch: 5/10... Step: 13550... Loss: 1.5113... Val Loss: 1.3941\n",
      "Epoch: 5/10... Step: 13560... Loss: 1.6206... Val Loss: 1.3955\n",
      "Epoch: 5/10... Step: 13570... Loss: 1.4348... Val Loss: 1.3935\n",
      "Epoch: 5/10... Step: 13580... Loss: 1.4509... Val Loss: 1.3916\n",
      "Epoch: 5/10... Step: 13590... Loss: 1.4645... Val Loss: 1.3944\n",
      "Epoch: 5/10... Step: 13600... Loss: 1.3788... Val Loss: 1.3933\n",
      "Epoch: 5/10... Step: 13610... Loss: 1.4599... Val Loss: 1.3929\n",
      "Epoch: 5/10... Step: 13620... Loss: 1.4681... Val Loss: 1.3935\n",
      "Epoch: 5/10... Step: 13630... Loss: 1.5072... Val Loss: 1.3948\n",
      "Epoch: 5/10... Step: 13640... Loss: 1.4399... Val Loss: 1.3967\n",
      "Epoch: 5/10... Step: 13650... Loss: 1.3484... Val Loss: 1.3969\n",
      "Epoch: 5/10... Step: 13660... Loss: 1.4357... Val Loss: 1.3976\n",
      "Epoch: 5/10... Step: 13670... Loss: 1.4341... Val Loss: 1.3960\n",
      "Epoch: 5/10... Step: 13680... Loss: 1.5085... Val Loss: 1.3931\n",
      "Epoch: 5/10... Step: 13690... Loss: 1.4699... Val Loss: 1.3918\n",
      "Epoch: 5/10... Step: 13700... Loss: 1.5569... Val Loss: 1.3909\n",
      "Epoch: 5/10... Step: 13710... Loss: 1.4805... Val Loss: 1.3907\n",
      "Epoch: 5/10... Step: 13720... Loss: 1.5676... Val Loss: 1.3962\n",
      "Epoch: 5/10... Step: 13730... Loss: 1.4097... Val Loss: 1.3954\n",
      "Epoch: 5/10... Step: 13740... Loss: 1.4355... Val Loss: 1.3926\n",
      "Epoch: 5/10... Step: 13750... Loss: 1.4535... Val Loss: 1.3933\n",
      "Epoch: 5/10... Step: 13760... Loss: 1.4783... Val Loss: 1.3928\n",
      "Epoch: 5/10... Step: 13770... Loss: 1.4409... Val Loss: 1.3943\n",
      "Epoch: 5/10... Step: 13780... Loss: 1.3932... Val Loss: 1.3935\n",
      "Epoch: 5/10... Step: 13790... Loss: 1.3487... Val Loss: 1.3926\n",
      "Epoch: 5/10... Step: 13800... Loss: 1.4548... Val Loss: 1.3927\n",
      "Epoch: 5/10... Step: 13810... Loss: 1.6189... Val Loss: 1.3939\n",
      "Epoch: 5/10... Step: 13820... Loss: 1.5404... Val Loss: 1.3954\n",
      "Epoch: 5/10... Step: 13830... Loss: 1.4506... Val Loss: 1.3960\n",
      "Epoch: 5/10... Step: 13840... Loss: 1.4700... Val Loss: 1.3954\n",
      "Epoch: 5/10... Step: 13850... Loss: 1.4591... Val Loss: 1.3946\n",
      "Epoch: 5/10... Step: 13860... Loss: 1.5178... Val Loss: 1.3958\n",
      "Epoch: 5/10... Step: 13870... Loss: 1.4069... Val Loss: 1.3957\n",
      "Epoch: 5/10... Step: 13880... Loss: 1.4419... Val Loss: 1.3943\n",
      "Epoch: 5/10... Step: 13890... Loss: 1.4694... Val Loss: 1.3924\n",
      "Epoch: 5/10... Step: 13900... Loss: 1.5149... Val Loss: 1.3941\n",
      "Epoch: 5/10... Step: 13910... Loss: 1.5838... Val Loss: 1.3924\n",
      "Epoch: 5/10... Step: 13920... Loss: 1.4371... Val Loss: 1.3912\n",
      "Epoch: 5/10... Step: 13930... Loss: 1.4570... Val Loss: 1.3914\n",
      "Epoch: 5/10... Step: 13940... Loss: 1.4298... Val Loss: 1.3946\n",
      "Epoch: 5/10... Step: 13950... Loss: 1.3918... Val Loss: 1.3922\n",
      "Epoch: 5/10... Step: 13960... Loss: 1.4099... Val Loss: 1.3924\n",
      "Epoch: 5/10... Step: 13970... Loss: 1.5335... Val Loss: 1.3926\n",
      "Epoch: 5/10... Step: 13980... Loss: 1.4271... Val Loss: 1.3944\n",
      "Epoch: 5/10... Step: 13990... Loss: 1.4446... Val Loss: 1.3919\n",
      "Epoch: 5/10... Step: 14000... Loss: 1.3995... Val Loss: 1.3909\n",
      "Epoch: 5/10... Step: 14010... Loss: 1.5511... Val Loss: 1.3897\n",
      "Epoch: 5/10... Step: 14020... Loss: 1.3803... Val Loss: 1.3902\n",
      "Epoch: 5/10... Step: 14030... Loss: 1.5261... Val Loss: 1.3898\n",
      "Epoch: 5/10... Step: 14040... Loss: 1.3957... Val Loss: 1.3888\n",
      "Epoch: 5/10... Step: 14050... Loss: 1.5479... Val Loss: 1.3874\n",
      "Epoch: 5/10... Step: 14060... Loss: 1.4465... Val Loss: 1.3867\n",
      "Epoch: 5/10... Step: 14070... Loss: 1.4449... Val Loss: 1.3876\n",
      "Epoch: 5/10... Step: 14080... Loss: 1.5212... Val Loss: 1.3887\n",
      "Epoch: 5/10... Step: 14090... Loss: 1.5198... Val Loss: 1.3906\n",
      "Epoch: 5/10... Step: 14100... Loss: 1.4816... Val Loss: 1.3895\n",
      "Epoch: 5/10... Step: 14110... Loss: 1.5098... Val Loss: 1.3873\n",
      "Epoch: 5/10... Step: 14120... Loss: 1.4972... Val Loss: 1.3866\n",
      "Epoch: 5/10... Step: 14130... Loss: 1.6355... Val Loss: 1.3853\n",
      "Epoch: 5/10... Step: 14140... Loss: 1.3851... Val Loss: 1.3864\n",
      "Epoch: 5/10... Step: 14150... Loss: 1.5353... Val Loss: 1.3897\n",
      "Epoch: 5/10... Step: 14160... Loss: 1.4569... Val Loss: 1.3892\n",
      "Epoch: 5/10... Step: 14170... Loss: 1.4588... Val Loss: 1.3873\n",
      "Epoch: 5/10... Step: 14180... Loss: 1.4280... Val Loss: 1.3876\n",
      "Epoch: 5/10... Step: 14190... Loss: 1.4704... Val Loss: 1.3875\n",
      "Epoch: 5/10... Step: 14200... Loss: 1.4390... Val Loss: 1.3850\n",
      "Epoch: 5/10... Step: 14210... Loss: 1.4858... Val Loss: 1.3862\n",
      "Epoch: 5/10... Step: 14220... Loss: 1.4847... Val Loss: 1.3883\n",
      "Epoch: 5/10... Step: 14230... Loss: 1.3537... Val Loss: 1.3882\n",
      "Epoch: 5/10... Step: 14240... Loss: 1.4077... Val Loss: 1.3889\n",
      "Epoch: 5/10... Step: 14250... Loss: 1.5281... Val Loss: 1.3885\n",
      "Epoch: 5/10... Step: 14260... Loss: 1.3911... Val Loss: 1.3890\n",
      "Epoch: 5/10... Step: 14270... Loss: 1.5601... Val Loss: 1.3904\n",
      "Epoch: 5/10... Step: 14280... Loss: 1.5351... Val Loss: 1.3927\n",
      "Epoch: 5/10... Step: 14290... Loss: 1.4468... Val Loss: 1.3873\n",
      "Epoch: 5/10... Step: 14300... Loss: 1.6918... Val Loss: 1.3879\n",
      "Epoch: 5/10... Step: 14310... Loss: 1.5041... Val Loss: 1.3890\n",
      "Epoch: 5/10... Step: 14320... Loss: 1.4485... Val Loss: 1.3868\n",
      "Epoch: 5/10... Step: 14330... Loss: 1.3718... Val Loss: 1.3880\n",
      "Epoch: 5/10... Step: 14340... Loss: 1.5091... Val Loss: 1.3871\n",
      "Epoch: 5/10... Step: 14350... Loss: 1.4660... Val Loss: 1.3885\n",
      "Epoch: 5/10... Step: 14360... Loss: 1.5247... Val Loss: 1.3902\n",
      "Epoch: 5/10... Step: 14370... Loss: 1.4772... Val Loss: 1.3875\n",
      "Epoch: 5/10... Step: 14380... Loss: 1.4079... Val Loss: 1.3878\n",
      "Epoch: 5/10... Step: 14390... Loss: 1.4844... Val Loss: 1.3878\n",
      "Epoch: 5/10... Step: 14400... Loss: 1.5109... Val Loss: 1.3879\n",
      "Epoch: 5/10... Step: 14410... Loss: 1.4648... Val Loss: 1.3874\n",
      "Epoch: 5/10... Step: 14420... Loss: 1.4208... Val Loss: 1.3882\n",
      "Epoch: 5/10... Step: 14430... Loss: 1.5298... Val Loss: 1.3882\n",
      "Epoch: 5/10... Step: 14440... Loss: 1.4253... Val Loss: 1.3885\n",
      "Epoch: 5/10... Step: 14450... Loss: 1.5445... Val Loss: 1.3876\n",
      "Epoch: 5/10... Step: 14460... Loss: 1.4504... Val Loss: 1.3900\n",
      "Epoch: 5/10... Step: 14470... Loss: 1.4984... Val Loss: 1.3921\n",
      "Epoch: 5/10... Step: 14480... Loss: 1.5984... Val Loss: 1.3885\n",
      "Epoch: 5/10... Step: 14490... Loss: 1.4234... Val Loss: 1.3868\n",
      "Epoch: 5/10... Step: 14500... Loss: 1.4263... Val Loss: 1.3876\n",
      "Epoch: 5/10... Step: 14510... Loss: 1.3833... Val Loss: 1.3869\n",
      "Epoch: 5/10... Step: 14520... Loss: 1.4166... Val Loss: 1.3871\n",
      "Epoch: 5/10... Step: 14530... Loss: 1.3648... Val Loss: 1.3874\n",
      "Epoch: 5/10... Step: 14540... Loss: 1.4801... Val Loss: 1.3880\n",
      "Epoch: 5/10... Step: 14550... Loss: 1.5531... Val Loss: 1.3885\n",
      "Epoch: 5/10... Step: 14560... Loss: 1.4564... Val Loss: 1.3889\n",
      "Epoch: 5/10... Step: 14570... Loss: 1.4407... Val Loss: 1.3905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10... Step: 14580... Loss: 1.5449... Val Loss: 1.3885\n",
      "Epoch: 5/10... Step: 14590... Loss: 1.4632... Val Loss: 1.3863\n",
      "Epoch: 5/10... Step: 14600... Loss: 1.4979... Val Loss: 1.3858\n",
      "Epoch: 5/10... Step: 14610... Loss: 1.5944... Val Loss: 1.3855\n",
      "Epoch: 5/10... Step: 14620... Loss: 1.5301... Val Loss: 1.3857\n",
      "Epoch: 5/10... Step: 14630... Loss: 1.5053... Val Loss: 1.3869\n",
      "Epoch: 5/10... Step: 14640... Loss: 1.5169... Val Loss: 1.3869\n",
      "Epoch: 5/10... Step: 14650... Loss: 1.4336... Val Loss: 1.3867\n",
      "Epoch: 5/10... Step: 14660... Loss: 1.5157... Val Loss: 1.3911\n",
      "Epoch: 5/10... Step: 14670... Loss: 1.5152... Val Loss: 1.3888\n",
      "Epoch: 5/10... Step: 14680... Loss: 1.4341... Val Loss: 1.3884\n",
      "Epoch: 5/10... Step: 14690... Loss: 1.4348... Val Loss: 1.3877\n",
      "Epoch: 5/10... Step: 14700... Loss: 1.3430... Val Loss: 1.3874\n",
      "Epoch: 5/10... Step: 14710... Loss: 1.4878... Val Loss: 1.3853\n",
      "Epoch: 5/10... Step: 14720... Loss: 1.4517... Val Loss: 1.3863\n",
      "Epoch: 5/10... Step: 14730... Loss: 1.7098... Val Loss: 1.3901\n",
      "Epoch: 5/10... Step: 14740... Loss: 1.4565... Val Loss: 1.3871\n",
      "Epoch: 5/10... Step: 14750... Loss: 1.4499... Val Loss: 1.3872\n",
      "Epoch: 5/10... Step: 14760... Loss: 1.4636... Val Loss: 1.3856\n",
      "Epoch: 5/10... Step: 14770... Loss: 1.5328... Val Loss: 1.3859\n",
      "Epoch: 5/10... Step: 14780... Loss: 1.5236... Val Loss: 1.3880\n",
      "Epoch: 5/10... Step: 14790... Loss: 1.3361... Val Loss: 1.3883\n",
      "Epoch: 5/10... Step: 14800... Loss: 1.3621... Val Loss: 1.3892\n",
      "Epoch: 5/10... Step: 14810... Loss: 1.3883... Val Loss: 1.3889\n",
      "Epoch: 5/10... Step: 14820... Loss: 1.5167... Val Loss: 1.3867\n",
      "Epoch: 5/10... Step: 14830... Loss: 1.5740... Val Loss: 1.3863\n",
      "Epoch: 5/10... Step: 14840... Loss: 1.3968... Val Loss: 1.3850\n",
      "Epoch: 5/10... Step: 14850... Loss: 1.4794... Val Loss: 1.3834\n",
      "Epoch: 5/10... Step: 14860... Loss: 1.4977... Val Loss: 1.3838\n",
      "Epoch: 5/10... Step: 14870... Loss: 1.4566... Val Loss: 1.3819\n",
      "Epoch: 5/10... Step: 14880... Loss: 1.4748... Val Loss: 1.3833\n",
      "Epoch: 5/10... Step: 14890... Loss: 1.3711... Val Loss: 1.3838\n",
      "Epoch: 5/10... Step: 14900... Loss: 1.4730... Val Loss: 1.3820\n",
      "Epoch: 5/10... Step: 14910... Loss: 1.5611... Val Loss: 1.3832\n",
      "Epoch: 5/10... Step: 14920... Loss: 1.6181... Val Loss: 1.3838\n",
      "Epoch: 5/10... Step: 14930... Loss: 1.5319... Val Loss: 1.3854\n",
      "Epoch: 5/10... Step: 14940... Loss: 1.5162... Val Loss: 1.3868\n",
      "Epoch: 5/10... Step: 14950... Loss: 1.3933... Val Loss: 1.3870\n",
      "Epoch: 5/10... Step: 14960... Loss: 1.4712... Val Loss: 1.3835\n",
      "Epoch: 5/10... Step: 14970... Loss: 1.3524... Val Loss: 1.3828\n",
      "Epoch: 5/10... Step: 14980... Loss: 1.4034... Val Loss: 1.3853\n",
      "Epoch: 5/10... Step: 14990... Loss: 1.5065... Val Loss: 1.3855\n",
      "Epoch: 5/10... Step: 15000... Loss: 1.4622... Val Loss: 1.3846\n",
      "Epoch: 5/10... Step: 15010... Loss: 1.4483... Val Loss: 1.3844\n",
      "Epoch: 5/10... Step: 15020... Loss: 1.4886... Val Loss: 1.3861\n",
      "Epoch: 5/10... Step: 15030... Loss: 1.5244... Val Loss: 1.3871\n",
      "Epoch: 5/10... Step: 15040... Loss: 1.4057... Val Loss: 1.3869\n",
      "Epoch: 5/10... Step: 15050... Loss: 1.4123... Val Loss: 1.3854\n",
      "Epoch: 5/10... Step: 15060... Loss: 1.5944... Val Loss: 1.3833\n",
      "Epoch: 5/10... Step: 15070... Loss: 1.5828... Val Loss: 1.3858\n",
      "Epoch: 5/10... Step: 15080... Loss: 1.3497... Val Loss: 1.3856\n",
      "Epoch: 5/10... Step: 15090... Loss: 1.4655... Val Loss: 1.3859\n",
      "Epoch: 5/10... Step: 15100... Loss: 1.4250... Val Loss: 1.3846\n",
      "Epoch: 5/10... Step: 15110... Loss: 1.5511... Val Loss: 1.3857\n",
      "Epoch: 5/10... Step: 15120... Loss: 1.5128... Val Loss: 1.3860\n",
      "Epoch: 5/10... Step: 15130... Loss: 1.5275... Val Loss: 1.3847\n",
      "Epoch: 5/10... Step: 15140... Loss: 1.5432... Val Loss: 1.3831\n",
      "Epoch: 5/10... Step: 15150... Loss: 1.4014... Val Loss: 1.3840\n",
      "Epoch: 5/10... Step: 15160... Loss: 1.4157... Val Loss: 1.3825\n",
      "Epoch: 5/10... Step: 15170... Loss: 1.5982... Val Loss: 1.3835\n",
      "Epoch: 5/10... Step: 15180... Loss: 1.4464... Val Loss: 1.3842\n",
      "Epoch: 5/10... Step: 15190... Loss: 1.4497... Val Loss: 1.3825\n",
      "Epoch: 5/10... Step: 15200... Loss: 1.3787... Val Loss: 1.3887\n",
      "Epoch: 5/10... Step: 15210... Loss: 1.4869... Val Loss: 1.3871\n",
      "Epoch: 5/10... Step: 15220... Loss: 1.4624... Val Loss: 1.3828\n",
      "Epoch: 5/10... Step: 15230... Loss: 1.5380... Val Loss: 1.3826\n",
      "Epoch: 5/10... Step: 15240... Loss: 1.5239... Val Loss: 1.3838\n",
      "Epoch: 5/10... Step: 15250... Loss: 1.5101... Val Loss: 1.3802\n",
      "Epoch: 5/10... Step: 15260... Loss: 1.5108... Val Loss: 1.3804\n",
      "Epoch: 5/10... Step: 15270... Loss: 1.5658... Val Loss: 1.3892\n",
      "Epoch: 5/10... Step: 15280... Loss: 1.6241... Val Loss: 1.3806\n",
      "Epoch: 5/10... Step: 15290... Loss: 1.3151... Val Loss: 1.3793\n",
      "Epoch: 5/10... Step: 15300... Loss: 1.5538... Val Loss: 1.3802\n",
      "Epoch: 5/10... Step: 15310... Loss: 1.4655... Val Loss: 1.3797\n",
      "Epoch: 5/10... Step: 15320... Loss: 1.4629... Val Loss: 1.3790\n",
      "Epoch: 5/10... Step: 15330... Loss: 1.3390... Val Loss: 1.3791\n",
      "Epoch: 5/10... Step: 15340... Loss: 1.3986... Val Loss: 1.3788\n",
      "Epoch: 5/10... Step: 15350... Loss: 1.3865... Val Loss: 1.3797\n",
      "Epoch: 5/10... Step: 15360... Loss: 1.4321... Val Loss: 1.3799\n",
      "Epoch: 5/10... Step: 15370... Loss: 1.4005... Val Loss: 1.3817\n",
      "Epoch: 5/10... Step: 15380... Loss: 1.4411... Val Loss: 1.3817\n",
      "Epoch: 5/10... Step: 15390... Loss: 1.4767... Val Loss: 1.3815\n",
      "Epoch: 5/10... Step: 15400... Loss: 1.3723... Val Loss: 1.3806\n",
      "Epoch: 5/10... Step: 15410... Loss: 1.4401... Val Loss: 1.3858\n",
      "Epoch: 5/10... Step: 15420... Loss: 1.5066... Val Loss: 1.3838\n",
      "Epoch: 5/10... Step: 15430... Loss: 1.5391... Val Loss: 1.3802\n",
      "Epoch: 5/10... Step: 15440... Loss: 1.5633... Val Loss: 1.3798\n",
      "Epoch: 5/10... Step: 15450... Loss: 1.5197... Val Loss: 1.3815\n",
      "Epoch: 5/10... Step: 15460... Loss: 1.3801... Val Loss: 1.3810\n",
      "Epoch: 5/10... Step: 15470... Loss: 1.3903... Val Loss: 1.3801\n",
      "Epoch: 5/10... Step: 15480... Loss: 1.4121... Val Loss: 1.3796\n",
      "Epoch: 5/10... Step: 15490... Loss: 1.3473... Val Loss: 1.3804\n",
      "Epoch: 5/10... Step: 15500... Loss: 1.4732... Val Loss: 1.3806\n",
      "Epoch: 5/10... Step: 15510... Loss: 1.4936... Val Loss: 1.3805\n",
      "Epoch: 5/10... Step: 15520... Loss: 1.4882... Val Loss: 1.3803\n",
      "Epoch: 5/10... Step: 15530... Loss: 1.3914... Val Loss: 1.3784\n",
      "Epoch: 5/10... Step: 15540... Loss: 1.3925... Val Loss: 1.3781\n",
      "Epoch: 5/10... Step: 15550... Loss: 1.4625... Val Loss: 1.3816\n",
      "Epoch: 5/10... Step: 15560... Loss: 1.5236... Val Loss: 1.3807\n",
      "Epoch: 5/10... Step: 15570... Loss: 1.4353... Val Loss: 1.3786\n",
      "Epoch: 5/10... Step: 15580... Loss: 1.3858... Val Loss: 1.3797\n",
      "Epoch: 5/10... Step: 15590... Loss: 1.3719... Val Loss: 1.3811\n",
      "Epoch: 5/10... Step: 15600... Loss: 1.4223... Val Loss: 1.3798\n",
      "Epoch: 5/10... Step: 15610... Loss: 1.4599... Val Loss: 1.3794\n",
      "Epoch: 5/10... Step: 15620... Loss: 1.4146... Val Loss: 1.3795\n",
      "Epoch: 5/10... Step: 15630... Loss: 1.5517... Val Loss: 1.3796\n",
      "Epoch: 5/10... Step: 15640... Loss: 1.4703... Val Loss: 1.3799\n",
      "Epoch: 5/10... Step: 15650... Loss: 1.4738... Val Loss: 1.3831\n",
      "Epoch: 5/10... Step: 15660... Loss: 1.4682... Val Loss: 1.3842\n",
      "Epoch: 5/10... Step: 15670... Loss: 1.5299... Val Loss: 1.3788\n",
      "Epoch: 5/10... Step: 15680... Loss: 1.4771... Val Loss: 1.3777\n",
      "Epoch: 5/10... Step: 15690... Loss: 1.5113... Val Loss: 1.3791\n",
      "Epoch: 5/10... Step: 15700... Loss: 1.4004... Val Loss: 1.3807\n",
      "Epoch: 5/10... Step: 15710... Loss: 1.4216... Val Loss: 1.3785\n",
      "Epoch: 5/10... Step: 15720... Loss: 1.4173... Val Loss: 1.3785\n",
      "Epoch: 5/10... Step: 15730... Loss: 1.5321... Val Loss: 1.3783\n",
      "Epoch: 5/10... Step: 15740... Loss: 1.4626... Val Loss: 1.3778\n",
      "Epoch: 5/10... Step: 15750... Loss: 1.4461... Val Loss: 1.3758\n",
      "Epoch: 5/10... Step: 15760... Loss: 1.5230... Val Loss: 1.3777\n",
      "Epoch: 5/10... Step: 15770... Loss: 1.4913... Val Loss: 1.3790\n",
      "Epoch: 5/10... Step: 15780... Loss: 1.4395... Val Loss: 1.3772\n",
      "Epoch: 5/10... Step: 15790... Loss: 1.3864... Val Loss: 1.3761\n",
      "Epoch: 5/10... Step: 15800... Loss: 1.5087... Val Loss: 1.3768\n",
      "Epoch: 5/10... Step: 15810... Loss: 1.4700... Val Loss: 1.3772\n",
      "Epoch: 5/10... Step: 15820... Loss: 1.5211... Val Loss: 1.3775\n",
      "Epoch: 5/10... Step: 15830... Loss: 1.4853... Val Loss: 1.3772\n",
      "Epoch: 5/10... Step: 15840... Loss: 1.4363... Val Loss: 1.3781\n",
      "Epoch: 5/10... Step: 15850... Loss: 1.4848... Val Loss: 1.3761\n",
      "Epoch: 5/10... Step: 15860... Loss: 1.4439... Val Loss: 1.3776\n",
      "Epoch: 5/10... Step: 15870... Loss: 1.3943... Val Loss: 1.3763\n",
      "Epoch: 5/10... Step: 15880... Loss: 1.4597... Val Loss: 1.3760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10... Step: 15890... Loss: 1.4983... Val Loss: 1.3764\n",
      "Epoch: 5/10... Step: 15900... Loss: 1.6018... Val Loss: 1.3769\n",
      "Epoch: 5/10... Step: 15910... Loss: 1.5367... Val Loss: 1.3776\n",
      "Epoch: 5/10... Step: 15920... Loss: 1.5353... Val Loss: 1.3790\n",
      "Epoch: 5/10... Step: 15930... Loss: 1.4310... Val Loss: 1.3789\n",
      "Epoch: 5/10... Step: 15940... Loss: 1.4601... Val Loss: 1.3787\n",
      "Epoch: 5/10... Step: 15950... Loss: 1.4474... Val Loss: 1.3771\n",
      "Epoch: 5/10... Step: 15960... Loss: 1.5071... Val Loss: 1.3761\n",
      "Epoch: 5/10... Step: 15970... Loss: 1.3971... Val Loss: 1.3806\n",
      "Epoch: 5/10... Step: 15980... Loss: 1.4821... Val Loss: 1.3774\n",
      "Epoch: 5/10... Step: 15990... Loss: 1.5271... Val Loss: 1.3765\n",
      "Epoch: 5/10... Step: 16000... Loss: 1.4715... Val Loss: 1.3790\n",
      "Epoch: 5/10... Step: 16010... Loss: 1.4150... Val Loss: 1.3799\n",
      "Epoch: 5/10... Step: 16020... Loss: 1.5482... Val Loss: 1.3780\n",
      "Epoch: 5/10... Step: 16030... Loss: 1.4200... Val Loss: 1.3766\n",
      "Epoch: 5/10... Step: 16040... Loss: 1.3987... Val Loss: 1.3783\n",
      "Epoch: 5/10... Step: 16050... Loss: 1.3939... Val Loss: 1.3836\n",
      "Epoch: 5/10... Step: 16060... Loss: 1.3499... Val Loss: 1.3771\n",
      "Epoch: 5/10... Step: 16070... Loss: 1.4276... Val Loss: 1.3757\n",
      "Epoch: 5/10... Step: 16080... Loss: 1.5018... Val Loss: 1.3767\n",
      "Epoch: 5/10... Step: 16090... Loss: 1.4830... Val Loss: 1.3786\n",
      "Epoch: 5/10... Step: 16100... Loss: 1.4358... Val Loss: 1.3782\n",
      "Epoch: 5/10... Step: 16110... Loss: 1.4845... Val Loss: 1.3762\n",
      "Epoch: 5/10... Step: 16120... Loss: 1.3703... Val Loss: 1.3765\n",
      "Epoch: 5/10... Step: 16130... Loss: 1.4327... Val Loss: 1.3795\n",
      "Epoch: 5/10... Step: 16140... Loss: 1.4470... Val Loss: 1.3790\n",
      "Epoch: 5/10... Step: 16150... Loss: 1.4407... Val Loss: 1.3763\n",
      "Epoch: 5/10... Step: 16160... Loss: 1.5870... Val Loss: 1.3752\n",
      "Epoch: 5/10... Step: 16170... Loss: 1.4245... Val Loss: 1.3756\n",
      "Epoch: 5/10... Step: 16180... Loss: 1.4183... Val Loss: 1.3746\n",
      "Epoch: 5/10... Step: 16190... Loss: 1.4963... Val Loss: 1.3760\n",
      "Epoch: 5/10... Step: 16200... Loss: 1.4391... Val Loss: 1.3755\n",
      "Epoch: 5/10... Step: 16210... Loss: 1.4058... Val Loss: 1.3747\n",
      "Epoch: 5/10... Step: 16220... Loss: 1.3715... Val Loss: 1.3744\n",
      "Epoch: 5/10... Step: 16230... Loss: 1.3482... Val Loss: 1.3769\n",
      "Epoch: 5/10... Step: 16240... Loss: 1.6073... Val Loss: 1.3770\n",
      "Epoch: 5/10... Step: 16250... Loss: 1.4604... Val Loss: 1.3755\n",
      "Epoch: 5/10... Step: 16260... Loss: 1.3808... Val Loss: 1.3749\n",
      "Epoch: 5/10... Step: 16270... Loss: 1.4357... Val Loss: 1.3751\n",
      "Epoch: 5/10... Step: 16280... Loss: 1.4874... Val Loss: 1.3790\n",
      "Epoch: 5/10... Step: 16290... Loss: 1.4886... Val Loss: 1.3752\n",
      "Epoch: 5/10... Step: 16300... Loss: 1.3750... Val Loss: 1.3747\n",
      "Epoch: 5/10... Step: 16310... Loss: 1.4780... Val Loss: 1.3788\n",
      "Epoch: 5/10... Step: 16320... Loss: 1.5422... Val Loss: 1.3775\n",
      "Epoch: 5/10... Step: 16330... Loss: 1.4130... Val Loss: 1.3751\n",
      "Epoch: 5/10... Step: 16340... Loss: 1.4783... Val Loss: 1.3751\n",
      "Epoch: 5/10... Step: 16350... Loss: 1.5226... Val Loss: 1.3744\n",
      "Epoch: 5/10... Step: 16360... Loss: 1.4252... Val Loss: 1.3741\n",
      "Epoch: 5/10... Step: 16370... Loss: 1.4857... Val Loss: 1.3745\n",
      "Epoch: 5/10... Step: 16380... Loss: 1.4513... Val Loss: 1.3752\n",
      "Epoch: 5/10... Step: 16390... Loss: 1.5909... Val Loss: 1.3775\n",
      "Epoch: 5/10... Step: 16400... Loss: 1.4476... Val Loss: 1.3766\n",
      "Epoch: 5/10... Step: 16410... Loss: 1.4626... Val Loss: 1.3765\n",
      "Epoch: 5/10... Step: 16420... Loss: 1.4338... Val Loss: 1.3761\n",
      "Epoch: 5/10... Step: 16430... Loss: 1.4435... Val Loss: 1.3771\n",
      "Epoch: 5/10... Step: 16440... Loss: 1.5225... Val Loss: 1.3764\n",
      "Epoch: 5/10... Step: 16450... Loss: 1.5344... Val Loss: 1.3757\n",
      "Epoch: 5/10... Step: 16460... Loss: 1.5541... Val Loss: 1.3764\n",
      "Epoch: 5/10... Step: 16470... Loss: 1.4393... Val Loss: 1.3758\n",
      "Epoch: 5/10... Step: 16480... Loss: 1.5453... Val Loss: 1.3780\n",
      "Epoch: 5/10... Step: 16490... Loss: 1.5436... Val Loss: 1.3774\n",
      "Epoch: 5/10... Step: 16500... Loss: 1.3612... Val Loss: 1.3768\n",
      "Epoch: 5/10... Step: 16510... Loss: 1.3839... Val Loss: 1.3787\n",
      "Epoch: 5/10... Step: 16520... Loss: 1.5160... Val Loss: 1.3791\n",
      "Epoch: 5/10... Step: 16530... Loss: 1.4447... Val Loss: 1.3771\n",
      "Epoch: 5/10... Step: 16540... Loss: 1.4446... Val Loss: 1.3770\n",
      "Epoch: 5/10... Step: 16550... Loss: 1.4611... Val Loss: 1.3782\n",
      "Epoch: 5/10... Step: 16560... Loss: 1.4625... Val Loss: 1.3782\n",
      "Epoch: 5/10... Step: 16570... Loss: 1.5093... Val Loss: 1.3764\n",
      "Epoch: 5/10... Step: 16580... Loss: 1.4729... Val Loss: 1.3757\n",
      "Epoch: 5/10... Step: 16590... Loss: 1.4721... Val Loss: 1.3788\n",
      "Epoch: 5/10... Step: 16600... Loss: 1.2862... Val Loss: 1.3776\n",
      "Epoch: 5/10... Step: 16610... Loss: 1.4696... Val Loss: 1.3763\n",
      "Epoch: 5/10... Step: 16620... Loss: 1.4007... Val Loss: 1.3764\n",
      "Epoch: 5/10... Step: 16630... Loss: 1.3898... Val Loss: 1.3791\n",
      "Epoch: 5/10... Step: 16640... Loss: 1.4196... Val Loss: 1.3783\n",
      "Epoch: 5/10... Step: 16650... Loss: 1.4596... Val Loss: 1.3771\n",
      "Epoch: 5/10... Step: 16660... Loss: 1.4599... Val Loss: 1.3758\n",
      "Epoch: 5/10... Step: 16670... Loss: 1.4066... Val Loss: 1.3743\n",
      "Epoch: 5/10... Step: 16680... Loss: 1.4504... Val Loss: 1.3734\n",
      "Epoch: 5/10... Step: 16690... Loss: 1.4020... Val Loss: 1.3741\n",
      "Epoch: 5/10... Step: 16700... Loss: 1.4025... Val Loss: 1.3768\n",
      "Epoch: 5/10... Step: 16710... Loss: 1.3998... Val Loss: 1.3756\n",
      "Epoch: 5/10... Step: 16720... Loss: 1.3882... Val Loss: 1.3745\n",
      "Epoch: 5/10... Step: 16730... Loss: 1.4270... Val Loss: 1.3752\n",
      "Epoch: 5/10... Step: 16740... Loss: 1.4358... Val Loss: 1.3757\n",
      "Epoch: 5/10... Step: 16750... Loss: 1.4195... Val Loss: 1.3774\n",
      "Epoch: 5/10... Step: 16760... Loss: 1.4657... Val Loss: 1.3781\n",
      "Epoch: 5/10... Step: 16770... Loss: 1.5456... Val Loss: 1.3767\n",
      "Epoch: 5/10... Step: 16780... Loss: 1.4681... Val Loss: 1.3768\n",
      "Epoch: 5/10... Step: 16790... Loss: 1.5108... Val Loss: 1.3777\n",
      "Epoch: 5/10... Step: 16800... Loss: 1.4609... Val Loss: 1.3778\n",
      "Epoch: 5/10... Step: 16810... Loss: 1.4636... Val Loss: 1.3782\n",
      "Epoch: 5/10... Step: 16820... Loss: 1.4589... Val Loss: 1.3766\n",
      "Epoch: 5/10... Step: 16830... Loss: 1.2766... Val Loss: 1.3745\n",
      "Epoch: 5/10... Step: 16840... Loss: 1.4839... Val Loss: 1.3748\n",
      "Epoch: 5/10... Step: 16850... Loss: 1.3628... Val Loss: 1.3762\n",
      "Epoch: 5/10... Step: 16860... Loss: 1.3782... Val Loss: 1.3768\n",
      "Epoch: 5/10... Step: 16870... Loss: 1.4720... Val Loss: 1.3749\n",
      "Epoch: 5/10... Step: 16880... Loss: 1.5454... Val Loss: 1.3738\n",
      "Epoch: 5/10... Step: 16890... Loss: 1.4666... Val Loss: 1.3731\n",
      "Epoch: 5/10... Step: 16900... Loss: 1.4723... Val Loss: 1.3727\n",
      "Epoch: 6/10... Step: 16910... Loss: 1.4674... Val Loss: 1.3717\n",
      "Epoch: 6/10... Step: 16920... Loss: 1.3889... Val Loss: 1.3742\n",
      "Epoch: 6/10... Step: 16930... Loss: 1.4582... Val Loss: 1.3730\n",
      "Epoch: 6/10... Step: 16940... Loss: 1.4998... Val Loss: 1.3722\n",
      "Epoch: 6/10... Step: 16950... Loss: 1.3853... Val Loss: 1.3737\n",
      "Epoch: 6/10... Step: 16960... Loss: 1.4488... Val Loss: 1.3717\n",
      "Epoch: 6/10... Step: 16970... Loss: 1.4393... Val Loss: 1.3740\n",
      "Epoch: 6/10... Step: 16980... Loss: 1.3926... Val Loss: 1.3742\n",
      "Epoch: 6/10... Step: 16990... Loss: 1.4165... Val Loss: 1.3734\n",
      "Epoch: 6/10... Step: 17000... Loss: 1.4962... Val Loss: 1.3738\n",
      "Epoch: 6/10... Step: 17010... Loss: 1.4266... Val Loss: 1.3771\n",
      "Epoch: 6/10... Step: 17020... Loss: 1.4956... Val Loss: 1.3743\n",
      "Epoch: 6/10... Step: 17030... Loss: 1.4869... Val Loss: 1.3750\n",
      "Epoch: 6/10... Step: 17040... Loss: 1.4340... Val Loss: 1.3773\n",
      "Epoch: 6/10... Step: 17050... Loss: 1.4099... Val Loss: 1.3760\n",
      "Epoch: 6/10... Step: 17060... Loss: 1.5074... Val Loss: 1.3730\n",
      "Epoch: 6/10... Step: 17070... Loss: 1.4400... Val Loss: 1.3717\n",
      "Epoch: 6/10... Step: 17080... Loss: 1.4481... Val Loss: 1.3718\n",
      "Epoch: 6/10... Step: 17090... Loss: 1.4138... Val Loss: 1.3721\n",
      "Epoch: 6/10... Step: 17100... Loss: 1.5694... Val Loss: 1.3736\n",
      "Epoch: 6/10... Step: 17110... Loss: 1.4629... Val Loss: 1.3740\n",
      "Epoch: 6/10... Step: 17120... Loss: 1.5043... Val Loss: 1.3731\n",
      "Epoch: 6/10... Step: 17130... Loss: 1.4758... Val Loss: 1.3732\n",
      "Epoch: 6/10... Step: 17140... Loss: 1.3909... Val Loss: 1.3726\n",
      "Epoch: 6/10... Step: 17150... Loss: 1.4830... Val Loss: 1.3727\n",
      "Epoch: 6/10... Step: 17160... Loss: 1.3079... Val Loss: 1.3730\n",
      "Epoch: 6/10... Step: 17170... Loss: 1.4913... Val Loss: 1.3729\n",
      "Epoch: 6/10... Step: 17180... Loss: 1.5077... Val Loss: 1.3719\n",
      "Epoch: 6/10... Step: 17190... Loss: 1.4912... Val Loss: 1.3726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10... Step: 17200... Loss: 1.4066... Val Loss: 1.3739\n",
      "Epoch: 6/10... Step: 17210... Loss: 1.4909... Val Loss: 1.3742\n",
      "Epoch: 6/10... Step: 17220... Loss: 1.5177... Val Loss: 1.3739\n",
      "Epoch: 6/10... Step: 17230... Loss: 1.4150... Val Loss: 1.3749\n",
      "Epoch: 6/10... Step: 17240... Loss: 1.4769... Val Loss: 1.3743\n",
      "Epoch: 6/10... Step: 17250... Loss: 1.3961... Val Loss: 1.3740\n",
      "Epoch: 6/10... Step: 17260... Loss: 1.4924... Val Loss: 1.3738\n",
      "Epoch: 6/10... Step: 17270... Loss: 1.4861... Val Loss: 1.3716\n",
      "Epoch: 6/10... Step: 17280... Loss: 1.4352... Val Loss: 1.3718\n",
      "Epoch: 6/10... Step: 17290... Loss: 1.4392... Val Loss: 1.3718\n",
      "Epoch: 6/10... Step: 17300... Loss: 1.5415... Val Loss: 1.3717\n",
      "Epoch: 6/10... Step: 17310... Loss: 1.4851... Val Loss: 1.3733\n",
      "Epoch: 6/10... Step: 17320... Loss: 1.6152... Val Loss: 1.3734\n",
      "Epoch: 6/10... Step: 17330... Loss: 1.4209... Val Loss: 1.3712\n",
      "Epoch: 6/10... Step: 17340... Loss: 1.5389... Val Loss: 1.3709\n",
      "Epoch: 6/10... Step: 17350... Loss: 1.4406... Val Loss: 1.3704\n",
      "Epoch: 6/10... Step: 17360... Loss: 1.4843... Val Loss: 1.3701\n",
      "Epoch: 6/10... Step: 17370... Loss: 1.4974... Val Loss: 1.3694\n",
      "Epoch: 6/10... Step: 17380... Loss: 1.5374... Val Loss: 1.3692\n",
      "Epoch: 6/10... Step: 17390... Loss: 1.4023... Val Loss: 1.3685\n",
      "Epoch: 6/10... Step: 17400... Loss: 1.5180... Val Loss: 1.3692\n",
      "Epoch: 6/10... Step: 17410... Loss: 1.4359... Val Loss: 1.3714\n",
      "Epoch: 6/10... Step: 17420... Loss: 1.4425... Val Loss: 1.3709\n",
      "Epoch: 6/10... Step: 17430... Loss: 1.4561... Val Loss: 1.3685\n",
      "Epoch: 6/10... Step: 17440... Loss: 1.4280... Val Loss: 1.3687\n",
      "Epoch: 6/10... Step: 17450... Loss: 1.4272... Val Loss: 1.3694\n",
      "Epoch: 6/10... Step: 17460... Loss: 1.4095... Val Loss: 1.3697\n",
      "Epoch: 6/10... Step: 17470... Loss: 1.4903... Val Loss: 1.3710\n",
      "Epoch: 6/10... Step: 17480... Loss: 1.4081... Val Loss: 1.3717\n",
      "Epoch: 6/10... Step: 17490... Loss: 1.3981... Val Loss: 1.3706\n",
      "Epoch: 6/10... Step: 17500... Loss: 1.3482... Val Loss: 1.3702\n",
      "Epoch: 6/10... Step: 17510... Loss: 1.4112... Val Loss: 1.3695\n",
      "Epoch: 6/10... Step: 17520... Loss: 1.4404... Val Loss: 1.3696\n",
      "Epoch: 6/10... Step: 17530... Loss: 1.4538... Val Loss: 1.3710\n",
      "Epoch: 6/10... Step: 17540... Loss: 1.5276... Val Loss: 1.3738\n",
      "Epoch: 6/10... Step: 17550... Loss: 1.4258... Val Loss: 1.3707\n",
      "Epoch: 6/10... Step: 17560... Loss: 1.3997... Val Loss: 1.3695\n",
      "Epoch: 6/10... Step: 17570... Loss: 1.5296... Val Loss: 1.3688\n",
      "Epoch: 6/10... Step: 17580... Loss: 1.4180... Val Loss: 1.3682\n",
      "Epoch: 6/10... Step: 17590... Loss: 1.4226... Val Loss: 1.3686\n",
      "Epoch: 6/10... Step: 17600... Loss: 1.3805... Val Loss: 1.3704\n",
      "Epoch: 6/10... Step: 17610... Loss: 1.3952... Val Loss: 1.3721\n",
      "Epoch: 6/10... Step: 17620... Loss: 1.4174... Val Loss: 1.3732\n",
      "Epoch: 6/10... Step: 17630... Loss: 1.6095... Val Loss: 1.3706\n",
      "Epoch: 6/10... Step: 17640... Loss: 1.4852... Val Loss: 1.3684\n",
      "Epoch: 6/10... Step: 17650... Loss: 1.4849... Val Loss: 1.3673\n",
      "Epoch: 6/10... Step: 17660... Loss: 1.4737... Val Loss: 1.3673\n",
      "Epoch: 6/10... Step: 17670... Loss: 1.5344... Val Loss: 1.3673\n",
      "Epoch: 6/10... Step: 17680... Loss: 1.5813... Val Loss: 1.3664\n",
      "Epoch: 6/10... Step: 17690... Loss: 1.4214... Val Loss: 1.3670\n",
      "Epoch: 6/10... Step: 17700... Loss: 1.3888... Val Loss: 1.3667\n",
      "Epoch: 6/10... Step: 17710... Loss: 1.3922... Val Loss: 1.3680\n",
      "Epoch: 6/10... Step: 17720... Loss: 1.4538... Val Loss: 1.3688\n",
      "Epoch: 6/10... Step: 17730... Loss: 1.3831... Val Loss: 1.3682\n",
      "Epoch: 6/10... Step: 17740... Loss: 1.4184... Val Loss: 1.3695\n",
      "Epoch: 6/10... Step: 17750... Loss: 1.5118... Val Loss: 1.3704\n",
      "Epoch: 6/10... Step: 17760... Loss: 1.4386... Val Loss: 1.3683\n",
      "Epoch: 6/10... Step: 17770... Loss: 1.5440... Val Loss: 1.3676\n",
      "Epoch: 6/10... Step: 17780... Loss: 1.4374... Val Loss: 1.3693\n",
      "Epoch: 6/10... Step: 17790... Loss: 1.4118... Val Loss: 1.3696\n",
      "Epoch: 6/10... Step: 17800... Loss: 1.4847... Val Loss: 1.3677\n",
      "Epoch: 6/10... Step: 17810... Loss: 1.3908... Val Loss: 1.3681\n",
      "Epoch: 6/10... Step: 17820... Loss: 1.4925... Val Loss: 1.3684\n",
      "Epoch: 6/10... Step: 17830... Loss: 1.3976... Val Loss: 1.3678\n",
      "Epoch: 6/10... Step: 17840... Loss: 1.4625... Val Loss: 1.3699\n",
      "Epoch: 6/10... Step: 17850... Loss: 1.4390... Val Loss: 1.3689\n",
      "Epoch: 6/10... Step: 17860... Loss: 1.3971... Val Loss: 1.3686\n",
      "Epoch: 6/10... Step: 17870... Loss: 1.4819... Val Loss: 1.3689\n",
      "Epoch: 6/10... Step: 17880... Loss: 1.3794... Val Loss: 1.3711\n",
      "Epoch: 6/10... Step: 17890... Loss: 1.5527... Val Loss: 1.3719\n",
      "Epoch: 6/10... Step: 17900... Loss: 1.4472... Val Loss: 1.3711\n",
      "Epoch: 6/10... Step: 17910... Loss: 1.3426... Val Loss: 1.3708\n",
      "Epoch: 6/10... Step: 17920... Loss: 1.4345... Val Loss: 1.3685\n",
      "Epoch: 6/10... Step: 17930... Loss: 1.4550... Val Loss: 1.3685\n",
      "Epoch: 6/10... Step: 17940... Loss: 1.3506... Val Loss: 1.3704\n",
      "Epoch: 6/10... Step: 17950... Loss: 1.4745... Val Loss: 1.3708\n",
      "Epoch: 6/10... Step: 17960... Loss: 1.4447... Val Loss: 1.3705\n",
      "Epoch: 6/10... Step: 17970... Loss: 1.5022... Val Loss: 1.3704\n",
      "Epoch: 6/10... Step: 17980... Loss: 1.4660... Val Loss: 1.3705\n",
      "Epoch: 6/10... Step: 17990... Loss: 1.4970... Val Loss: 1.3683\n",
      "Epoch: 6/10... Step: 18000... Loss: 1.4941... Val Loss: 1.3672\n",
      "Epoch: 6/10... Step: 18010... Loss: 1.4643... Val Loss: 1.3670\n",
      "Epoch: 6/10... Step: 18020... Loss: 1.5728... Val Loss: 1.3694\n",
      "Epoch: 6/10... Step: 18030... Loss: 1.5567... Val Loss: 1.3682\n",
      "Epoch: 6/10... Step: 18040... Loss: 1.5313... Val Loss: 1.3671\n",
      "Epoch: 6/10... Step: 18050... Loss: 1.3039... Val Loss: 1.3667\n",
      "Epoch: 6/10... Step: 18060... Loss: 1.4220... Val Loss: 1.3684\n",
      "Epoch: 6/10... Step: 18070... Loss: 1.4544... Val Loss: 1.3679\n",
      "Epoch: 6/10... Step: 18080... Loss: 1.4722... Val Loss: 1.3681\n",
      "Epoch: 6/10... Step: 18090... Loss: 1.5319... Val Loss: 1.3679\n",
      "Epoch: 6/10... Step: 18100... Loss: 1.5038... Val Loss: 1.3679\n",
      "Epoch: 6/10... Step: 18110... Loss: 1.5362... Val Loss: 1.3691\n",
      "Epoch: 6/10... Step: 18120... Loss: 1.4490... Val Loss: 1.3688\n",
      "Epoch: 6/10... Step: 18130... Loss: 1.4343... Val Loss: 1.3687\n",
      "Epoch: 6/10... Step: 18140... Loss: 1.5117... Val Loss: 1.3684\n",
      "Epoch: 6/10... Step: 18150... Loss: 1.4821... Val Loss: 1.3677\n",
      "Epoch: 6/10... Step: 18160... Loss: 1.4430... Val Loss: 1.3669\n",
      "Epoch: 6/10... Step: 18170... Loss: 1.4209... Val Loss: 1.3666\n",
      "Epoch: 6/10... Step: 18180... Loss: 1.3883... Val Loss: 1.3661\n",
      "Epoch: 6/10... Step: 18190... Loss: 1.2895... Val Loss: 1.3669\n",
      "Epoch: 6/10... Step: 18200... Loss: 1.5491... Val Loss: 1.3667\n",
      "Epoch: 6/10... Step: 18210... Loss: 1.3877... Val Loss: 1.3661\n",
      "Epoch: 6/10... Step: 18220... Loss: 1.6410... Val Loss: 1.3657\n",
      "Epoch: 6/10... Step: 18230... Loss: 1.3875... Val Loss: 1.3661\n",
      "Epoch: 6/10... Step: 18240... Loss: 1.4691... Val Loss: 1.3656\n",
      "Epoch: 6/10... Step: 18250... Loss: 1.2619... Val Loss: 1.3672\n",
      "Epoch: 6/10... Step: 18260... Loss: 1.3658... Val Loss: 1.3678\n",
      "Epoch: 6/10... Step: 18270... Loss: 1.4053... Val Loss: 1.3673\n",
      "Epoch: 6/10... Step: 18280... Loss: 1.4785... Val Loss: 1.3672\n",
      "Epoch: 6/10... Step: 18290... Loss: 1.4370... Val Loss: 1.3688\n",
      "Epoch: 6/10... Step: 18300... Loss: 1.5008... Val Loss: 1.3683\n",
      "Epoch: 6/10... Step: 18310... Loss: 1.5146... Val Loss: 1.3706\n",
      "Epoch: 6/10... Step: 18320... Loss: 1.4843... Val Loss: 1.3720\n",
      "Epoch: 6/10... Step: 18330... Loss: 1.5288... Val Loss: 1.3716\n",
      "Epoch: 6/10... Step: 18340... Loss: 1.4907... Val Loss: 1.3698\n",
      "Epoch: 6/10... Step: 18350... Loss: 1.3326... Val Loss: 1.3685\n",
      "Epoch: 6/10... Step: 18360... Loss: 1.4754... Val Loss: 1.3683\n",
      "Epoch: 6/10... Step: 18370... Loss: 1.3771... Val Loss: 1.3711\n",
      "Epoch: 6/10... Step: 18380... Loss: 1.4659... Val Loss: 1.3703\n",
      "Epoch: 6/10... Step: 18390... Loss: 1.4722... Val Loss: 1.3668\n",
      "Epoch: 6/10... Step: 18400... Loss: 1.4820... Val Loss: 1.3682\n",
      "Epoch: 6/10... Step: 18410... Loss: 1.4158... Val Loss: 1.3690\n",
      "Epoch: 6/10... Step: 18420... Loss: 1.4406... Val Loss: 1.3682\n",
      "Epoch: 6/10... Step: 18430... Loss: 1.3939... Val Loss: 1.3669\n",
      "Epoch: 6/10... Step: 18440... Loss: 1.4266... Val Loss: 1.3653\n",
      "Epoch: 6/10... Step: 18450... Loss: 1.5539... Val Loss: 1.3654\n",
      "Epoch: 6/10... Step: 18460... Loss: 1.4193... Val Loss: 1.3661\n",
      "Epoch: 6/10... Step: 18470... Loss: 1.4627... Val Loss: 1.3672\n",
      "Epoch: 6/10... Step: 18480... Loss: 1.4233... Val Loss: 1.3682\n",
      "Epoch: 6/10... Step: 18490... Loss: 1.4057... Val Loss: 1.3700\n",
      "Epoch: 6/10... Step: 18500... Loss: 1.3995... Val Loss: 1.3712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10... Step: 18510... Loss: 1.4906... Val Loss: 1.3691\n",
      "Epoch: 6/10... Step: 18520... Loss: 1.3828... Val Loss: 1.3672\n",
      "Epoch: 6/10... Step: 18530... Loss: 1.3804... Val Loss: 1.3661\n",
      "Epoch: 6/10... Step: 18540... Loss: 1.4678... Val Loss: 1.3681\n",
      "Epoch: 6/10... Step: 18550... Loss: 1.4836... Val Loss: 1.3653\n",
      "Epoch: 6/10... Step: 18560... Loss: 1.4010... Val Loss: 1.3648\n",
      "Epoch: 6/10... Step: 18570... Loss: 1.4114... Val Loss: 1.3656\n",
      "Epoch: 6/10... Step: 18580... Loss: 1.4093... Val Loss: 1.3640\n",
      "Epoch: 6/10... Step: 18590... Loss: 1.4016... Val Loss: 1.3635\n",
      "Epoch: 6/10... Step: 18600... Loss: 1.4467... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 18610... Loss: 1.4186... Val Loss: 1.3631\n",
      "Epoch: 6/10... Step: 18620... Loss: 1.4369... Val Loss: 1.3641\n",
      "Epoch: 6/10... Step: 18630... Loss: 1.4745... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 18640... Loss: 1.5184... Val Loss: 1.3625\n",
      "Epoch: 6/10... Step: 18650... Loss: 1.4055... Val Loss: 1.3640\n",
      "Epoch: 6/10... Step: 18660... Loss: 1.4935... Val Loss: 1.3638\n",
      "Epoch: 6/10... Step: 18670... Loss: 1.4198... Val Loss: 1.3605\n",
      "Epoch: 6/10... Step: 18680... Loss: 1.3709... Val Loss: 1.3597\n",
      "Epoch: 6/10... Step: 18690... Loss: 1.5167... Val Loss: 1.3614\n",
      "Epoch: 6/10... Step: 18700... Loss: 1.4231... Val Loss: 1.3613\n",
      "Epoch: 6/10... Step: 18710... Loss: 1.3958... Val Loss: 1.3607\n",
      "Epoch: 6/10... Step: 18720... Loss: 1.4459... Val Loss: 1.3612\n",
      "Epoch: 6/10... Step: 18730... Loss: 1.4651... Val Loss: 1.3613\n",
      "Epoch: 6/10... Step: 18740... Loss: 1.4518... Val Loss: 1.3610\n",
      "Epoch: 6/10... Step: 18750... Loss: 1.5371... Val Loss: 1.3639\n",
      "Epoch: 6/10... Step: 18760... Loss: 1.4246... Val Loss: 1.3635\n",
      "Epoch: 6/10... Step: 18770... Loss: 1.4546... Val Loss: 1.3641\n",
      "Epoch: 6/10... Step: 18780... Loss: 1.4710... Val Loss: 1.3643\n",
      "Epoch: 6/10... Step: 18790... Loss: 1.4752... Val Loss: 1.3686\n",
      "Epoch: 6/10... Step: 18800... Loss: 1.3463... Val Loss: 1.3692\n",
      "Epoch: 6/10... Step: 18810... Loss: 1.3567... Val Loss: 1.3657\n",
      "Epoch: 6/10... Step: 18820... Loss: 1.4041... Val Loss: 1.3649\n",
      "Epoch: 6/10... Step: 18830... Loss: 1.4050... Val Loss: 1.3658\n",
      "Epoch: 6/10... Step: 18840... Loss: 1.3438... Val Loss: 1.3645\n",
      "Epoch: 6/10... Step: 18850... Loss: 1.3415... Val Loss: 1.3629\n",
      "Epoch: 6/10... Step: 18860... Loss: 1.3493... Val Loss: 1.3624\n",
      "Epoch: 6/10... Step: 18870... Loss: 1.4796... Val Loss: 1.3623\n",
      "Epoch: 6/10... Step: 18880... Loss: 1.4401... Val Loss: 1.3624\n",
      "Epoch: 6/10... Step: 18890... Loss: 1.4744... Val Loss: 1.3638\n",
      "Epoch: 6/10... Step: 18900... Loss: 1.4155... Val Loss: 1.3667\n",
      "Epoch: 6/10... Step: 18910... Loss: 1.4002... Val Loss: 1.3651\n",
      "Epoch: 6/10... Step: 18920... Loss: 1.4810... Val Loss: 1.3641\n",
      "Epoch: 6/10... Step: 18930... Loss: 1.3118... Val Loss: 1.3663\n",
      "Epoch: 6/10... Step: 18940... Loss: 1.5692... Val Loss: 1.3660\n",
      "Epoch: 6/10... Step: 18950... Loss: 1.4656... Val Loss: 1.3644\n",
      "Epoch: 6/10... Step: 18960... Loss: 1.3733... Val Loss: 1.3639\n",
      "Epoch: 6/10... Step: 18970... Loss: 1.4181... Val Loss: 1.3669\n",
      "Epoch: 6/10... Step: 18980... Loss: 1.5125... Val Loss: 1.3644\n",
      "Epoch: 6/10... Step: 18990... Loss: 1.4434... Val Loss: 1.3641\n",
      "Epoch: 6/10... Step: 19000... Loss: 1.4014... Val Loss: 1.3651\n",
      "Epoch: 6/10... Step: 19010... Loss: 1.5313... Val Loss: 1.3652\n",
      "Epoch: 6/10... Step: 19020... Loss: 1.5365... Val Loss: 1.3640\n",
      "Epoch: 6/10... Step: 19030... Loss: 1.4539... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 19040... Loss: 1.5106... Val Loss: 1.3641\n",
      "Epoch: 6/10... Step: 19050... Loss: 1.4735... Val Loss: 1.3641\n",
      "Epoch: 6/10... Step: 19060... Loss: 1.5216... Val Loss: 1.3654\n",
      "Epoch: 6/10... Step: 19070... Loss: 1.5265... Val Loss: 1.3629\n",
      "Epoch: 6/10... Step: 19080... Loss: 1.4300... Val Loss: 1.3624\n",
      "Epoch: 6/10... Step: 19090... Loss: 1.5516... Val Loss: 1.3626\n",
      "Epoch: 6/10... Step: 19100... Loss: 1.4727... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 19110... Loss: 1.5386... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 19120... Loss: 1.4981... Val Loss: 1.3638\n",
      "Epoch: 6/10... Step: 19130... Loss: 1.4293... Val Loss: 1.3636\n",
      "Epoch: 6/10... Step: 19140... Loss: 1.5629... Val Loss: 1.3643\n",
      "Epoch: 6/10... Step: 19150... Loss: 1.4241... Val Loss: 1.3655\n",
      "Epoch: 6/10... Step: 19160... Loss: 1.4670... Val Loss: 1.3641\n",
      "Epoch: 6/10... Step: 19170... Loss: 1.3939... Val Loss: 1.3632\n",
      "Epoch: 6/10... Step: 19180... Loss: 1.4778... Val Loss: 1.3620\n",
      "Epoch: 6/10... Step: 19190... Loss: 1.4930... Val Loss: 1.3617\n",
      "Epoch: 6/10... Step: 19200... Loss: 1.4254... Val Loss: 1.3624\n",
      "Epoch: 6/10... Step: 19210... Loss: 1.4895... Val Loss: 1.3621\n",
      "Epoch: 6/10... Step: 19220... Loss: 1.4109... Val Loss: 1.3617\n",
      "Epoch: 6/10... Step: 19230... Loss: 1.3743... Val Loss: 1.3606\n",
      "Epoch: 6/10... Step: 19240... Loss: 1.4197... Val Loss: 1.3598\n",
      "Epoch: 6/10... Step: 19250... Loss: 1.5282... Val Loss: 1.3618\n",
      "Epoch: 6/10... Step: 19260... Loss: 1.4871... Val Loss: 1.3597\n",
      "Epoch: 6/10... Step: 19270... Loss: 1.6457... Val Loss: 1.3590\n",
      "Epoch: 6/10... Step: 19280... Loss: 1.4689... Val Loss: 1.3611\n",
      "Epoch: 6/10... Step: 19290... Loss: 1.4719... Val Loss: 1.3609\n",
      "Epoch: 6/10... Step: 19300... Loss: 1.4787... Val Loss: 1.3615\n",
      "Epoch: 6/10... Step: 19310... Loss: 1.3855... Val Loss: 1.3700\n",
      "Epoch: 6/10... Step: 19320... Loss: 1.4826... Val Loss: 1.3656\n",
      "Epoch: 6/10... Step: 19330... Loss: 1.4705... Val Loss: 1.3623\n",
      "Epoch: 6/10... Step: 19340... Loss: 1.5168... Val Loss: 1.3619\n",
      "Epoch: 6/10... Step: 19350... Loss: 1.4919... Val Loss: 1.3607\n",
      "Epoch: 6/10... Step: 19360... Loss: 1.4603... Val Loss: 1.3617\n",
      "Epoch: 6/10... Step: 19370... Loss: 1.4261... Val Loss: 1.3602\n",
      "Epoch: 6/10... Step: 19380... Loss: 1.5017... Val Loss: 1.3615\n",
      "Epoch: 6/10... Step: 19390... Loss: 1.3854... Val Loss: 1.3634\n",
      "Epoch: 6/10... Step: 19400... Loss: 1.3927... Val Loss: 1.3605\n",
      "Epoch: 6/10... Step: 19410... Loss: 1.4776... Val Loss: 1.3593\n",
      "Epoch: 6/10... Step: 19420... Loss: 1.4275... Val Loss: 1.3604\n",
      "Epoch: 6/10... Step: 19430... Loss: 1.4066... Val Loss: 1.3658\n",
      "Epoch: 6/10... Step: 19440... Loss: 1.4863... Val Loss: 1.3638\n",
      "Epoch: 6/10... Step: 19450... Loss: 1.3454... Val Loss: 1.3607\n",
      "Epoch: 6/10... Step: 19460... Loss: 1.3645... Val Loss: 1.3595\n",
      "Epoch: 6/10... Step: 19470... Loss: 1.3568... Val Loss: 1.3606\n",
      "Epoch: 6/10... Step: 19480... Loss: 1.4812... Val Loss: 1.3620\n",
      "Epoch: 6/10... Step: 19490... Loss: 1.4090... Val Loss: 1.3618\n",
      "Epoch: 6/10... Step: 19500... Loss: 1.4268... Val Loss: 1.3618\n",
      "Epoch: 6/10... Step: 19510... Loss: 1.4002... Val Loss: 1.3634\n",
      "Epoch: 6/10... Step: 19520... Loss: 1.4649... Val Loss: 1.3659\n",
      "Epoch: 6/10... Step: 19530... Loss: 1.4267... Val Loss: 1.3624\n",
      "Epoch: 6/10... Step: 19540... Loss: 1.4350... Val Loss: 1.3616\n",
      "Epoch: 6/10... Step: 19550... Loss: 1.4746... Val Loss: 1.3607\n",
      "Epoch: 6/10... Step: 19560... Loss: 1.3893... Val Loss: 1.3615\n",
      "Epoch: 6/10... Step: 19570... Loss: 1.4655... Val Loss: 1.3617\n",
      "Epoch: 6/10... Step: 19580... Loss: 1.3301... Val Loss: 1.3599\n",
      "Epoch: 6/10... Step: 19590... Loss: 1.5511... Val Loss: 1.3596\n",
      "Epoch: 6/10... Step: 19600... Loss: 1.4881... Val Loss: 1.3597\n",
      "Epoch: 6/10... Step: 19610... Loss: 1.3053... Val Loss: 1.3616\n",
      "Epoch: 6/10... Step: 19620... Loss: 1.4950... Val Loss: 1.3608\n",
      "Epoch: 6/10... Step: 19630... Loss: 1.4409... Val Loss: 1.3604\n",
      "Epoch: 6/10... Step: 19640... Loss: 1.4965... Val Loss: 1.3612\n",
      "Epoch: 6/10... Step: 19650... Loss: 1.4044... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 19660... Loss: 1.4097... Val Loss: 1.3611\n",
      "Epoch: 6/10... Step: 19670... Loss: 1.3483... Val Loss: 1.3607\n",
      "Epoch: 6/10... Step: 19680... Loss: 1.5740... Val Loss: 1.3604\n",
      "Epoch: 6/10... Step: 19690... Loss: 1.4410... Val Loss: 1.3635\n",
      "Epoch: 6/10... Step: 19700... Loss: 1.5059... Val Loss: 1.3635\n",
      "Epoch: 6/10... Step: 19710... Loss: 1.4187... Val Loss: 1.3621\n",
      "Epoch: 6/10... Step: 19720... Loss: 1.4509... Val Loss: 1.3610\n",
      "Epoch: 6/10... Step: 19730... Loss: 1.4456... Val Loss: 1.3603\n",
      "Epoch: 6/10... Step: 19740... Loss: 1.4758... Val Loss: 1.3608\n",
      "Epoch: 6/10... Step: 19750... Loss: 1.4050... Val Loss: 1.3618\n",
      "Epoch: 6/10... Step: 19760... Loss: 1.5627... Val Loss: 1.3626\n",
      "Epoch: 6/10... Step: 19770... Loss: 1.4448... Val Loss: 1.3614\n",
      "Epoch: 6/10... Step: 19780... Loss: 1.4742... Val Loss: 1.3613\n",
      "Epoch: 6/10... Step: 19790... Loss: 1.4399... Val Loss: 1.3600\n",
      "Epoch: 6/10... Step: 19800... Loss: 1.4650... Val Loss: 1.3607\n",
      "Epoch: 6/10... Step: 19810... Loss: 1.4604... Val Loss: 1.3604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10... Step: 19820... Loss: 1.5150... Val Loss: 1.3609\n",
      "Epoch: 6/10... Step: 19830... Loss: 1.4110... Val Loss: 1.3602\n",
      "Epoch: 6/10... Step: 19840... Loss: 1.4262... Val Loss: 1.3601\n",
      "Epoch: 6/10... Step: 19850... Loss: 1.3484... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 19860... Loss: 1.4542... Val Loss: 1.3645\n",
      "Epoch: 6/10... Step: 19870... Loss: 1.4957... Val Loss: 1.3634\n",
      "Epoch: 6/10... Step: 19880... Loss: 1.4784... Val Loss: 1.3626\n",
      "Epoch: 6/10... Step: 19890... Loss: 1.3954... Val Loss: 1.3630\n",
      "Epoch: 6/10... Step: 19900... Loss: 1.4842... Val Loss: 1.3623\n",
      "Epoch: 6/10... Step: 19910... Loss: 1.5109... Val Loss: 1.3635\n",
      "Epoch: 6/10... Step: 19920... Loss: 1.3701... Val Loss: 1.3633\n",
      "Epoch: 6/10... Step: 19930... Loss: 1.4914... Val Loss: 1.3643\n",
      "Epoch: 6/10... Step: 19940... Loss: 1.4343... Val Loss: 1.3639\n",
      "Epoch: 6/10... Step: 19950... Loss: 1.4134... Val Loss: 1.3605\n",
      "Epoch: 6/10... Step: 19960... Loss: 1.3991... Val Loss: 1.3603\n",
      "Epoch: 6/10... Step: 19970... Loss: 1.3269... Val Loss: 1.3631\n",
      "Epoch: 6/10... Step: 19980... Loss: 1.5142... Val Loss: 1.3622\n",
      "Epoch: 6/10... Step: 19990... Loss: 1.3148... Val Loss: 1.3626\n",
      "Epoch: 6/10... Step: 20000... Loss: 1.4377... Val Loss: 1.3615\n",
      "Epoch: 6/10... Step: 20010... Loss: 1.4454... Val Loss: 1.3615\n",
      "Epoch: 6/10... Step: 20020... Loss: 1.4443... Val Loss: 1.3636\n",
      "Epoch: 6/10... Step: 20030... Loss: 1.4736... Val Loss: 1.3635\n",
      "Epoch: 6/10... Step: 20040... Loss: 1.5007... Val Loss: 1.3604\n",
      "Epoch: 6/10... Step: 20050... Loss: 1.3574... Val Loss: 1.3594\n",
      "Epoch: 6/10... Step: 20060... Loss: 1.4355... Val Loss: 1.3606\n",
      "Epoch: 6/10... Step: 20070... Loss: 1.4417... Val Loss: 1.3611\n",
      "Epoch: 6/10... Step: 20080... Loss: 1.3783... Val Loss: 1.3621\n",
      "Epoch: 6/10... Step: 20090... Loss: 1.4142... Val Loss: 1.3617\n",
      "Epoch: 6/10... Step: 20100... Loss: 1.4767... Val Loss: 1.3606\n",
      "Epoch: 6/10... Step: 20110... Loss: 1.4123... Val Loss: 1.3621\n",
      "Epoch: 6/10... Step: 20120... Loss: 1.3625... Val Loss: 1.3654\n",
      "Epoch: 6/10... Step: 20130... Loss: 1.3913... Val Loss: 1.3648\n",
      "Epoch: 6/10... Step: 20140... Loss: 1.4620... Val Loss: 1.3639\n",
      "Epoch: 6/10... Step: 20150... Loss: 1.4371... Val Loss: 1.3643\n",
      "Epoch: 6/10... Step: 20160... Loss: 1.5171... Val Loss: 1.3634\n",
      "Epoch: 6/10... Step: 20170... Loss: 1.3850... Val Loss: 1.3621\n",
      "Epoch: 6/10... Step: 20180... Loss: 1.3194... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 20190... Loss: 1.4245... Val Loss: 1.3648\n",
      "Epoch: 6/10... Step: 20200... Loss: 1.4580... Val Loss: 1.3630\n",
      "Epoch: 6/10... Step: 20210... Loss: 1.4221... Val Loss: 1.3602\n",
      "Epoch: 6/10... Step: 20220... Loss: 1.3567... Val Loss: 1.3617\n",
      "Epoch: 6/10... Step: 20230... Loss: 1.3977... Val Loss: 1.3627\n",
      "Epoch: 6/10... Step: 20240... Loss: 1.3537... Val Loss: 1.3640\n",
      "Epoch: 6/10... Step: 20250... Loss: 1.3662... Val Loss: 1.3629\n",
      "Epoch: 6/10... Step: 20260... Loss: 1.4293... Val Loss: 1.3632\n",
      "Epoch: 6/10... Step: 20270... Loss: 1.3498... Val Loss: 1.3646\n",
      "Epoch: 6/10... Step: 20280... Loss: 1.3969... Val Loss: 1.3607\n",
      "Epoch: 7/10... Step: 20290... Loss: 1.4354... Val Loss: 1.3587\n",
      "Epoch: 7/10... Step: 20300... Loss: 1.3525... Val Loss: 1.3611\n",
      "Epoch: 7/10... Step: 20310... Loss: 1.3829... Val Loss: 1.3609\n",
      "Epoch: 7/10... Step: 20320... Loss: 1.4144... Val Loss: 1.3605\n",
      "Epoch: 7/10... Step: 20330... Loss: 1.3726... Val Loss: 1.3595\n",
      "Epoch: 7/10... Step: 20340... Loss: 1.4064... Val Loss: 1.3612\n",
      "Epoch: 7/10... Step: 20350... Loss: 1.3858... Val Loss: 1.3623\n",
      "Epoch: 7/10... Step: 20360... Loss: 1.3871... Val Loss: 1.3617\n",
      "Epoch: 7/10... Step: 20370... Loss: 1.4359... Val Loss: 1.3616\n",
      "Epoch: 7/10... Step: 20380... Loss: 1.4102... Val Loss: 1.3618\n",
      "Epoch: 7/10... Step: 20390... Loss: 1.4292... Val Loss: 1.3635\n",
      "Epoch: 7/10... Step: 20400... Loss: 1.4673... Val Loss: 1.3606\n",
      "Epoch: 7/10... Step: 20410... Loss: 1.4616... Val Loss: 1.3616\n",
      "Epoch: 7/10... Step: 20420... Loss: 1.4942... Val Loss: 1.3630\n",
      "Epoch: 7/10... Step: 20430... Loss: 1.4729... Val Loss: 1.3626\n",
      "Epoch: 7/10... Step: 20440... Loss: 1.4448... Val Loss: 1.3613\n",
      "Epoch: 7/10... Step: 20450... Loss: 1.3551... Val Loss: 1.3607\n",
      "Epoch: 7/10... Step: 20460... Loss: 1.2931... Val Loss: 1.3604\n",
      "Epoch: 7/10... Step: 20470... Loss: 1.4158... Val Loss: 1.3614\n",
      "Epoch: 7/10... Step: 20480... Loss: 1.4511... Val Loss: 1.3631\n",
      "Epoch: 7/10... Step: 20490... Loss: 1.4757... Val Loss: 1.3641\n",
      "Epoch: 7/10... Step: 20500... Loss: 1.4835... Val Loss: 1.3634\n",
      "Epoch: 7/10... Step: 20510... Loss: 1.4091... Val Loss: 1.3634\n",
      "Epoch: 7/10... Step: 20520... Loss: 1.3426... Val Loss: 1.3618\n",
      "Epoch: 7/10... Step: 20530... Loss: 1.2983... Val Loss: 1.3603\n",
      "Epoch: 7/10... Step: 20540... Loss: 1.3794... Val Loss: 1.3604\n",
      "Epoch: 7/10... Step: 20550... Loss: 1.4309... Val Loss: 1.3632\n",
      "Epoch: 7/10... Step: 20560... Loss: 1.4384... Val Loss: 1.3624\n",
      "Epoch: 7/10... Step: 20570... Loss: 1.5122... Val Loss: 1.3613\n",
      "Epoch: 7/10... Step: 20580... Loss: 1.4059... Val Loss: 1.3604\n",
      "Epoch: 7/10... Step: 20590... Loss: 1.4688... Val Loss: 1.3625\n",
      "Epoch: 7/10... Step: 20600... Loss: 1.4423... Val Loss: 1.3609\n",
      "Epoch: 7/10... Step: 20610... Loss: 1.5001... Val Loss: 1.3592\n",
      "Epoch: 7/10... Step: 20620... Loss: 1.4652... Val Loss: 1.3606\n",
      "Epoch: 7/10... Step: 20630... Loss: 1.3228... Val Loss: 1.3609\n",
      "Epoch: 7/10... Step: 20640... Loss: 1.5016... Val Loss: 1.3611\n",
      "Epoch: 7/10... Step: 20650... Loss: 1.4280... Val Loss: 1.3597\n",
      "Epoch: 7/10... Step: 20660... Loss: 1.5402... Val Loss: 1.3594\n",
      "Epoch: 7/10... Step: 20670... Loss: 1.5459... Val Loss: 1.3597\n",
      "Epoch: 7/10... Step: 20680... Loss: 1.4123... Val Loss: 1.3596\n",
      "Epoch: 7/10... Step: 20690... Loss: 1.4187... Val Loss: 1.3595\n",
      "Epoch: 7/10... Step: 20700... Loss: 1.4759... Val Loss: 1.3629\n",
      "Epoch: 7/10... Step: 20710... Loss: 1.5414... Val Loss: 1.3612\n",
      "Epoch: 7/10... Step: 20720... Loss: 1.4044... Val Loss: 1.3607\n",
      "Epoch: 7/10... Step: 20730... Loss: 1.4325... Val Loss: 1.3603\n",
      "Epoch: 7/10... Step: 20740... Loss: 1.4212... Val Loss: 1.3608\n",
      "Epoch: 7/10... Step: 20750... Loss: 1.4466... Val Loss: 1.3592\n",
      "Epoch: 7/10... Step: 20760... Loss: 1.4462... Val Loss: 1.3596\n",
      "Epoch: 7/10... Step: 20770... Loss: 1.3464... Val Loss: 1.3606\n",
      "Epoch: 7/10... Step: 20780... Loss: 1.5251... Val Loss: 1.3595\n",
      "Epoch: 7/10... Step: 20790... Loss: 1.4486... Val Loss: 1.3594\n",
      "Epoch: 7/10... Step: 20800... Loss: 1.4850... Val Loss: 1.3610\n",
      "Epoch: 7/10... Step: 20810... Loss: 1.4624... Val Loss: 1.3588\n",
      "Epoch: 7/10... Step: 20820... Loss: 1.4796... Val Loss: 1.3584\n",
      "Epoch: 7/10... Step: 20830... Loss: 1.3807... Val Loss: 1.3580\n",
      "Epoch: 7/10... Step: 20840... Loss: 1.4215... Val Loss: 1.3589\n",
      "Epoch: 7/10... Step: 20850... Loss: 1.3758... Val Loss: 1.3570\n",
      "Epoch: 7/10... Step: 20860... Loss: 1.4850... Val Loss: 1.3571\n",
      "Epoch: 7/10... Step: 20870... Loss: 1.4048... Val Loss: 1.3560\n",
      "Epoch: 7/10... Step: 20880... Loss: 1.4444... Val Loss: 1.3561\n",
      "Epoch: 7/10... Step: 20890... Loss: 1.4595... Val Loss: 1.3572\n",
      "Epoch: 7/10... Step: 20900... Loss: 1.3585... Val Loss: 1.3579\n",
      "Epoch: 7/10... Step: 20910... Loss: 1.3832... Val Loss: 1.3589\n",
      "Epoch: 7/10... Step: 20920... Loss: 1.4664... Val Loss: 1.3605\n",
      "Epoch: 7/10... Step: 20930... Loss: 1.4136... Val Loss: 1.3570\n",
      "Epoch: 7/10... Step: 20940... Loss: 1.4557... Val Loss: 1.3559\n",
      "Epoch: 7/10... Step: 20950... Loss: 1.4240... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 20960... Loss: 1.4129... Val Loss: 1.3540\n",
      "Epoch: 7/10... Step: 20970... Loss: 1.4458... Val Loss: 1.3545\n",
      "Epoch: 7/10... Step: 20980... Loss: 1.3377... Val Loss: 1.3572\n",
      "Epoch: 7/10... Step: 20990... Loss: 1.3403... Val Loss: 1.3586\n",
      "Epoch: 7/10... Step: 21000... Loss: 1.3788... Val Loss: 1.3599\n",
      "Epoch: 7/10... Step: 21010... Loss: 1.2933... Val Loss: 1.3593\n",
      "Epoch: 7/10... Step: 21020... Loss: 1.4381... Val Loss: 1.3579\n",
      "Epoch: 7/10... Step: 21030... Loss: 1.5200... Val Loss: 1.3582\n",
      "Epoch: 7/10... Step: 21040... Loss: 1.4705... Val Loss: 1.3581\n",
      "Epoch: 7/10... Step: 21050... Loss: 1.4843... Val Loss: 1.3603\n",
      "Epoch: 7/10... Step: 21060... Loss: 1.5429... Val Loss: 1.3593\n",
      "Epoch: 7/10... Step: 21070... Loss: 1.4494... Val Loss: 1.3579\n",
      "Epoch: 7/10... Step: 21080... Loss: 1.4199... Val Loss: 1.3572\n",
      "Epoch: 7/10... Step: 21090... Loss: 1.4788... Val Loss: 1.3580\n",
      "Epoch: 7/10... Step: 21100... Loss: 1.4229... Val Loss: 1.3590\n",
      "Epoch: 7/10... Step: 21110... Loss: 1.3697... Val Loss: 1.3575\n",
      "Epoch: 7/10... Step: 21120... Loss: 1.4308... Val Loss: 1.3595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10... Step: 21130... Loss: 1.5627... Val Loss: 1.3591\n",
      "Epoch: 7/10... Step: 21140... Loss: 1.3775... Val Loss: 1.3593\n",
      "Epoch: 7/10... Step: 21150... Loss: 1.4239... Val Loss: 1.3597\n",
      "Epoch: 7/10... Step: 21160... Loss: 1.5100... Val Loss: 1.3581\n",
      "Epoch: 7/10... Step: 21170... Loss: 1.3387... Val Loss: 1.3584\n",
      "Epoch: 7/10... Step: 21180... Loss: 1.3777... Val Loss: 1.3595\n",
      "Epoch: 7/10... Step: 21190... Loss: 1.4095... Val Loss: 1.3603\n",
      "Epoch: 7/10... Step: 21200... Loss: 1.5091... Val Loss: 1.3605\n",
      "Epoch: 7/10... Step: 21210... Loss: 1.4375... Val Loss: 1.3603\n",
      "Epoch: 7/10... Step: 21220... Loss: 1.5015... Val Loss: 1.3603\n",
      "Epoch: 7/10... Step: 21230... Loss: 1.3939... Val Loss: 1.3592\n",
      "Epoch: 7/10... Step: 21240... Loss: 1.4422... Val Loss: 1.3583\n",
      "Epoch: 7/10... Step: 21250... Loss: 1.4735... Val Loss: 1.3590\n",
      "Epoch: 7/10... Step: 21260... Loss: 1.3010... Val Loss: 1.3627\n",
      "Epoch: 7/10... Step: 21270... Loss: 1.3855... Val Loss: 1.3604\n",
      "Epoch: 7/10... Step: 21280... Loss: 1.4112... Val Loss: 1.3606\n",
      "Epoch: 7/10... Step: 21290... Loss: 1.4315... Val Loss: 1.3608\n",
      "Epoch: 7/10... Step: 21300... Loss: 1.4201... Val Loss: 1.3598\n",
      "Epoch: 7/10... Step: 21310... Loss: 1.3948... Val Loss: 1.3596\n",
      "Epoch: 7/10... Step: 21320... Loss: 1.4562... Val Loss: 1.3586\n",
      "Epoch: 7/10... Step: 21330... Loss: 1.5410... Val Loss: 1.3610\n",
      "Epoch: 7/10... Step: 21340... Loss: 1.4586... Val Loss: 1.3614\n",
      "Epoch: 7/10... Step: 21350... Loss: 1.4357... Val Loss: 1.3582\n",
      "Epoch: 7/10... Step: 21360... Loss: 1.4007... Val Loss: 1.3572\n",
      "Epoch: 7/10... Step: 21370... Loss: 1.3749... Val Loss: 1.3569\n",
      "Epoch: 7/10... Step: 21380... Loss: 1.3546... Val Loss: 1.3565\n",
      "Epoch: 7/10... Step: 21390... Loss: 1.4056... Val Loss: 1.3569\n",
      "Epoch: 7/10... Step: 21400... Loss: 1.4028... Val Loss: 1.3571\n",
      "Epoch: 7/10... Step: 21410... Loss: 1.4314... Val Loss: 1.3569\n",
      "Epoch: 7/10... Step: 21420... Loss: 1.4460... Val Loss: 1.3545\n",
      "Epoch: 7/10... Step: 21430... Loss: 1.3256... Val Loss: 1.3542\n",
      "Epoch: 7/10... Step: 21440... Loss: 1.4465... Val Loss: 1.3568\n",
      "Epoch: 7/10... Step: 21450... Loss: 1.5094... Val Loss: 1.3571\n",
      "Epoch: 7/10... Step: 21460... Loss: 1.4557... Val Loss: 1.3563\n",
      "Epoch: 7/10... Step: 21470... Loss: 1.5031... Val Loss: 1.3567\n",
      "Epoch: 7/10... Step: 21480... Loss: 1.3665... Val Loss: 1.3567\n",
      "Epoch: 7/10... Step: 21490... Loss: 1.4458... Val Loss: 1.3569\n",
      "Epoch: 7/10... Step: 21500... Loss: 1.4722... Val Loss: 1.3577\n",
      "Epoch: 7/10... Step: 21510... Loss: 1.4470... Val Loss: 1.3569\n",
      "Epoch: 7/10... Step: 21520... Loss: 1.3887... Val Loss: 1.3549\n",
      "Epoch: 7/10... Step: 21530... Loss: 1.4897... Val Loss: 1.3541\n",
      "Epoch: 7/10... Step: 21540... Loss: 1.4325... Val Loss: 1.3539\n",
      "Epoch: 7/10... Step: 21550... Loss: 1.4312... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 21560... Loss: 1.5456... Val Loss: 1.3561\n",
      "Epoch: 7/10... Step: 21570... Loss: 1.4185... Val Loss: 1.3591\n",
      "Epoch: 7/10... Step: 21580... Loss: 1.4819... Val Loss: 1.3559\n",
      "Epoch: 7/10... Step: 21590... Loss: 1.4867... Val Loss: 1.3553\n",
      "Epoch: 7/10... Step: 21600... Loss: 1.5110... Val Loss: 1.3542\n",
      "Epoch: 7/10... Step: 21610... Loss: 1.4694... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 21620... Loss: 1.4409... Val Loss: 1.3524\n",
      "Epoch: 7/10... Step: 21630... Loss: 1.4719... Val Loss: 1.3528\n",
      "Epoch: 7/10... Step: 21640... Loss: 1.3694... Val Loss: 1.3526\n",
      "Epoch: 7/10... Step: 21650... Loss: 1.5506... Val Loss: 1.3545\n",
      "Epoch: 7/10... Step: 21660... Loss: 1.4690... Val Loss: 1.3527\n",
      "Epoch: 7/10... Step: 21670... Loss: 1.4715... Val Loss: 1.3532\n",
      "Epoch: 7/10... Step: 21680... Loss: 1.5711... Val Loss: 1.3540\n",
      "Epoch: 7/10... Step: 21690... Loss: 1.3858... Val Loss: 1.3542\n",
      "Epoch: 7/10... Step: 21700... Loss: 1.4955... Val Loss: 1.3568\n",
      "Epoch: 7/10... Step: 21710... Loss: 1.4879... Val Loss: 1.3585\n",
      "Epoch: 7/10... Step: 21720... Loss: 1.4129... Val Loss: 1.3561\n",
      "Epoch: 7/10... Step: 21730... Loss: 1.4126... Val Loss: 1.3544\n",
      "Epoch: 7/10... Step: 21740... Loss: 1.3634... Val Loss: 1.3548\n",
      "Epoch: 7/10... Step: 21750... Loss: 1.5666... Val Loss: 1.3589\n",
      "Epoch: 7/10... Step: 21760... Loss: 1.4042... Val Loss: 1.3603\n",
      "Epoch: 7/10... Step: 21770... Loss: 1.4080... Val Loss: 1.3563\n",
      "Epoch: 7/10... Step: 21780... Loss: 1.4397... Val Loss: 1.3573\n",
      "Epoch: 7/10... Step: 21790... Loss: 1.3853... Val Loss: 1.3604\n",
      "Epoch: 7/10... Step: 21800... Loss: 1.3772... Val Loss: 1.3607\n",
      "Epoch: 7/10... Step: 21810... Loss: 1.4494... Val Loss: 1.3598\n",
      "Epoch: 7/10... Step: 21820... Loss: 1.3474... Val Loss: 1.3568\n",
      "Epoch: 7/10... Step: 21830... Loss: 1.4075... Val Loss: 1.3551\n",
      "Epoch: 7/10... Step: 21840... Loss: 1.5719... Val Loss: 1.3534\n",
      "Epoch: 7/10... Step: 21850... Loss: 1.4508... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 21860... Loss: 1.4207... Val Loss: 1.3558\n",
      "Epoch: 7/10... Step: 21870... Loss: 1.5670... Val Loss: 1.3580\n",
      "Epoch: 7/10... Step: 21880... Loss: 1.3484... Val Loss: 1.3563\n",
      "Epoch: 7/10... Step: 21890... Loss: 1.4280... Val Loss: 1.3561\n",
      "Epoch: 7/10... Step: 21900... Loss: 1.3819... Val Loss: 1.3544\n",
      "Epoch: 7/10... Step: 21910... Loss: 1.4847... Val Loss: 1.3532\n",
      "Epoch: 7/10... Step: 21920... Loss: 1.4091... Val Loss: 1.3547\n",
      "Epoch: 7/10... Step: 21930... Loss: 1.4089... Val Loss: 1.3543\n",
      "Epoch: 7/10... Step: 21940... Loss: 1.3404... Val Loss: 1.3536\n",
      "Epoch: 7/10... Step: 21950... Loss: 1.3912... Val Loss: 1.3538\n",
      "Epoch: 7/10... Step: 21960... Loss: 1.4040... Val Loss: 1.3551\n",
      "Epoch: 7/10... Step: 21970... Loss: 1.4237... Val Loss: 1.3557\n",
      "Epoch: 7/10... Step: 21980... Loss: 1.5474... Val Loss: 1.3553\n",
      "Epoch: 7/10... Step: 21990... Loss: 1.4391... Val Loss: 1.3534\n",
      "Epoch: 7/10... Step: 22000... Loss: 1.4474... Val Loss: 1.3528\n",
      "Epoch: 7/10... Step: 22010... Loss: 1.4163... Val Loss: 1.3542\n",
      "Epoch: 7/10... Step: 22020... Loss: 1.5242... Val Loss: 1.3525\n",
      "Epoch: 7/10... Step: 22030... Loss: 1.4054... Val Loss: 1.3541\n",
      "Epoch: 7/10... Step: 22040... Loss: 1.4617... Val Loss: 1.3548\n",
      "Epoch: 7/10... Step: 22050... Loss: 1.3766... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 22060... Loss: 1.5254... Val Loss: 1.3532\n",
      "Epoch: 7/10... Step: 22070... Loss: 1.4337... Val Loss: 1.3533\n",
      "Epoch: 7/10... Step: 22080... Loss: 1.3968... Val Loss: 1.3530\n",
      "Epoch: 7/10... Step: 22090... Loss: 1.3814... Val Loss: 1.3522\n",
      "Epoch: 7/10... Step: 22100... Loss: 1.4165... Val Loss: 1.3523\n",
      "Epoch: 7/10... Step: 22110... Loss: 1.4766... Val Loss: 1.3522\n",
      "Epoch: 7/10... Step: 22120... Loss: 1.4529... Val Loss: 1.3524\n",
      "Epoch: 7/10... Step: 22130... Loss: 1.4445... Val Loss: 1.3545\n",
      "Epoch: 7/10... Step: 22140... Loss: 1.3671... Val Loss: 1.3551\n",
      "Epoch: 7/10... Step: 22150... Loss: 1.3581... Val Loss: 1.3548\n",
      "Epoch: 7/10... Step: 22160... Loss: 1.3556... Val Loss: 1.3546\n",
      "Epoch: 7/10... Step: 22170... Loss: 1.4013... Val Loss: 1.3545\n",
      "Epoch: 7/10... Step: 22180... Loss: 1.4437... Val Loss: 1.3560\n",
      "Epoch: 7/10... Step: 22190... Loss: 1.3715... Val Loss: 1.3555\n",
      "Epoch: 7/10... Step: 22200... Loss: 1.2922... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 22210... Loss: 1.5882... Val Loss: 1.3553\n",
      "Epoch: 7/10... Step: 22220... Loss: 1.4694... Val Loss: 1.3543\n",
      "Epoch: 7/10... Step: 22230... Loss: 1.3447... Val Loss: 1.3522\n",
      "Epoch: 7/10... Step: 22240... Loss: 1.3965... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 22250... Loss: 1.3971... Val Loss: 1.3524\n",
      "Epoch: 7/10... Step: 22260... Loss: 1.4042... Val Loss: 1.3523\n",
      "Epoch: 7/10... Step: 22270... Loss: 1.4524... Val Loss: 1.3534\n",
      "Epoch: 7/10... Step: 22280... Loss: 1.3795... Val Loss: 1.3549\n",
      "Epoch: 7/10... Step: 22290... Loss: 1.3689... Val Loss: 1.3533\n",
      "Epoch: 7/10... Step: 22300... Loss: 1.3979... Val Loss: 1.3531\n",
      "Epoch: 7/10... Step: 22310... Loss: 1.3870... Val Loss: 1.3546\n",
      "Epoch: 7/10... Step: 22320... Loss: 1.4147... Val Loss: 1.3536\n",
      "Epoch: 7/10... Step: 22330... Loss: 1.4693... Val Loss: 1.3507\n",
      "Epoch: 7/10... Step: 22340... Loss: 1.4967... Val Loss: 1.3531\n",
      "Epoch: 7/10... Step: 22350... Loss: 1.3518... Val Loss: 1.3535\n",
      "Epoch: 7/10... Step: 22360... Loss: 1.4586... Val Loss: 1.3525\n",
      "Epoch: 7/10... Step: 22370... Loss: 1.4689... Val Loss: 1.3527\n",
      "Epoch: 7/10... Step: 22380... Loss: 1.3762... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 22390... Loss: 1.3953... Val Loss: 1.3540\n",
      "Epoch: 7/10... Step: 22400... Loss: 1.4006... Val Loss: 1.3526\n",
      "Epoch: 7/10... Step: 22410... Loss: 1.4531... Val Loss: 1.3533\n",
      "Epoch: 7/10... Step: 22420... Loss: 1.4822... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 22430... Loss: 1.4354... Val Loss: 1.3516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10... Step: 22440... Loss: 1.3825... Val Loss: 1.3502\n",
      "Epoch: 7/10... Step: 22450... Loss: 1.3807... Val Loss: 1.3506\n",
      "Epoch: 7/10... Step: 22460... Loss: 1.5623... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 22470... Loss: 1.4136... Val Loss: 1.3543\n",
      "Epoch: 7/10... Step: 22480... Loss: 1.4097... Val Loss: 1.3525\n",
      "Epoch: 7/10... Step: 22490... Loss: 1.4830... Val Loss: 1.3526\n",
      "Epoch: 7/10... Step: 22500... Loss: 1.4531... Val Loss: 1.3535\n",
      "Epoch: 7/10... Step: 22510... Loss: 1.3814... Val Loss: 1.3540\n",
      "Epoch: 7/10... Step: 22520... Loss: 1.4306... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 22530... Loss: 1.3940... Val Loss: 1.3551\n",
      "Epoch: 7/10... Step: 22540... Loss: 1.4097... Val Loss: 1.3542\n",
      "Epoch: 7/10... Step: 22550... Loss: 1.5212... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 22560... Loss: 1.4149... Val Loss: 1.3540\n",
      "Epoch: 7/10... Step: 22570... Loss: 1.3617... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 22580... Loss: 1.4362... Val Loss: 1.3527\n",
      "Epoch: 7/10... Step: 22590... Loss: 1.3770... Val Loss: 1.3521\n",
      "Epoch: 7/10... Step: 22600... Loss: 1.3223... Val Loss: 1.3520\n",
      "Epoch: 7/10... Step: 22610... Loss: 1.3428... Val Loss: 1.3512\n",
      "Epoch: 7/10... Step: 22620... Loss: 1.4226... Val Loss: 1.3513\n",
      "Epoch: 7/10... Step: 22630... Loss: 1.5019... Val Loss: 1.3505\n",
      "Epoch: 7/10... Step: 22640... Loss: 1.5165... Val Loss: 1.3489\n",
      "Epoch: 7/10... Step: 22650... Loss: 1.4086... Val Loss: 1.3489\n",
      "Epoch: 7/10... Step: 22660... Loss: 1.5722... Val Loss: 1.3504\n",
      "Epoch: 7/10... Step: 22670... Loss: 1.4776... Val Loss: 1.3524\n",
      "Epoch: 7/10... Step: 22680... Loss: 1.5365... Val Loss: 1.3520\n",
      "Epoch: 7/10... Step: 22690... Loss: 1.3860... Val Loss: 1.3531\n",
      "Epoch: 7/10... Step: 22700... Loss: 1.3607... Val Loss: 1.3545\n",
      "Epoch: 7/10... Step: 22710... Loss: 1.4495... Val Loss: 1.3533\n",
      "Epoch: 7/10... Step: 22720... Loss: 1.3870... Val Loss: 1.3525\n",
      "Epoch: 7/10... Step: 22730... Loss: 1.4594... Val Loss: 1.3522\n",
      "Epoch: 7/10... Step: 22740... Loss: 1.5258... Val Loss: 1.3546\n",
      "Epoch: 7/10... Step: 22750... Loss: 1.5340... Val Loss: 1.3510\n",
      "Epoch: 7/10... Step: 22760... Loss: 1.4296... Val Loss: 1.3545\n",
      "Epoch: 7/10... Step: 22770... Loss: 1.5487... Val Loss: 1.3534\n",
      "Epoch: 7/10... Step: 22780... Loss: 1.3969... Val Loss: 1.3521\n",
      "Epoch: 7/10... Step: 22790... Loss: 1.4389... Val Loss: 1.3531\n",
      "Epoch: 7/10... Step: 22800... Loss: 1.2896... Val Loss: 1.3539\n",
      "Epoch: 7/10... Step: 22810... Loss: 1.4043... Val Loss: 1.3536\n",
      "Epoch: 7/10... Step: 22820... Loss: 1.4453... Val Loss: 1.3515\n",
      "Epoch: 7/10... Step: 22830... Loss: 1.3716... Val Loss: 1.3491\n",
      "Epoch: 7/10... Step: 22840... Loss: 1.3232... Val Loss: 1.3489\n",
      "Epoch: 7/10... Step: 22850... Loss: 1.3947... Val Loss: 1.3498\n",
      "Epoch: 7/10... Step: 22860... Loss: 1.4193... Val Loss: 1.3522\n",
      "Epoch: 7/10... Step: 22870... Loss: 1.4229... Val Loss: 1.3523\n",
      "Epoch: 7/10... Step: 22880... Loss: 1.3491... Val Loss: 1.3512\n",
      "Epoch: 7/10... Step: 22890... Loss: 1.4126... Val Loss: 1.3513\n",
      "Epoch: 7/10... Step: 22900... Loss: 1.4416... Val Loss: 1.3522\n",
      "Epoch: 7/10... Step: 22910... Loss: 1.5742... Val Loss: 1.3542\n",
      "Epoch: 7/10... Step: 22920... Loss: 1.3799... Val Loss: 1.3519\n",
      "Epoch: 7/10... Step: 22930... Loss: 1.4702... Val Loss: 1.3520\n",
      "Epoch: 7/10... Step: 22940... Loss: 1.4794... Val Loss: 1.3513\n",
      "Epoch: 7/10... Step: 22950... Loss: 1.4488... Val Loss: 1.3523\n",
      "Epoch: 7/10... Step: 22960... Loss: 1.3795... Val Loss: 1.3511\n",
      "Epoch: 7/10... Step: 22970... Loss: 1.3769... Val Loss: 1.3514\n",
      "Epoch: 7/10... Step: 22980... Loss: 1.3814... Val Loss: 1.3533\n",
      "Epoch: 7/10... Step: 22990... Loss: 1.4711... Val Loss: 1.3532\n",
      "Epoch: 7/10... Step: 23000... Loss: 1.3353... Val Loss: 1.3541\n",
      "Epoch: 7/10... Step: 23010... Loss: 1.4776... Val Loss: 1.3546\n",
      "Epoch: 7/10... Step: 23020... Loss: 1.3877... Val Loss: 1.3544\n",
      "Epoch: 7/10... Step: 23030... Loss: 1.4497... Val Loss: 1.3541\n",
      "Epoch: 7/10... Step: 23040... Loss: 1.4802... Val Loss: 1.3541\n",
      "Epoch: 7/10... Step: 23050... Loss: 1.3339... Val Loss: 1.3535\n",
      "Epoch: 7/10... Step: 23060... Loss: 1.4674... Val Loss: 1.3535\n",
      "Epoch: 7/10... Step: 23070... Loss: 1.3749... Val Loss: 1.3566\n",
      "Epoch: 7/10... Step: 23080... Loss: 1.4226... Val Loss: 1.3546\n",
      "Epoch: 7/10... Step: 23090... Loss: 1.4927... Val Loss: 1.3527\n",
      "Epoch: 7/10... Step: 23100... Loss: 1.4737... Val Loss: 1.3525\n",
      "Epoch: 7/10... Step: 23110... Loss: 1.4138... Val Loss: 1.3522\n",
      "Epoch: 7/10... Step: 23120... Loss: 1.3674... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 23130... Loss: 1.4129... Val Loss: 1.3535\n",
      "Epoch: 7/10... Step: 23140... Loss: 1.5213... Val Loss: 1.3521\n",
      "Epoch: 7/10... Step: 23150... Loss: 1.4095... Val Loss: 1.3509\n",
      "Epoch: 7/10... Step: 23160... Loss: 1.3020... Val Loss: 1.3509\n",
      "Epoch: 7/10... Step: 23170... Loss: 1.4001... Val Loss: 1.3507\n",
      "Epoch: 7/10... Step: 23180... Loss: 1.4815... Val Loss: 1.3516\n",
      "Epoch: 7/10... Step: 23190... Loss: 1.3965... Val Loss: 1.3514\n",
      "Epoch: 7/10... Step: 23200... Loss: 1.3636... Val Loss: 1.3532\n",
      "Epoch: 7/10... Step: 23210... Loss: 1.3315... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 23220... Loss: 1.3469... Val Loss: 1.3524\n",
      "Epoch: 7/10... Step: 23230... Loss: 1.4516... Val Loss: 1.3524\n",
      "Epoch: 7/10... Step: 23240... Loss: 1.4678... Val Loss: 1.3536\n",
      "Epoch: 7/10... Step: 23250... Loss: 1.3743... Val Loss: 1.3521\n",
      "Epoch: 7/10... Step: 23260... Loss: 1.4204... Val Loss: 1.3512\n",
      "Epoch: 7/10... Step: 23270... Loss: 1.3608... Val Loss: 1.3509\n",
      "Epoch: 7/10... Step: 23280... Loss: 1.3784... Val Loss: 1.3512\n",
      "Epoch: 7/10... Step: 23290... Loss: 1.3908... Val Loss: 1.3514\n",
      "Epoch: 7/10... Step: 23300... Loss: 1.3668... Val Loss: 1.3507\n",
      "Epoch: 7/10... Step: 23310... Loss: 1.4199... Val Loss: 1.3516\n",
      "Epoch: 7/10... Step: 23320... Loss: 1.5149... Val Loss: 1.3556\n",
      "Epoch: 7/10... Step: 23330... Loss: 1.4011... Val Loss: 1.3504\n",
      "Epoch: 7/10... Step: 23340... Loss: 1.4096... Val Loss: 1.3500\n",
      "Epoch: 7/10... Step: 23350... Loss: 1.4953... Val Loss: 1.3521\n",
      "Epoch: 7/10... Step: 23360... Loss: 1.3519... Val Loss: 1.3526\n",
      "Epoch: 7/10... Step: 23370... Loss: 1.5255... Val Loss: 1.3520\n",
      "Epoch: 7/10... Step: 23380... Loss: 1.3853... Val Loss: 1.3516\n",
      "Epoch: 7/10... Step: 23390... Loss: 1.3702... Val Loss: 1.3508\n",
      "Epoch: 7/10... Step: 23400... Loss: 1.3868... Val Loss: 1.3511\n",
      "Epoch: 7/10... Step: 23410... Loss: 1.4238... Val Loss: 1.3509\n",
      "Epoch: 7/10... Step: 23420... Loss: 1.4667... Val Loss: 1.3505\n",
      "Epoch: 7/10... Step: 23430... Loss: 1.5309... Val Loss: 1.3490\n",
      "Epoch: 7/10... Step: 23440... Loss: 1.4781... Val Loss: 1.3502\n",
      "Epoch: 7/10... Step: 23450... Loss: 1.3774... Val Loss: 1.3501\n",
      "Epoch: 7/10... Step: 23460... Loss: 1.4975... Val Loss: 1.3515\n",
      "Epoch: 7/10... Step: 23470... Loss: 1.4001... Val Loss: 1.3522\n",
      "Epoch: 7/10... Step: 23480... Loss: 1.3387... Val Loss: 1.3516\n",
      "Epoch: 7/10... Step: 23490... Loss: 1.3606... Val Loss: 1.3506\n",
      "Epoch: 7/10... Step: 23500... Loss: 1.4570... Val Loss: 1.3515\n",
      "Epoch: 7/10... Step: 23510... Loss: 1.3837... Val Loss: 1.3532\n",
      "Epoch: 7/10... Step: 23520... Loss: 1.4735... Val Loss: 1.3560\n",
      "Epoch: 7/10... Step: 23530... Loss: 1.4098... Val Loss: 1.3515\n",
      "Epoch: 7/10... Step: 23540... Loss: 1.4656... Val Loss: 1.3504\n",
      "Epoch: 7/10... Step: 23550... Loss: 1.5889... Val Loss: 1.3495\n",
      "Epoch: 7/10... Step: 23560... Loss: 1.4039... Val Loss: 1.3497\n",
      "Epoch: 7/10... Step: 23570... Loss: 1.4325... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 23580... Loss: 1.3816... Val Loss: 1.3530\n",
      "Epoch: 7/10... Step: 23590... Loss: 1.4539... Val Loss: 1.3506\n",
      "Epoch: 7/10... Step: 23600... Loss: 1.4667... Val Loss: 1.3516\n",
      "Epoch: 7/10... Step: 23610... Loss: 1.2781... Val Loss: 1.3532\n",
      "Epoch: 7/10... Step: 23620... Loss: 1.5007... Val Loss: 1.3550\n",
      "Epoch: 7/10... Step: 23630... Loss: 1.3799... Val Loss: 1.3529\n",
      "Epoch: 7/10... Step: 23640... Loss: 1.3670... Val Loss: 1.3521\n",
      "Epoch: 7/10... Step: 23650... Loss: 1.3493... Val Loss: 1.3512\n",
      "Epoch: 7/10... Step: 23660... Loss: 1.3464... Val Loss: 1.3509\n",
      "Epoch: 8/10... Step: 23670... Loss: 1.4071... Val Loss: 1.3506\n",
      "Epoch: 8/10... Step: 23680... Loss: 1.4575... Val Loss: 1.3490\n",
      "Epoch: 8/10... Step: 23690... Loss: 1.3988... Val Loss: 1.3496\n",
      "Epoch: 8/10... Step: 23700... Loss: 1.3731... Val Loss: 1.3500\n",
      "Epoch: 8/10... Step: 23710... Loss: 1.4880... Val Loss: 1.3502\n",
      "Epoch: 8/10... Step: 23720... Loss: 1.4253... Val Loss: 1.3522\n",
      "Epoch: 8/10... Step: 23730... Loss: 1.3694... Val Loss: 1.3531\n",
      "Epoch: 8/10... Step: 23740... Loss: 1.4963... Val Loss: 1.3522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10... Step: 23750... Loss: 1.3550... Val Loss: 1.3517\n",
      "Epoch: 8/10... Step: 23760... Loss: 1.4816... Val Loss: 1.3511\n",
      "Epoch: 8/10... Step: 23770... Loss: 1.4986... Val Loss: 1.3499\n",
      "Epoch: 8/10... Step: 23780... Loss: 1.4283... Val Loss: 1.3520\n",
      "Epoch: 8/10... Step: 23790... Loss: 1.3714... Val Loss: 1.3528\n",
      "Epoch: 8/10... Step: 23800... Loss: 1.4268... Val Loss: 1.3542\n",
      "Epoch: 8/10... Step: 23810... Loss: 1.3784... Val Loss: 1.3547\n",
      "Epoch: 8/10... Step: 23820... Loss: 1.4303... Val Loss: 1.3510\n",
      "Epoch: 8/10... Step: 23830... Loss: 1.4212... Val Loss: 1.3502\n",
      "Epoch: 8/10... Step: 23840... Loss: 1.3794... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 23850... Loss: 1.4529... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 23860... Loss: 1.3628... Val Loss: 1.3505\n",
      "Epoch: 8/10... Step: 23870... Loss: 1.4028... Val Loss: 1.3545\n",
      "Epoch: 8/10... Step: 23880... Loss: 1.4895... Val Loss: 1.3529\n",
      "Epoch: 8/10... Step: 23890... Loss: 1.4889... Val Loss: 1.3511\n",
      "Epoch: 8/10... Step: 23900... Loss: 1.3813... Val Loss: 1.3505\n",
      "Epoch: 8/10... Step: 23910... Loss: 1.4210... Val Loss: 1.3507\n",
      "Epoch: 8/10... Step: 23920... Loss: 1.3523... Val Loss: 1.3516\n",
      "Epoch: 8/10... Step: 23930... Loss: 1.4112... Val Loss: 1.3509\n",
      "Epoch: 8/10... Step: 23940... Loss: 1.4345... Val Loss: 1.3520\n",
      "Epoch: 8/10... Step: 23950... Loss: 1.3960... Val Loss: 1.3501\n",
      "Epoch: 8/10... Step: 23960... Loss: 1.5104... Val Loss: 1.3509\n",
      "Epoch: 8/10... Step: 23970... Loss: 1.4133... Val Loss: 1.3517\n",
      "Epoch: 8/10... Step: 23980... Loss: 1.5061... Val Loss: 1.3508\n",
      "Epoch: 8/10... Step: 23990... Loss: 1.3759... Val Loss: 1.3501\n",
      "Epoch: 8/10... Step: 24000... Loss: 1.5182... Val Loss: 1.3511\n",
      "Epoch: 8/10... Step: 24010... Loss: 1.4420... Val Loss: 1.3518\n",
      "Epoch: 8/10... Step: 24020... Loss: 1.3013... Val Loss: 1.3521\n",
      "Epoch: 8/10... Step: 24030... Loss: 1.4737... Val Loss: 1.3510\n",
      "Epoch: 8/10... Step: 24040... Loss: 1.4784... Val Loss: 1.3515\n",
      "Epoch: 8/10... Step: 24050... Loss: 1.4552... Val Loss: 1.3504\n",
      "Epoch: 8/10... Step: 24060... Loss: 1.4891... Val Loss: 1.3506\n",
      "Epoch: 8/10... Step: 24070... Loss: 1.3961... Val Loss: 1.3514\n",
      "Epoch: 8/10... Step: 24080... Loss: 1.5480... Val Loss: 1.3538\n",
      "Epoch: 8/10... Step: 24090... Loss: 1.4211... Val Loss: 1.3518\n",
      "Epoch: 8/10... Step: 24100... Loss: 1.4259... Val Loss: 1.3504\n",
      "Epoch: 8/10... Step: 24110... Loss: 1.4692... Val Loss: 1.3490\n",
      "Epoch: 8/10... Step: 24120... Loss: 1.4149... Val Loss: 1.3481\n",
      "Epoch: 8/10... Step: 24130... Loss: 1.4349... Val Loss: 1.3480\n",
      "Epoch: 8/10... Step: 24140... Loss: 1.4038... Val Loss: 1.3472\n",
      "Epoch: 8/10... Step: 24150... Loss: 1.3221... Val Loss: 1.3476\n",
      "Epoch: 8/10... Step: 24160... Loss: 1.3808... Val Loss: 1.3488\n",
      "Epoch: 8/10... Step: 24170... Loss: 1.4804... Val Loss: 1.3490\n",
      "Epoch: 8/10... Step: 24180... Loss: 1.3505... Val Loss: 1.3478\n",
      "Epoch: 8/10... Step: 24190... Loss: 1.4024... Val Loss: 1.3476\n",
      "Epoch: 8/10... Step: 24200... Loss: 1.4627... Val Loss: 1.3473\n",
      "Epoch: 8/10... Step: 24210... Loss: 1.3713... Val Loss: 1.3478\n",
      "Epoch: 8/10... Step: 24220... Loss: 1.4286... Val Loss: 1.3482\n",
      "Epoch: 8/10... Step: 24230... Loss: 1.4818... Val Loss: 1.3491\n",
      "Epoch: 8/10... Step: 24240... Loss: 1.4878... Val Loss: 1.3500\n",
      "Epoch: 8/10... Step: 24250... Loss: 1.3008... Val Loss: 1.3485\n",
      "Epoch: 8/10... Step: 24260... Loss: 1.4731... Val Loss: 1.3481\n",
      "Epoch: 8/10... Step: 24270... Loss: 1.4180... Val Loss: 1.3471\n",
      "Epoch: 8/10... Step: 24280... Loss: 1.4963... Val Loss: 1.3475\n",
      "Epoch: 8/10... Step: 24290... Loss: 1.5299... Val Loss: 1.3489\n",
      "Epoch: 8/10... Step: 24300... Loss: 1.4076... Val Loss: 1.3508\n",
      "Epoch: 8/10... Step: 24310... Loss: 1.4264... Val Loss: 1.3476\n",
      "Epoch: 8/10... Step: 24320... Loss: 1.4108... Val Loss: 1.3470\n",
      "Epoch: 8/10... Step: 24330... Loss: 1.3384... Val Loss: 1.3475\n",
      "Epoch: 8/10... Step: 24340... Loss: 1.3735... Val Loss: 1.3481\n",
      "Epoch: 8/10... Step: 24350... Loss: 1.3379... Val Loss: 1.3465\n",
      "Epoch: 8/10... Step: 24360... Loss: 1.4873... Val Loss: 1.3485\n",
      "Epoch: 8/10... Step: 24370... Loss: 1.3376... Val Loss: 1.3491\n",
      "Epoch: 8/10... Step: 24380... Loss: 1.4343... Val Loss: 1.3492\n",
      "Epoch: 8/10... Step: 24390... Loss: 1.4814... Val Loss: 1.3498\n",
      "Epoch: 8/10... Step: 24400... Loss: 1.3671... Val Loss: 1.3501\n",
      "Epoch: 8/10... Step: 24410... Loss: 1.4481... Val Loss: 1.3504\n",
      "Epoch: 8/10... Step: 24420... Loss: 1.4243... Val Loss: 1.3483\n",
      "Epoch: 8/10... Step: 24430... Loss: 1.5470... Val Loss: 1.3470\n",
      "Epoch: 8/10... Step: 24440... Loss: 1.4264... Val Loss: 1.3474\n",
      "Epoch: 8/10... Step: 24450... Loss: 1.5744... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 24460... Loss: 1.4073... Val Loss: 1.3486\n",
      "Epoch: 8/10... Step: 24470... Loss: 1.3986... Val Loss: 1.3471\n",
      "Epoch: 8/10... Step: 24480... Loss: 1.4345... Val Loss: 1.3466\n",
      "Epoch: 8/10... Step: 24490... Loss: 1.4347... Val Loss: 1.3456\n",
      "Epoch: 8/10... Step: 24500... Loss: 1.4150... Val Loss: 1.3486\n",
      "Epoch: 8/10... Step: 24510... Loss: 1.4046... Val Loss: 1.3509\n",
      "Epoch: 8/10... Step: 24520... Loss: 1.4109... Val Loss: 1.3511\n",
      "Epoch: 8/10... Step: 24530... Loss: 1.3344... Val Loss: 1.3504\n",
      "Epoch: 8/10... Step: 24540... Loss: 1.5400... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 24550... Loss: 1.4034... Val Loss: 1.3483\n",
      "Epoch: 8/10... Step: 24560... Loss: 1.4643... Val Loss: 1.3481\n",
      "Epoch: 8/10... Step: 24570... Loss: 1.2939... Val Loss: 1.3484\n",
      "Epoch: 8/10... Step: 24580... Loss: 1.3761... Val Loss: 1.3490\n",
      "Epoch: 8/10... Step: 24590... Loss: 1.3375... Val Loss: 1.3485\n",
      "Epoch: 8/10... Step: 24600... Loss: 1.4242... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 24610... Loss: 1.3998... Val Loss: 1.3503\n",
      "Epoch: 8/10... Step: 24620... Loss: 1.2991... Val Loss: 1.3500\n",
      "Epoch: 8/10... Step: 24630... Loss: 1.4014... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 24640... Loss: 1.4516... Val Loss: 1.3503\n",
      "Epoch: 8/10... Step: 24650... Loss: 1.3254... Val Loss: 1.3516\n",
      "Epoch: 8/10... Step: 24660... Loss: 1.4113... Val Loss: 1.3531\n",
      "Epoch: 8/10... Step: 24670... Loss: 1.4044... Val Loss: 1.3519\n",
      "Epoch: 8/10... Step: 24680... Loss: 1.4230... Val Loss: 1.3507\n",
      "Epoch: 8/10... Step: 24690... Loss: 1.3700... Val Loss: 1.3506\n",
      "Epoch: 8/10... Step: 24700... Loss: 1.3763... Val Loss: 1.3503\n",
      "Epoch: 8/10... Step: 24710... Loss: 1.4345... Val Loss: 1.3492\n",
      "Epoch: 8/10... Step: 24720... Loss: 1.3891... Val Loss: 1.3493\n",
      "Epoch: 8/10... Step: 24730... Loss: 1.5144... Val Loss: 1.3485\n",
      "Epoch: 8/10... Step: 24740... Loss: 1.3814... Val Loss: 1.3498\n",
      "Epoch: 8/10... Step: 24750... Loss: 1.4629... Val Loss: 1.3501\n",
      "Epoch: 8/10... Step: 24760... Loss: 1.3657... Val Loss: 1.3477\n",
      "Epoch: 8/10... Step: 24770... Loss: 1.4892... Val Loss: 1.3478\n",
      "Epoch: 8/10... Step: 24780... Loss: 1.3679... Val Loss: 1.3481\n",
      "Epoch: 8/10... Step: 24790... Loss: 1.5421... Val Loss: 1.3471\n",
      "Epoch: 8/10... Step: 24800... Loss: 1.4646... Val Loss: 1.3463\n",
      "Epoch: 8/10... Step: 24810... Loss: 1.3885... Val Loss: 1.3485\n",
      "Epoch: 8/10... Step: 24820... Loss: 1.4018... Val Loss: 1.3493\n",
      "Epoch: 8/10... Step: 24830... Loss: 1.4355... Val Loss: 1.3502\n",
      "Epoch: 8/10... Step: 24840... Loss: 1.4343... Val Loss: 1.3486\n",
      "Epoch: 8/10... Step: 24850... Loss: 1.4055... Val Loss: 1.3468\n",
      "Epoch: 8/10... Step: 24860... Loss: 1.4679... Val Loss: 1.3463\n",
      "Epoch: 8/10... Step: 24870... Loss: 1.2875... Val Loss: 1.3500\n",
      "Epoch: 8/10... Step: 24880... Loss: 1.3779... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 24890... Loss: 1.3662... Val Loss: 1.3477\n",
      "Epoch: 8/10... Step: 24900... Loss: 1.3817... Val Loss: 1.3474\n",
      "Epoch: 8/10... Step: 24910... Loss: 1.4461... Val Loss: 1.3472\n",
      "Epoch: 8/10... Step: 24920... Loss: 1.4638... Val Loss: 1.3458\n",
      "Epoch: 8/10... Step: 24930... Loss: 1.3751... Val Loss: 1.3480\n",
      "Epoch: 8/10... Step: 24940... Loss: 1.3228... Val Loss: 1.3465\n",
      "Epoch: 8/10... Step: 24950... Loss: 1.5077... Val Loss: 1.3468\n",
      "Epoch: 8/10... Step: 24960... Loss: 1.3484... Val Loss: 1.3461\n",
      "Epoch: 8/10... Step: 24970... Loss: 1.4026... Val Loss: 1.3464\n",
      "Epoch: 8/10... Step: 24980... Loss: 1.4200... Val Loss: 1.3454\n",
      "Epoch: 8/10... Step: 24990... Loss: 1.3594... Val Loss: 1.3448\n",
      "Epoch: 8/10... Step: 25000... Loss: 1.4282... Val Loss: 1.3444\n",
      "Epoch: 8/10... Step: 25010... Loss: 1.3610... Val Loss: 1.3451\n",
      "Epoch: 8/10... Step: 25020... Loss: 1.4159... Val Loss: 1.3448\n",
      "Epoch: 8/10... Step: 25030... Loss: 1.4845... Val Loss: 1.3446\n",
      "Epoch: 8/10... Step: 25040... Loss: 1.4003... Val Loss: 1.3448\n",
      "Epoch: 8/10... Step: 25050... Loss: 1.4487... Val Loss: 1.3451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10... Step: 25060... Loss: 1.4545... Val Loss: 1.3447\n",
      "Epoch: 8/10... Step: 25070... Loss: 1.4206... Val Loss: 1.3457\n",
      "Epoch: 8/10... Step: 25080... Loss: 1.4697... Val Loss: 1.3489\n",
      "Epoch: 8/10... Step: 25090... Loss: 1.3915... Val Loss: 1.3486\n",
      "Epoch: 8/10... Step: 25100... Loss: 1.3416... Val Loss: 1.3471\n",
      "Epoch: 8/10... Step: 25110... Loss: 1.4516... Val Loss: 1.3471\n",
      "Epoch: 8/10... Step: 25120... Loss: 1.3884... Val Loss: 1.3472\n",
      "Epoch: 8/10... Step: 25130... Loss: 1.3627... Val Loss: 1.3483\n",
      "Epoch: 8/10... Step: 25140... Loss: 1.3938... Val Loss: 1.3503\n",
      "Epoch: 8/10... Step: 25150... Loss: 1.3585... Val Loss: 1.3499\n",
      "Epoch: 8/10... Step: 25160... Loss: 1.3356... Val Loss: 1.3486\n",
      "Epoch: 8/10... Step: 25170... Loss: 1.3689... Val Loss: 1.3501\n",
      "Epoch: 8/10... Step: 25180... Loss: 1.4332... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 25190... Loss: 1.4039... Val Loss: 1.3494\n",
      "Epoch: 8/10... Step: 25200... Loss: 1.4233... Val Loss: 1.3487\n",
      "Epoch: 8/10... Step: 25210... Loss: 1.3311... Val Loss: 1.3485\n",
      "Epoch: 8/10... Step: 25220... Loss: 1.4914... Val Loss: 1.3480\n",
      "Epoch: 8/10... Step: 25230... Loss: 1.3645... Val Loss: 1.3484\n",
      "Epoch: 8/10... Step: 25240... Loss: 1.5076... Val Loss: 1.3490\n",
      "Epoch: 8/10... Step: 25250... Loss: 1.4812... Val Loss: 1.3493\n",
      "Epoch: 8/10... Step: 25260... Loss: 1.4182... Val Loss: 1.3487\n",
      "Epoch: 8/10... Step: 25270... Loss: 1.4384... Val Loss: 1.3466\n",
      "Epoch: 8/10... Step: 25280... Loss: 1.3880... Val Loss: 1.3452\n",
      "Epoch: 8/10... Step: 25290... Loss: 1.4080... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 25300... Loss: 1.4444... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 25310... Loss: 1.5502... Val Loss: 1.3446\n",
      "Epoch: 8/10... Step: 25320... Loss: 1.3709... Val Loss: 1.3429\n",
      "Epoch: 8/10... Step: 25330... Loss: 1.4267... Val Loss: 1.3431\n",
      "Epoch: 8/10... Step: 25340... Loss: 1.4590... Val Loss: 1.3427\n",
      "Epoch: 8/10... Step: 25350... Loss: 1.5000... Val Loss: 1.3435\n",
      "Epoch: 8/10... Step: 25360... Loss: 1.5428... Val Loss: 1.3431\n",
      "Epoch: 8/10... Step: 25370... Loss: 1.5094... Val Loss: 1.3435\n",
      "Epoch: 8/10... Step: 25380... Loss: 1.3716... Val Loss: 1.3437\n",
      "Epoch: 8/10... Step: 25390... Loss: 1.4760... Val Loss: 1.3465\n",
      "Epoch: 8/10... Step: 25400... Loss: 1.4622... Val Loss: 1.3448\n",
      "Epoch: 8/10... Step: 25410... Loss: 1.4923... Val Loss: 1.3439\n",
      "Epoch: 8/10... Step: 25420... Loss: 1.3865... Val Loss: 1.3442\n",
      "Epoch: 8/10... Step: 25430... Loss: 1.3643... Val Loss: 1.3427\n",
      "Epoch: 8/10... Step: 25440... Loss: 1.3533... Val Loss: 1.3423\n",
      "Epoch: 8/10... Step: 25450... Loss: 1.4544... Val Loss: 1.3425\n",
      "Epoch: 8/10... Step: 25460... Loss: 1.4967... Val Loss: 1.3439\n",
      "Epoch: 8/10... Step: 25470... Loss: 1.4294... Val Loss: 1.3425\n",
      "Epoch: 8/10... Step: 25480... Loss: 1.3304... Val Loss: 1.3420\n",
      "Epoch: 8/10... Step: 25490... Loss: 1.4163... Val Loss: 1.3420\n",
      "Epoch: 8/10... Step: 25500... Loss: 1.5012... Val Loss: 1.3430\n",
      "Epoch: 8/10... Step: 25510... Loss: 1.3449... Val Loss: 1.3433\n",
      "Epoch: 8/10... Step: 25520... Loss: 1.3699... Val Loss: 1.3466\n",
      "Epoch: 8/10... Step: 25530... Loss: 1.4923... Val Loss: 1.3465\n",
      "Epoch: 8/10... Step: 25540... Loss: 1.3238... Val Loss: 1.3455\n",
      "Epoch: 8/10... Step: 25550... Loss: 1.3556... Val Loss: 1.3459\n",
      "Epoch: 8/10... Step: 25560... Loss: 1.3996... Val Loss: 1.3465\n",
      "Epoch: 8/10... Step: 25570... Loss: 1.4262... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 25580... Loss: 1.3819... Val Loss: 1.3448\n",
      "Epoch: 8/10... Step: 25590... Loss: 1.3830... Val Loss: 1.3457\n",
      "Epoch: 8/10... Step: 25600... Loss: 1.3904... Val Loss: 1.3441\n",
      "Epoch: 8/10... Step: 25610... Loss: 1.3927... Val Loss: 1.3428\n",
      "Epoch: 8/10... Step: 25620... Loss: 1.3989... Val Loss: 1.3425\n",
      "Epoch: 8/10... Step: 25630... Loss: 1.3509... Val Loss: 1.3430\n",
      "Epoch: 8/10... Step: 25640... Loss: 1.4354... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 25650... Loss: 1.4359... Val Loss: 1.3453\n",
      "Epoch: 8/10... Step: 25660... Loss: 1.4305... Val Loss: 1.3456\n",
      "Epoch: 8/10... Step: 25670... Loss: 1.2658... Val Loss: 1.3457\n",
      "Epoch: 8/10... Step: 25680... Loss: 1.4717... Val Loss: 1.3431\n",
      "Epoch: 8/10... Step: 25690... Loss: 1.3932... Val Loss: 1.3439\n",
      "Epoch: 8/10... Step: 25700... Loss: 1.4361... Val Loss: 1.3459\n",
      "Epoch: 8/10... Step: 25710... Loss: 1.3251... Val Loss: 1.3438\n",
      "Epoch: 8/10... Step: 25720... Loss: 1.3741... Val Loss: 1.3428\n",
      "Epoch: 8/10... Step: 25730... Loss: 1.3939... Val Loss: 1.3441\n",
      "Epoch: 8/10... Step: 25740... Loss: 1.4197... Val Loss: 1.3433\n",
      "Epoch: 8/10... Step: 25750... Loss: 1.3587... Val Loss: 1.3439\n",
      "Epoch: 8/10... Step: 25760... Loss: 1.4193... Val Loss: 1.3435\n",
      "Epoch: 8/10... Step: 25770... Loss: 1.4251... Val Loss: 1.3438\n",
      "Epoch: 8/10... Step: 25780... Loss: 1.4122... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 25790... Loss: 1.4018... Val Loss: 1.3441\n",
      "Epoch: 8/10... Step: 25800... Loss: 1.3979... Val Loss: 1.3452\n",
      "Epoch: 8/10... Step: 25810... Loss: 1.5057... Val Loss: 1.3447\n",
      "Epoch: 8/10... Step: 25820... Loss: 1.4613... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 25830... Loss: 1.4490... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 25840... Loss: 1.3789... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 25850... Loss: 1.3974... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 25860... Loss: 1.4270... Val Loss: 1.3456\n",
      "Epoch: 8/10... Step: 25870... Loss: 1.4438... Val Loss: 1.3446\n",
      "Epoch: 8/10... Step: 25880... Loss: 1.3199... Val Loss: 1.3442\n",
      "Epoch: 8/10... Step: 25890... Loss: 1.3888... Val Loss: 1.3427\n",
      "Epoch: 8/10... Step: 25900... Loss: 1.3813... Val Loss: 1.3442\n",
      "Epoch: 8/10... Step: 25910... Loss: 1.4253... Val Loss: 1.3446\n",
      "Epoch: 8/10... Step: 25920... Loss: 1.5172... Val Loss: 1.3429\n",
      "Epoch: 8/10... Step: 25930... Loss: 1.4130... Val Loss: 1.3443\n",
      "Epoch: 8/10... Step: 25940... Loss: 1.4050... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 25950... Loss: 1.4134... Val Loss: 1.3448\n",
      "Epoch: 8/10... Step: 25960... Loss: 1.4695... Val Loss: 1.3454\n",
      "Epoch: 8/10... Step: 25970... Loss: 1.4612... Val Loss: 1.3463\n",
      "Epoch: 8/10... Step: 25980... Loss: 1.4755... Val Loss: 1.3483\n",
      "Epoch: 8/10... Step: 25990... Loss: 1.4474... Val Loss: 1.3485\n",
      "Epoch: 8/10... Step: 26000... Loss: 1.5263... Val Loss: 1.3451\n",
      "Epoch: 8/10... Step: 26010... Loss: 1.4255... Val Loss: 1.3458\n",
      "Epoch: 8/10... Step: 26020... Loss: 1.4911... Val Loss: 1.3465\n",
      "Epoch: 8/10... Step: 26030... Loss: 1.4580... Val Loss: 1.3446\n",
      "Epoch: 8/10... Step: 26040... Loss: 1.4672... Val Loss: 1.3454\n",
      "Epoch: 8/10... Step: 26050... Loss: 1.4915... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 26060... Loss: 1.5898... Val Loss: 1.3443\n",
      "Epoch: 8/10... Step: 26070... Loss: 1.5102... Val Loss: 1.3484\n",
      "Epoch: 8/10... Step: 26080... Loss: 1.3542... Val Loss: 1.3474\n",
      "Epoch: 8/10... Step: 26090... Loss: 1.3708... Val Loss: 1.3443\n",
      "Epoch: 8/10... Step: 26100... Loss: 1.4027... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 26110... Loss: 1.4904... Val Loss: 1.3455\n",
      "Epoch: 8/10... Step: 26120... Loss: 1.4600... Val Loss: 1.3453\n",
      "Epoch: 8/10... Step: 26130... Loss: 1.4298... Val Loss: 1.3443\n",
      "Epoch: 8/10... Step: 26140... Loss: 1.4840... Val Loss: 1.3460\n",
      "Epoch: 8/10... Step: 26150... Loss: 1.3907... Val Loss: 1.3472\n",
      "Epoch: 8/10... Step: 26160... Loss: 1.4176... Val Loss: 1.3450\n",
      "Epoch: 8/10... Step: 26170... Loss: 1.4311... Val Loss: 1.3445\n",
      "Epoch: 8/10... Step: 26180... Loss: 1.4255... Val Loss: 1.3448\n",
      "Epoch: 8/10... Step: 26190... Loss: 1.3330... Val Loss: 1.3480\n",
      "Epoch: 8/10... Step: 26200... Loss: 1.3418... Val Loss: 1.3462\n",
      "Epoch: 8/10... Step: 26210... Loss: 1.3791... Val Loss: 1.3420\n",
      "Epoch: 8/10... Step: 26220... Loss: 1.5338... Val Loss: 1.3420\n",
      "Epoch: 8/10... Step: 26230... Loss: 1.3909... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 26240... Loss: 1.4282... Val Loss: 1.3446\n",
      "Epoch: 8/10... Step: 26250... Loss: 1.3895... Val Loss: 1.3431\n",
      "Epoch: 8/10... Step: 26260... Loss: 1.3822... Val Loss: 1.3439\n",
      "Epoch: 8/10... Step: 26270... Loss: 1.3963... Val Loss: 1.3445\n",
      "Epoch: 8/10... Step: 26280... Loss: 1.4519... Val Loss: 1.3466\n",
      "Epoch: 8/10... Step: 26290... Loss: 1.3831... Val Loss: 1.3462\n",
      "Epoch: 8/10... Step: 26300... Loss: 1.4637... Val Loss: 1.3438\n",
      "Epoch: 8/10... Step: 26310... Loss: 1.4013... Val Loss: 1.3431\n",
      "Epoch: 8/10... Step: 26320... Loss: 1.4367... Val Loss: 1.3426\n",
      "Epoch: 8/10... Step: 26330... Loss: 1.4038... Val Loss: 1.3437\n",
      "Epoch: 8/10... Step: 26340... Loss: 1.3761... Val Loss: 1.3454\n",
      "Epoch: 8/10... Step: 26350... Loss: 1.3684... Val Loss: 1.3434\n",
      "Epoch: 8/10... Step: 26360... Loss: 1.3975... Val Loss: 1.3427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10... Step: 26370... Loss: 1.4090... Val Loss: 1.3430\n",
      "Epoch: 8/10... Step: 26380... Loss: 1.3282... Val Loss: 1.3444\n",
      "Epoch: 8/10... Step: 26390... Loss: 1.4293... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 26400... Loss: 1.4049... Val Loss: 1.3458\n",
      "Epoch: 8/10... Step: 26410... Loss: 1.3914... Val Loss: 1.3429\n",
      "Epoch: 8/10... Step: 26420... Loss: 1.4208... Val Loss: 1.3438\n",
      "Epoch: 8/10... Step: 26430... Loss: 1.3520... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 26440... Loss: 1.3370... Val Loss: 1.3427\n",
      "Epoch: 8/10... Step: 26450... Loss: 1.3240... Val Loss: 1.3431\n",
      "Epoch: 8/10... Step: 26460... Loss: 1.3906... Val Loss: 1.3441\n",
      "Epoch: 8/10... Step: 26470... Loss: 1.3825... Val Loss: 1.3429\n",
      "Epoch: 8/10... Step: 26480... Loss: 1.3079... Val Loss: 1.3427\n",
      "Epoch: 8/10... Step: 26490... Loss: 1.3881... Val Loss: 1.3414\n",
      "Epoch: 8/10... Step: 26500... Loss: 1.4764... Val Loss: 1.3445\n",
      "Epoch: 8/10... Step: 26510... Loss: 1.4324... Val Loss: 1.3468\n",
      "Epoch: 8/10... Step: 26520... Loss: 1.4609... Val Loss: 1.3445\n",
      "Epoch: 8/10... Step: 26530... Loss: 1.4689... Val Loss: 1.3426\n",
      "Epoch: 8/10... Step: 26540... Loss: 1.3256... Val Loss: 1.3423\n",
      "Epoch: 8/10... Step: 26550... Loss: 1.5068... Val Loss: 1.3423\n",
      "Epoch: 8/10... Step: 26560... Loss: 1.4031... Val Loss: 1.3442\n",
      "Epoch: 8/10... Step: 26570... Loss: 1.3505... Val Loss: 1.3445\n",
      "Epoch: 8/10... Step: 26580... Loss: 1.4455... Val Loss: 1.3449\n",
      "Epoch: 8/10... Step: 26590... Loss: 1.4540... Val Loss: 1.3432\n",
      "Epoch: 8/10... Step: 26600... Loss: 1.3426... Val Loss: 1.3429\n",
      "Epoch: 8/10... Step: 26610... Loss: 1.4881... Val Loss: 1.3427\n",
      "Epoch: 8/10... Step: 26620... Loss: 1.3556... Val Loss: 1.3444\n",
      "Epoch: 8/10... Step: 26630... Loss: 1.3250... Val Loss: 1.3447\n",
      "Epoch: 8/10... Step: 26640... Loss: 1.4477... Val Loss: 1.3447\n",
      "Epoch: 8/10... Step: 26650... Loss: 1.3431... Val Loss: 1.3458\n",
      "Epoch: 8/10... Step: 26660... Loss: 1.4613... Val Loss: 1.3465\n",
      "Epoch: 8/10... Step: 26670... Loss: 1.3651... Val Loss: 1.3469\n",
      "Epoch: 8/10... Step: 26680... Loss: 1.4068... Val Loss: 1.3464\n",
      "Epoch: 8/10... Step: 26690... Loss: 1.3238... Val Loss: 1.3468\n",
      "Epoch: 8/10... Step: 26700... Loss: 1.3740... Val Loss: 1.3472\n",
      "Epoch: 8/10... Step: 26710... Loss: 1.4028... Val Loss: 1.3455\n",
      "Epoch: 8/10... Step: 26720... Loss: 1.3882... Val Loss: 1.3450\n",
      "Epoch: 8/10... Step: 26730... Loss: 1.4502... Val Loss: 1.3472\n",
      "Epoch: 8/10... Step: 26740... Loss: 1.3471... Val Loss: 1.3472\n",
      "Epoch: 8/10... Step: 26750... Loss: 1.3943... Val Loss: 1.3462\n",
      "Epoch: 8/10... Step: 26760... Loss: 1.3382... Val Loss: 1.3464\n",
      "Epoch: 8/10... Step: 26770... Loss: 1.4540... Val Loss: 1.3452\n",
      "Epoch: 8/10... Step: 26780... Loss: 1.3167... Val Loss: 1.3441\n",
      "Epoch: 8/10... Step: 26790... Loss: 1.3296... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 26800... Loss: 1.5110... Val Loss: 1.3428\n",
      "Epoch: 8/10... Step: 26810... Loss: 1.4847... Val Loss: 1.3417\n",
      "Epoch: 8/10... Step: 26820... Loss: 1.3838... Val Loss: 1.3427\n",
      "Epoch: 8/10... Step: 26830... Loss: 1.3424... Val Loss: 1.3431\n",
      "Epoch: 8/10... Step: 26840... Loss: 1.3387... Val Loss: 1.3455\n",
      "Epoch: 8/10... Step: 26850... Loss: 1.2758... Val Loss: 1.3458\n",
      "Epoch: 8/10... Step: 26860... Loss: 1.3520... Val Loss: 1.3453\n",
      "Epoch: 8/10... Step: 26870... Loss: 1.3949... Val Loss: 1.3440\n",
      "Epoch: 8/10... Step: 26880... Loss: 1.3055... Val Loss: 1.3434\n",
      "Epoch: 8/10... Step: 26890... Loss: 1.4987... Val Loss: 1.3458\n",
      "Epoch: 8/10... Step: 26900... Loss: 1.4764... Val Loss: 1.3456\n",
      "Epoch: 8/10... Step: 26910... Loss: 1.4936... Val Loss: 1.3450\n",
      "Epoch: 8/10... Step: 26920... Loss: 1.4029... Val Loss: 1.3437\n",
      "Epoch: 8/10... Step: 26930... Loss: 1.3360... Val Loss: 1.3435\n",
      "Epoch: 8/10... Step: 26940... Loss: 1.4565... Val Loss: 1.3445\n",
      "Epoch: 8/10... Step: 26950... Loss: 1.4537... Val Loss: 1.3444\n",
      "Epoch: 8/10... Step: 26960... Loss: 1.5008... Val Loss: 1.3428\n",
      "Epoch: 8/10... Step: 26970... Loss: 1.4578... Val Loss: 1.3420\n",
      "Epoch: 8/10... Step: 26980... Loss: 1.3820... Val Loss: 1.3428\n",
      "Epoch: 8/10... Step: 26990... Loss: 1.3565... Val Loss: 1.3432\n",
      "Epoch: 8/10... Step: 27000... Loss: 1.4178... Val Loss: 1.3434\n",
      "Epoch: 8/10... Step: 27010... Loss: 1.4234... Val Loss: 1.3425\n",
      "Epoch: 8/10... Step: 27020... Loss: 1.4165... Val Loss: 1.3438\n",
      "Epoch: 8/10... Step: 27030... Loss: 1.3935... Val Loss: 1.3423\n",
      "Epoch: 8/10... Step: 27040... Loss: 1.3855... Val Loss: 1.3425\n",
      "Epoch: 9/10... Step: 27050... Loss: 1.4288... Val Loss: 1.3445\n",
      "Epoch: 9/10... Step: 27060... Loss: 1.4301... Val Loss: 1.3443\n",
      "Epoch: 9/10... Step: 27070... Loss: 1.3761... Val Loss: 1.3425\n",
      "Epoch: 9/10... Step: 27080... Loss: 1.3598... Val Loss: 1.3425\n",
      "Epoch: 9/10... Step: 27090... Loss: 1.3119... Val Loss: 1.3431\n",
      "Epoch: 9/10... Step: 27100... Loss: 1.4628... Val Loss: 1.3419\n",
      "Epoch: 9/10... Step: 27110... Loss: 1.3712... Val Loss: 1.3431\n",
      "Epoch: 9/10... Step: 27120... Loss: 1.3549... Val Loss: 1.3439\n",
      "Epoch: 9/10... Step: 27130... Loss: 1.4655... Val Loss: 1.3437\n",
      "Epoch: 9/10... Step: 27140... Loss: 1.4113... Val Loss: 1.3432\n",
      "Epoch: 9/10... Step: 27150... Loss: 1.3398... Val Loss: 1.3435\n",
      "Epoch: 9/10... Step: 27160... Loss: 1.4176... Val Loss: 1.3423\n",
      "Epoch: 9/10... Step: 27170... Loss: 1.5890... Val Loss: 1.3421\n",
      "Epoch: 9/10... Step: 27180... Loss: 1.4701... Val Loss: 1.3433\n",
      "Epoch: 9/10... Step: 27190... Loss: 1.3429... Val Loss: 1.3443\n",
      "Epoch: 9/10... Step: 27200... Loss: 1.5634... Val Loss: 1.3442\n",
      "Epoch: 9/10... Step: 27210... Loss: 1.4395... Val Loss: 1.3420\n",
      "Epoch: 9/10... Step: 27220... Loss: 1.4012... Val Loss: 1.3411\n",
      "Epoch: 9/10... Step: 27230... Loss: 1.5055... Val Loss: 1.3407\n",
      "Epoch: 9/10... Step: 27240... Loss: 1.4242... Val Loss: 1.3425\n",
      "Epoch: 9/10... Step: 27250... Loss: 1.3793... Val Loss: 1.3441\n",
      "Epoch: 9/10... Step: 27260... Loss: 1.4237... Val Loss: 1.3416\n",
      "Epoch: 9/10... Step: 27270... Loss: 1.3395... Val Loss: 1.3419\n",
      "Epoch: 9/10... Step: 27280... Loss: 1.3500... Val Loss: 1.3411\n",
      "Epoch: 9/10... Step: 27290... Loss: 1.3061... Val Loss: 1.3432\n",
      "Epoch: 9/10... Step: 27300... Loss: 1.4885... Val Loss: 1.3419\n",
      "Epoch: 9/10... Step: 27310... Loss: 1.3253... Val Loss: 1.3422\n",
      "Epoch: 9/10... Step: 27320... Loss: 1.3517... Val Loss: 1.3420\n",
      "Epoch: 9/10... Step: 27330... Loss: 1.4144... Val Loss: 1.3418\n",
      "Epoch: 9/10... Step: 27340... Loss: 1.5118... Val Loss: 1.3413\n",
      "Epoch: 9/10... Step: 27350... Loss: 1.4472... Val Loss: 1.3423\n",
      "Epoch: 9/10... Step: 27360... Loss: 1.3542... Val Loss: 1.3428\n",
      "Epoch: 9/10... Step: 27370... Loss: 1.5268... Val Loss: 1.3417\n",
      "Epoch: 9/10... Step: 27380... Loss: 1.4139... Val Loss: 1.3421\n",
      "Epoch: 9/10... Step: 27390... Loss: 1.5081... Val Loss: 1.3425\n",
      "Epoch: 9/10... Step: 27400... Loss: 1.4233... Val Loss: 1.3436\n",
      "Epoch: 9/10... Step: 27410... Loss: 1.3138... Val Loss: 1.3429\n",
      "Epoch: 9/10... Step: 27420... Loss: 1.4445... Val Loss: 1.3421\n",
      "Epoch: 9/10... Step: 27430... Loss: 1.3634... Val Loss: 1.3423\n",
      "Epoch: 9/10... Step: 27440... Loss: 1.4332... Val Loss: 1.3433\n",
      "Epoch: 9/10... Step: 27450... Loss: 1.3971... Val Loss: 1.3440\n",
      "Epoch: 9/10... Step: 27460... Loss: 1.3462... Val Loss: 1.3434\n",
      "Epoch: 9/10... Step: 27470... Loss: 1.4858... Val Loss: 1.3427\n",
      "Epoch: 9/10... Step: 27480... Loss: 1.4624... Val Loss: 1.3426\n",
      "Epoch: 9/10... Step: 27490... Loss: 1.2254... Val Loss: 1.3419\n",
      "Epoch: 9/10... Step: 27500... Loss: 1.3105... Val Loss: 1.3418\n",
      "Epoch: 9/10... Step: 27510... Loss: 1.4185... Val Loss: 1.3417\n",
      "Epoch: 9/10... Step: 27520... Loss: 1.4917... Val Loss: 1.3413\n",
      "Epoch: 9/10... Step: 27530... Loss: 1.4992... Val Loss: 1.3421\n",
      "Epoch: 9/10... Step: 27540... Loss: 1.4660... Val Loss: 1.3434\n",
      "Epoch: 9/10... Step: 27550... Loss: 1.4367... Val Loss: 1.3441\n",
      "Epoch: 9/10... Step: 27560... Loss: 1.4252... Val Loss: 1.3409\n",
      "Epoch: 9/10... Step: 27570... Loss: 1.5070... Val Loss: 1.3411\n",
      "Epoch: 9/10... Step: 27580... Loss: 1.4600... Val Loss: 1.3410\n",
      "Epoch: 9/10... Step: 27590... Loss: 1.3825... Val Loss: 1.3411\n",
      "Epoch: 9/10... Step: 27600... Loss: 1.3911... Val Loss: 1.3416\n",
      "Epoch: 9/10... Step: 27610... Loss: 1.2905... Val Loss: 1.3412\n",
      "Epoch: 9/10... Step: 27620... Loss: 1.3981... Val Loss: 1.3422\n",
      "Epoch: 9/10... Step: 27630... Loss: 1.4951... Val Loss: 1.3417\n",
      "Epoch: 9/10... Step: 27640... Loss: 1.3900... Val Loss: 1.3412\n",
      "Epoch: 9/10... Step: 27650... Loss: 1.4138... Val Loss: 1.3403\n",
      "Epoch: 9/10... Step: 27660... Loss: 1.4364... Val Loss: 1.3404\n",
      "Epoch: 9/10... Step: 27670... Loss: 1.4124... Val Loss: 1.3417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10... Step: 27680... Loss: 1.4876... Val Loss: 1.3424\n",
      "Epoch: 9/10... Step: 27690... Loss: 1.4727... Val Loss: 1.3414\n",
      "Epoch: 9/10... Step: 27700... Loss: 1.4514... Val Loss: 1.3418\n",
      "Epoch: 9/10... Step: 27710... Loss: 1.3780... Val Loss: 1.3419\n",
      "Epoch: 9/10... Step: 27720... Loss: 1.3576... Val Loss: 1.3428\n",
      "Epoch: 9/10... Step: 27730... Loss: 1.3951... Val Loss: 1.3394\n",
      "Epoch: 9/10... Step: 27740... Loss: 1.4497... Val Loss: 1.3398\n",
      "Epoch: 9/10... Step: 27750... Loss: 1.2747... Val Loss: 1.3413\n",
      "Epoch: 9/10... Step: 27760... Loss: 1.3940... Val Loss: 1.3414\n",
      "Epoch: 9/10... Step: 27770... Loss: 1.4108... Val Loss: 1.3414\n",
      "Epoch: 9/10... Step: 27780... Loss: 1.4105... Val Loss: 1.3417\n",
      "Epoch: 9/10... Step: 27790... Loss: 1.4644... Val Loss: 1.3419\n",
      "Epoch: 9/10... Step: 27800... Loss: 1.4931... Val Loss: 1.3423\n",
      "Epoch: 9/10... Step: 27810... Loss: 1.4400... Val Loss: 1.3399\n",
      "Epoch: 9/10... Step: 27820... Loss: 1.3806... Val Loss: 1.3401\n",
      "Epoch: 9/10... Step: 27830... Loss: 1.4271... Val Loss: 1.3410\n",
      "Epoch: 9/10... Step: 27840... Loss: 1.3622... Val Loss: 1.3402\n",
      "Epoch: 9/10... Step: 27850... Loss: 1.4813... Val Loss: 1.3400\n",
      "Epoch: 9/10... Step: 27860... Loss: 1.3211... Val Loss: 1.3405\n",
      "Epoch: 9/10... Step: 27870... Loss: 1.3747... Val Loss: 1.3401\n",
      "Epoch: 9/10... Step: 27880... Loss: 1.2584... Val Loss: 1.3400\n",
      "Epoch: 9/10... Step: 27890... Loss: 1.4029... Val Loss: 1.3408\n",
      "Epoch: 9/10... Step: 27900... Loss: 1.4218... Val Loss: 1.3413\n",
      "Epoch: 9/10... Step: 27910... Loss: 1.5224... Val Loss: 1.3414\n",
      "Epoch: 9/10... Step: 27920... Loss: 1.4166... Val Loss: 1.3396\n",
      "Epoch: 9/10... Step: 27930... Loss: 1.4553... Val Loss: 1.3398\n",
      "Epoch: 9/10... Step: 27940... Loss: 1.3617... Val Loss: 1.3410\n",
      "Epoch: 9/10... Step: 27950... Loss: 1.4281... Val Loss: 1.3422\n",
      "Epoch: 9/10... Step: 27960... Loss: 1.4641... Val Loss: 1.3410\n",
      "Epoch: 9/10... Step: 27970... Loss: 1.4000... Val Loss: 1.3410\n",
      "Epoch: 9/10... Step: 27980... Loss: 1.5621... Val Loss: 1.3426\n",
      "Epoch: 9/10... Step: 27990... Loss: 1.3838... Val Loss: 1.3418\n",
      "Epoch: 9/10... Step: 28000... Loss: 1.4781... Val Loss: 1.3409\n",
      "Epoch: 9/10... Step: 28010... Loss: 1.3862... Val Loss: 1.3400\n",
      "Epoch: 9/10... Step: 28020... Loss: 1.4114... Val Loss: 1.3417\n",
      "Epoch: 9/10... Step: 28030... Loss: 1.3282... Val Loss: 1.3433\n",
      "Epoch: 9/10... Step: 28040... Loss: 1.3016... Val Loss: 1.3429\n",
      "Epoch: 9/10... Step: 28050... Loss: 1.3692... Val Loss: 1.3411\n",
      "Epoch: 9/10... Step: 28060... Loss: 1.4326... Val Loss: 1.3402\n",
      "Epoch: 9/10... Step: 28070... Loss: 1.3749... Val Loss: 1.3410\n",
      "Epoch: 9/10... Step: 28080... Loss: 1.4533... Val Loss: 1.3400\n",
      "Epoch: 9/10... Step: 28090... Loss: 1.3669... Val Loss: 1.3391\n",
      "Epoch: 9/10... Step: 28100... Loss: 1.4089... Val Loss: 1.3389\n",
      "Epoch: 9/10... Step: 28110... Loss: 1.4477... Val Loss: 1.3405\n",
      "Epoch: 9/10... Step: 28120... Loss: 1.3491... Val Loss: 1.3416\n",
      "Epoch: 9/10... Step: 28130... Loss: 1.3668... Val Loss: 1.3423\n",
      "Epoch: 9/10... Step: 28140... Loss: 1.3383... Val Loss: 1.3406\n",
      "Epoch: 9/10... Step: 28150... Loss: 1.4412... Val Loss: 1.3410\n",
      "Epoch: 9/10... Step: 28160... Loss: 1.4488... Val Loss: 1.3406\n",
      "Epoch: 9/10... Step: 28170... Loss: 1.4494... Val Loss: 1.3399\n",
      "Epoch: 9/10... Step: 28180... Loss: 1.5606... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 28190... Loss: 1.4648... Val Loss: 1.3409\n",
      "Epoch: 9/10... Step: 28200... Loss: 1.4160... Val Loss: 1.3404\n",
      "Epoch: 9/10... Step: 28210... Loss: 1.4243... Val Loss: 1.3401\n",
      "Epoch: 9/10... Step: 28220... Loss: 1.4242... Val Loss: 1.3399\n",
      "Epoch: 9/10... Step: 28230... Loss: 1.3737... Val Loss: 1.3397\n",
      "Epoch: 9/10... Step: 28240... Loss: 1.5372... Val Loss: 1.3394\n",
      "Epoch: 9/10... Step: 28250... Loss: 1.4997... Val Loss: 1.3395\n",
      "Epoch: 9/10... Step: 28260... Loss: 1.4905... Val Loss: 1.3402\n",
      "Epoch: 9/10... Step: 28270... Loss: 1.3717... Val Loss: 1.3414\n",
      "Epoch: 9/10... Step: 28280... Loss: 1.4292... Val Loss: 1.3409\n",
      "Epoch: 9/10... Step: 28290... Loss: 1.3985... Val Loss: 1.3394\n",
      "Epoch: 9/10... Step: 28300... Loss: 1.3912... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 28310... Loss: 1.3118... Val Loss: 1.3396\n",
      "Epoch: 9/10... Step: 28320... Loss: 1.4372... Val Loss: 1.3394\n",
      "Epoch: 9/10... Step: 28330... Loss: 1.3846... Val Loss: 1.3400\n",
      "Epoch: 9/10... Step: 28340... Loss: 1.3535... Val Loss: 1.3411\n",
      "Epoch: 9/10... Step: 28350... Loss: 1.3668... Val Loss: 1.3397\n",
      "Epoch: 9/10... Step: 28360... Loss: 1.2587... Val Loss: 1.3385\n",
      "Epoch: 9/10... Step: 28370... Loss: 1.4288... Val Loss: 1.3378\n",
      "Epoch: 9/10... Step: 28380... Loss: 1.4365... Val Loss: 1.3383\n",
      "Epoch: 9/10... Step: 28390... Loss: 1.4083... Val Loss: 1.3378\n",
      "Epoch: 9/10... Step: 28400... Loss: 1.3154... Val Loss: 1.3366\n",
      "Epoch: 9/10... Step: 28410... Loss: 1.4509... Val Loss: 1.3369\n",
      "Epoch: 9/10... Step: 28420... Loss: 1.4291... Val Loss: 1.3376\n",
      "Epoch: 9/10... Step: 28430... Loss: 1.4371... Val Loss: 1.3400\n",
      "Epoch: 9/10... Step: 28440... Loss: 1.3950... Val Loss: 1.3398\n",
      "Epoch: 9/10... Step: 28450... Loss: 1.4392... Val Loss: 1.3387\n",
      "Epoch: 9/10... Step: 28460... Loss: 1.4304... Val Loss: 1.3405\n",
      "Epoch: 9/10... Step: 28470... Loss: 1.4376... Val Loss: 1.3423\n",
      "Epoch: 9/10... Step: 28480... Loss: 1.3845... Val Loss: 1.3400\n",
      "Epoch: 9/10... Step: 28490... Loss: 1.3789... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 28500... Loss: 1.4143... Val Loss: 1.3397\n",
      "Epoch: 9/10... Step: 28510... Loss: 1.3615... Val Loss: 1.3411\n",
      "Epoch: 9/10... Step: 28520... Loss: 1.4192... Val Loss: 1.3410\n",
      "Epoch: 9/10... Step: 28530... Loss: 1.4285... Val Loss: 1.3394\n",
      "Epoch: 9/10... Step: 28540... Loss: 1.3094... Val Loss: 1.3387\n",
      "Epoch: 9/10... Step: 28550... Loss: 1.3941... Val Loss: 1.3397\n",
      "Epoch: 9/10... Step: 28560... Loss: 1.4482... Val Loss: 1.3396\n",
      "Epoch: 9/10... Step: 28570... Loss: 1.4573... Val Loss: 1.3394\n",
      "Epoch: 9/10... Step: 28580... Loss: 1.4400... Val Loss: 1.3382\n",
      "Epoch: 9/10... Step: 28590... Loss: 1.3933... Val Loss: 1.3383\n",
      "Epoch: 9/10... Step: 28600... Loss: 1.3347... Val Loss: 1.3392\n",
      "Epoch: 9/10... Step: 28610... Loss: 1.4296... Val Loss: 1.3387\n",
      "Epoch: 9/10... Step: 28620... Loss: 1.3598... Val Loss: 1.3385\n",
      "Epoch: 9/10... Step: 28630... Loss: 1.3429... Val Loss: 1.3391\n",
      "Epoch: 9/10... Step: 28640... Loss: 1.3113... Val Loss: 1.3398\n",
      "Epoch: 9/10... Step: 28650... Loss: 1.4285... Val Loss: 1.3396\n",
      "Epoch: 9/10... Step: 28660... Loss: 1.4855... Val Loss: 1.3378\n",
      "Epoch: 9/10... Step: 28670... Loss: 1.3229... Val Loss: 1.3365\n",
      "Epoch: 9/10... Step: 28680... Loss: 1.3908... Val Loss: 1.3364\n",
      "Epoch: 9/10... Step: 28690... Loss: 1.3931... Val Loss: 1.3370\n",
      "Epoch: 9/10... Step: 28700... Loss: 1.3843... Val Loss: 1.3398\n",
      "Epoch: 9/10... Step: 28710... Loss: 1.4816... Val Loss: 1.3398\n",
      "Epoch: 9/10... Step: 28720... Loss: 1.3165... Val Loss: 1.3378\n",
      "Epoch: 9/10... Step: 28730... Loss: 1.4664... Val Loss: 1.3391\n",
      "Epoch: 9/10... Step: 28740... Loss: 1.4555... Val Loss: 1.3376\n",
      "Epoch: 9/10... Step: 28750... Loss: 1.4301... Val Loss: 1.3363\n",
      "Epoch: 9/10... Step: 28760... Loss: 1.5104... Val Loss: 1.3358\n",
      "Epoch: 9/10... Step: 28770... Loss: 1.3858... Val Loss: 1.3359\n",
      "Epoch: 9/10... Step: 28780... Loss: 1.3665... Val Loss: 1.3361\n",
      "Epoch: 9/10... Step: 28790... Loss: 1.3996... Val Loss: 1.3371\n",
      "Epoch: 9/10... Step: 28800... Loss: 1.4115... Val Loss: 1.3361\n",
      "Epoch: 9/10... Step: 28810... Loss: 1.4593... Val Loss: 1.3356\n",
      "Epoch: 9/10... Step: 28820... Loss: 1.4559... Val Loss: 1.3356\n",
      "Epoch: 9/10... Step: 28830... Loss: 1.3576... Val Loss: 1.3361\n",
      "Epoch: 9/10... Step: 28840... Loss: 1.4980... Val Loss: 1.3357\n",
      "Epoch: 9/10... Step: 28850... Loss: 1.3799... Val Loss: 1.3358\n",
      "Epoch: 9/10... Step: 28860... Loss: 1.5438... Val Loss: 1.3357\n",
      "Epoch: 9/10... Step: 28870... Loss: 1.2913... Val Loss: 1.3354\n",
      "Epoch: 9/10... Step: 28880... Loss: 1.3612... Val Loss: 1.3354\n",
      "Epoch: 9/10... Step: 28890... Loss: 1.4297... Val Loss: 1.3357\n",
      "Epoch: 9/10... Step: 28900... Loss: 1.4325... Val Loss: 1.3389\n",
      "Epoch: 9/10... Step: 28910... Loss: 1.3766... Val Loss: 1.3377\n",
      "Epoch: 9/10... Step: 28920... Loss: 1.4028... Val Loss: 1.3375\n",
      "Epoch: 9/10... Step: 28930... Loss: 1.3695... Val Loss: 1.3375\n",
      "Epoch: 9/10... Step: 28940... Loss: 1.5302... Val Loss: 1.3380\n",
      "Epoch: 9/10... Step: 28950... Loss: 1.3595... Val Loss: 1.3378\n",
      "Epoch: 9/10... Step: 28960... Loss: 1.3699... Val Loss: 1.3363\n",
      "Epoch: 9/10... Step: 28970... Loss: 1.3429... Val Loss: 1.3356\n",
      "Epoch: 9/10... Step: 28980... Loss: 1.3710... Val Loss: 1.3358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10... Step: 28990... Loss: 1.4489... Val Loss: 1.3370\n",
      "Epoch: 9/10... Step: 29000... Loss: 1.4231... Val Loss: 1.3357\n",
      "Epoch: 9/10... Step: 29010... Loss: 1.3512... Val Loss: 1.3347\n",
      "Epoch: 9/10... Step: 29020... Loss: 1.4256... Val Loss: 1.3355\n",
      "Epoch: 9/10... Step: 29030... Loss: 1.3390... Val Loss: 1.3363\n",
      "Epoch: 9/10... Step: 29040... Loss: 1.5013... Val Loss: 1.3365\n",
      "Epoch: 9/10... Step: 29050... Loss: 1.3778... Val Loss: 1.3362\n",
      "Epoch: 9/10... Step: 29060... Loss: 1.3800... Val Loss: 1.3367\n",
      "Epoch: 9/10... Step: 29070... Loss: 1.4342... Val Loss: 1.3385\n",
      "Epoch: 9/10... Step: 29080... Loss: 1.3852... Val Loss: 1.3382\n",
      "Epoch: 9/10... Step: 29090... Loss: 1.4782... Val Loss: 1.3367\n",
      "Epoch: 9/10... Step: 29100... Loss: 1.4504... Val Loss: 1.3370\n",
      "Epoch: 9/10... Step: 29110... Loss: 1.4617... Val Loss: 1.3375\n",
      "Epoch: 9/10... Step: 29120... Loss: 1.4776... Val Loss: 1.3362\n",
      "Epoch: 9/10... Step: 29130... Loss: 1.4931... Val Loss: 1.3356\n",
      "Epoch: 9/10... Step: 29140... Loss: 1.3966... Val Loss: 1.3371\n",
      "Epoch: 9/10... Step: 29150... Loss: 1.3873... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 29160... Loss: 1.4591... Val Loss: 1.3383\n",
      "Epoch: 9/10... Step: 29170... Loss: 1.4358... Val Loss: 1.3380\n",
      "Epoch: 9/10... Step: 29180... Loss: 1.3790... Val Loss: 1.3374\n",
      "Epoch: 9/10... Step: 29190... Loss: 1.3305... Val Loss: 1.3386\n",
      "Epoch: 9/10... Step: 29200... Loss: 1.5312... Val Loss: 1.3381\n",
      "Epoch: 9/10... Step: 29210... Loss: 1.4981... Val Loss: 1.3368\n",
      "Epoch: 9/10... Step: 29220... Loss: 1.3472... Val Loss: 1.3359\n",
      "Epoch: 9/10... Step: 29230... Loss: 1.4392... Val Loss: 1.3370\n",
      "Epoch: 9/10... Step: 29240... Loss: 1.3980... Val Loss: 1.3404\n",
      "Epoch: 9/10... Step: 29250... Loss: 1.3620... Val Loss: 1.3374\n",
      "Epoch: 9/10... Step: 29260... Loss: 1.4890... Val Loss: 1.3374\n",
      "Epoch: 9/10... Step: 29270... Loss: 1.4425... Val Loss: 1.3377\n",
      "Epoch: 9/10... Step: 29280... Loss: 1.3567... Val Loss: 1.3400\n",
      "Epoch: 9/10... Step: 29290... Loss: 1.3953... Val Loss: 1.3388\n",
      "Epoch: 9/10... Step: 29300... Loss: 1.4998... Val Loss: 1.3386\n",
      "Epoch: 9/10... Step: 29310... Loss: 1.3986... Val Loss: 1.3392\n",
      "Epoch: 9/10... Step: 29320... Loss: 1.4140... Val Loss: 1.3390\n",
      "Epoch: 9/10... Step: 29330... Loss: 1.3624... Val Loss: 1.3381\n",
      "Epoch: 9/10... Step: 29340... Loss: 1.2894... Val Loss: 1.3374\n",
      "Epoch: 9/10... Step: 29350... Loss: 1.3607... Val Loss: 1.3381\n",
      "Epoch: 9/10... Step: 29360... Loss: 1.4768... Val Loss: 1.3397\n",
      "Epoch: 9/10... Step: 29370... Loss: 1.4016... Val Loss: 1.3376\n",
      "Epoch: 9/10... Step: 29380... Loss: 1.3536... Val Loss: 1.3358\n",
      "Epoch: 9/10... Step: 29390... Loss: 1.2888... Val Loss: 1.3359\n",
      "Epoch: 9/10... Step: 29400... Loss: 1.4472... Val Loss: 1.3358\n",
      "Epoch: 9/10... Step: 29410... Loss: 1.5211... Val Loss: 1.3360\n",
      "Epoch: 9/10... Step: 29420... Loss: 1.4850... Val Loss: 1.3359\n",
      "Epoch: 9/10... Step: 29430... Loss: 1.4477... Val Loss: 1.3364\n",
      "Epoch: 9/10... Step: 29440... Loss: 1.4506... Val Loss: 1.3366\n",
      "Epoch: 9/10... Step: 29450... Loss: 1.3527... Val Loss: 1.3396\n",
      "Epoch: 9/10... Step: 29460... Loss: 1.4021... Val Loss: 1.3423\n",
      "Epoch: 9/10... Step: 29470... Loss: 1.3483... Val Loss: 1.3371\n",
      "Epoch: 9/10... Step: 29480... Loss: 1.4129... Val Loss: 1.3363\n",
      "Epoch: 9/10... Step: 29490... Loss: 1.2814... Val Loss: 1.3369\n",
      "Epoch: 9/10... Step: 29500... Loss: 1.3903... Val Loss: 1.3371\n",
      "Epoch: 9/10... Step: 29510... Loss: 1.4886... Val Loss: 1.3372\n",
      "Epoch: 9/10... Step: 29520... Loss: 1.4822... Val Loss: 1.3367\n",
      "Epoch: 9/10... Step: 29530... Loss: 1.4328... Val Loss: 1.3374\n",
      "Epoch: 9/10... Step: 29540... Loss: 1.3686... Val Loss: 1.3384\n",
      "Epoch: 9/10... Step: 29550... Loss: 1.4099... Val Loss: 1.3367\n",
      "Epoch: 9/10... Step: 29560... Loss: 1.3952... Val Loss: 1.3356\n",
      "Epoch: 9/10... Step: 29570... Loss: 1.2627... Val Loss: 1.3368\n",
      "Epoch: 9/10... Step: 29580... Loss: 1.1961... Val Loss: 1.3384\n",
      "Epoch: 9/10... Step: 29590... Loss: 1.3529... Val Loss: 1.3369\n",
      "Epoch: 9/10... Step: 29600... Loss: 1.3899... Val Loss: 1.3360\n",
      "Epoch: 9/10... Step: 29610... Loss: 1.3513... Val Loss: 1.3360\n",
      "Epoch: 9/10... Step: 29620... Loss: 1.4327... Val Loss: 1.3378\n",
      "Epoch: 9/10... Step: 29630... Loss: 1.4162... Val Loss: 1.3369\n",
      "Epoch: 9/10... Step: 29640... Loss: 1.5061... Val Loss: 1.3362\n",
      "Epoch: 9/10... Step: 29650... Loss: 1.4560... Val Loss: 1.3362\n",
      "Epoch: 9/10... Step: 29660... Loss: 1.3688... Val Loss: 1.3381\n",
      "Epoch: 9/10... Step: 29670... Loss: 1.3312... Val Loss: 1.3387\n",
      "Epoch: 9/10... Step: 29680... Loss: 1.4499... Val Loss: 1.3355\n",
      "Epoch: 9/10... Step: 29690... Loss: 1.4157... Val Loss: 1.3364\n",
      "Epoch: 9/10... Step: 29700... Loss: 1.4116... Val Loss: 1.3368\n",
      "Epoch: 9/10... Step: 29710... Loss: 1.3860... Val Loss: 1.3356\n",
      "Epoch: 9/10... Step: 29720... Loss: 1.4244... Val Loss: 1.3394\n",
      "Epoch: 9/10... Step: 29730... Loss: 1.4953... Val Loss: 1.3353\n",
      "Epoch: 9/10... Step: 29740... Loss: 1.3445... Val Loss: 1.3349\n",
      "Epoch: 9/10... Step: 29750... Loss: 1.4343... Val Loss: 1.3348\n",
      "Epoch: 9/10... Step: 29760... Loss: 1.3100... Val Loss: 1.3360\n",
      "Epoch: 9/10... Step: 29770... Loss: 1.3973... Val Loss: 1.3369\n",
      "Epoch: 9/10... Step: 29780... Loss: 1.3878... Val Loss: 1.3351\n",
      "Epoch: 9/10... Step: 29790... Loss: 1.4590... Val Loss: 1.3340\n",
      "Epoch: 9/10... Step: 29800... Loss: 1.3340... Val Loss: 1.3348\n",
      "Epoch: 9/10... Step: 29810... Loss: 1.3671... Val Loss: 1.3369\n",
      "Epoch: 9/10... Step: 29820... Loss: 1.5201... Val Loss: 1.3347\n",
      "Epoch: 9/10... Step: 29830... Loss: 1.4765... Val Loss: 1.3358\n",
      "Epoch: 9/10... Step: 29840... Loss: 1.4159... Val Loss: 1.3379\n",
      "Epoch: 9/10... Step: 29850... Loss: 1.3640... Val Loss: 1.3363\n",
      "Epoch: 9/10... Step: 29860... Loss: 1.4846... Val Loss: 1.3348\n",
      "Epoch: 9/10... Step: 29870... Loss: 1.4197... Val Loss: 1.3340\n",
      "Epoch: 9/10... Step: 29880... Loss: 1.3689... Val Loss: 1.3341\n",
      "Epoch: 9/10... Step: 29890... Loss: 1.3434... Val Loss: 1.3346\n",
      "Epoch: 9/10... Step: 29900... Loss: 1.4124... Val Loss: 1.3351\n",
      "Epoch: 9/10... Step: 29910... Loss: 1.4872... Val Loss: 1.3347\n",
      "Epoch: 9/10... Step: 29920... Loss: 1.3792... Val Loss: 1.3355\n",
      "Epoch: 9/10... Step: 29930... Loss: 1.3623... Val Loss: 1.3352\n",
      "Epoch: 9/10... Step: 29940... Loss: 1.5193... Val Loss: 1.3366\n",
      "Epoch: 9/10... Step: 29950... Loss: 1.3983... Val Loss: 1.3368\n",
      "Epoch: 9/10... Step: 29960... Loss: 1.4485... Val Loss: 1.3378\n",
      "Epoch: 9/10... Step: 29970... Loss: 1.3744... Val Loss: 1.3363\n",
      "Epoch: 9/10... Step: 29980... Loss: 1.4587... Val Loss: 1.3371\n",
      "Epoch: 9/10... Step: 29990... Loss: 1.3393... Val Loss: 1.3372\n",
      "Epoch: 9/10... Step: 30000... Loss: 1.4503... Val Loss: 1.3371\n",
      "Epoch: 9/10... Step: 30010... Loss: 1.5359... Val Loss: 1.3376\n",
      "Epoch: 9/10... Step: 30020... Loss: 1.4677... Val Loss: 1.3383\n",
      "Epoch: 9/10... Step: 30030... Loss: 1.3778... Val Loss: 1.3388\n",
      "Epoch: 9/10... Step: 30040... Loss: 1.3780... Val Loss: 1.3368\n",
      "Epoch: 9/10... Step: 30050... Loss: 1.3793... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 30060... Loss: 1.4768... Val Loss: 1.3395\n",
      "Epoch: 9/10... Step: 30070... Loss: 1.3967... Val Loss: 1.3399\n",
      "Epoch: 9/10... Step: 30080... Loss: 1.4461... Val Loss: 1.3401\n",
      "Epoch: 9/10... Step: 30090... Loss: 1.3779... Val Loss: 1.3383\n",
      "Epoch: 9/10... Step: 30100... Loss: 1.3927... Val Loss: 1.3386\n",
      "Epoch: 9/10... Step: 30110... Loss: 1.4438... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 30120... Loss: 1.4208... Val Loss: 1.3396\n",
      "Epoch: 9/10... Step: 30130... Loss: 1.3908... Val Loss: 1.3394\n",
      "Epoch: 9/10... Step: 30140... Loss: 1.4299... Val Loss: 1.3404\n",
      "Epoch: 9/10... Step: 30150... Loss: 1.4361... Val Loss: 1.3395\n",
      "Epoch: 9/10... Step: 30160... Loss: 1.4753... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 30170... Loss: 1.4165... Val Loss: 1.3383\n",
      "Epoch: 9/10... Step: 30180... Loss: 1.3737... Val Loss: 1.3381\n",
      "Epoch: 9/10... Step: 30190... Loss: 1.4527... Val Loss: 1.3358\n",
      "Epoch: 9/10... Step: 30200... Loss: 1.3386... Val Loss: 1.3349\n",
      "Epoch: 9/10... Step: 30210... Loss: 1.3700... Val Loss: 1.3348\n",
      "Epoch: 9/10... Step: 30220... Loss: 1.4412... Val Loss: 1.3370\n",
      "Epoch: 9/10... Step: 30230... Loss: 1.4078... Val Loss: 1.3384\n",
      "Epoch: 9/10... Step: 30240... Loss: 1.4321... Val Loss: 1.3379\n",
      "Epoch: 9/10... Step: 30250... Loss: 1.3544... Val Loss: 1.3369\n",
      "Epoch: 9/10... Step: 30260... Loss: 1.3478... Val Loss: 1.3371\n",
      "Epoch: 9/10... Step: 30270... Loss: 1.5072... Val Loss: 1.3393\n",
      "Epoch: 9/10... Step: 30280... Loss: 1.2913... Val Loss: 1.3384\n",
      "Epoch: 9/10... Step: 30290... Loss: 1.4387... Val Loss: 1.3374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10... Step: 30300... Loss: 1.3870... Val Loss: 1.3370\n",
      "Epoch: 9/10... Step: 30310... Loss: 1.4152... Val Loss: 1.3375\n",
      "Epoch: 9/10... Step: 30320... Loss: 1.3417... Val Loss: 1.3370\n",
      "Epoch: 9/10... Step: 30330... Loss: 1.5043... Val Loss: 1.3381\n",
      "Epoch: 9/10... Step: 30340... Loss: 1.5020... Val Loss: 1.3390\n",
      "Epoch: 9/10... Step: 30350... Loss: 1.3366... Val Loss: 1.3380\n",
      "Epoch: 9/10... Step: 30360... Loss: 1.4550... Val Loss: 1.3381\n",
      "Epoch: 9/10... Step: 30370... Loss: 1.4001... Val Loss: 1.3398\n",
      "Epoch: 9/10... Step: 30380... Loss: 1.3991... Val Loss: 1.3411\n",
      "Epoch: 9/10... Step: 30390... Loss: 1.5323... Val Loss: 1.3389\n",
      "Epoch: 9/10... Step: 30400... Loss: 1.3840... Val Loss: 1.3385\n",
      "Epoch: 9/10... Step: 30410... Loss: 1.4145... Val Loss: 1.3381\n",
      "Epoch: 9/10... Step: 30420... Loss: 1.4573... Val Loss: 1.3372\n",
      "Epoch: 10/10... Step: 30430... Loss: 1.6361... Val Loss: 1.3371\n",
      "Epoch: 10/10... Step: 30440... Loss: 1.3293... Val Loss: 1.3388\n",
      "Epoch: 10/10... Step: 30450... Loss: 1.2585... Val Loss: 1.3373\n",
      "Epoch: 10/10... Step: 30460... Loss: 1.3749... Val Loss: 1.3380\n",
      "Epoch: 10/10... Step: 30470... Loss: 1.4542... Val Loss: 1.3381\n",
      "Epoch: 10/10... Step: 30480... Loss: 1.3186... Val Loss: 1.3381\n",
      "Epoch: 10/10... Step: 30490... Loss: 1.3985... Val Loss: 1.3387\n",
      "Epoch: 10/10... Step: 30500... Loss: 1.4152... Val Loss: 1.3386\n",
      "Epoch: 10/10... Step: 30510... Loss: 1.5031... Val Loss: 1.3395\n",
      "Epoch: 10/10... Step: 30520... Loss: 1.4242... Val Loss: 1.3402\n",
      "Epoch: 10/10... Step: 30530... Loss: 1.3274... Val Loss: 1.3398\n",
      "Epoch: 10/10... Step: 30540... Loss: 1.3414... Val Loss: 1.3384\n",
      "Epoch: 10/10... Step: 30550... Loss: 1.4852... Val Loss: 1.3375\n",
      "Epoch: 10/10... Step: 30560... Loss: 1.3812... Val Loss: 1.3375\n",
      "Epoch: 10/10... Step: 30570... Loss: 1.3055... Val Loss: 1.3390\n",
      "Epoch: 10/10... Step: 30580... Loss: 1.3553... Val Loss: 1.3387\n",
      "Epoch: 10/10... Step: 30590... Loss: 1.4621... Val Loss: 1.3372\n",
      "Epoch: 10/10... Step: 30600... Loss: 1.4911... Val Loss: 1.3366\n",
      "Epoch: 10/10... Step: 30610... Loss: 1.4284... Val Loss: 1.3366\n",
      "Epoch: 10/10... Step: 30620... Loss: 1.4647... Val Loss: 1.3368\n",
      "Epoch: 10/10... Step: 30630... Loss: 1.4637... Val Loss: 1.3370\n",
      "Epoch: 10/10... Step: 30640... Loss: 1.4317... Val Loss: 1.3379\n",
      "Epoch: 10/10... Step: 30650... Loss: 1.3287... Val Loss: 1.3386\n",
      "Epoch: 10/10... Step: 30660... Loss: 1.4029... Val Loss: 1.3380\n",
      "Epoch: 10/10... Step: 30670... Loss: 1.4061... Val Loss: 1.3389\n",
      "Epoch: 10/10... Step: 30680... Loss: 1.3973... Val Loss: 1.3383\n",
      "Epoch: 10/10... Step: 30690... Loss: 1.3997... Val Loss: 1.3383\n",
      "Epoch: 10/10... Step: 30700... Loss: 1.4178... Val Loss: 1.3387\n",
      "Epoch: 10/10... Step: 30710... Loss: 1.4861... Val Loss: 1.3393\n",
      "Epoch: 10/10... Step: 30720... Loss: 1.4541... Val Loss: 1.3390\n",
      "Epoch: 10/10... Step: 30730... Loss: 1.4554... Val Loss: 1.3399\n",
      "Epoch: 10/10... Step: 30740... Loss: 1.5569... Val Loss: 1.3397\n",
      "Epoch: 10/10... Step: 30750... Loss: 1.3225... Val Loss: 1.3383\n",
      "Epoch: 10/10... Step: 30760... Loss: 1.3942... Val Loss: 1.3385\n",
      "Epoch: 10/10... Step: 30770... Loss: 1.3292... Val Loss: 1.3397\n",
      "Epoch: 10/10... Step: 30780... Loss: 1.2932... Val Loss: 1.3400\n",
      "Epoch: 10/10... Step: 30790... Loss: 1.3624... Val Loss: 1.3390\n",
      "Epoch: 10/10... Step: 30800... Loss: 1.4446... Val Loss: 1.3391\n",
      "Epoch: 10/10... Step: 30810... Loss: 1.4565... Val Loss: 1.3396\n",
      "Epoch: 10/10... Step: 30820... Loss: 1.4135... Val Loss: 1.3388\n",
      "Epoch: 10/10... Step: 30830... Loss: 1.4684... Val Loss: 1.3398\n",
      "Epoch: 10/10... Step: 30840... Loss: 1.4861... Val Loss: 1.3407\n",
      "Epoch: 10/10... Step: 30850... Loss: 1.3234... Val Loss: 1.3404\n",
      "Epoch: 10/10... Step: 30860... Loss: 1.3503... Val Loss: 1.3393\n",
      "Epoch: 10/10... Step: 30870... Loss: 1.4661... Val Loss: 1.3379\n",
      "Epoch: 10/10... Step: 30880... Loss: 1.3776... Val Loss: 1.3374\n",
      "Epoch: 10/10... Step: 30890... Loss: 1.4378... Val Loss: 1.3373\n",
      "Epoch: 10/10... Step: 30900... Loss: 1.4121... Val Loss: 1.3368\n",
      "Epoch: 10/10... Step: 30910... Loss: 1.4895... Val Loss: 1.3370\n",
      "Epoch: 10/10... Step: 30920... Loss: 1.3637... Val Loss: 1.3363\n",
      "Epoch: 10/10... Step: 30930... Loss: 1.4685... Val Loss: 1.3359\n",
      "Epoch: 10/10... Step: 30940... Loss: 1.4605... Val Loss: 1.3350\n",
      "Epoch: 10/10... Step: 30950... Loss: 1.4287... Val Loss: 1.3358\n",
      "Epoch: 10/10... Step: 30960... Loss: 1.4372... Val Loss: 1.3361\n",
      "Epoch: 10/10... Step: 30970... Loss: 1.4599... Val Loss: 1.3369\n",
      "Epoch: 10/10... Step: 30980... Loss: 1.3620... Val Loss: 1.3362\n",
      "Epoch: 10/10... Step: 30990... Loss: 1.4717... Val Loss: 1.3362\n",
      "Epoch: 10/10... Step: 31000... Loss: 1.4442... Val Loss: 1.3372\n",
      "Epoch: 10/10... Step: 31010... Loss: 1.4360... Val Loss: 1.3370\n",
      "Epoch: 10/10... Step: 31020... Loss: 1.4180... Val Loss: 1.3359\n",
      "Epoch: 10/10... Step: 31030... Loss: 1.4522... Val Loss: 1.3353\n",
      "Epoch: 10/10... Step: 31040... Loss: 1.3291... Val Loss: 1.3354\n",
      "Epoch: 10/10... Step: 31050... Loss: 1.3730... Val Loss: 1.3360\n",
      "Epoch: 10/10... Step: 31060... Loss: 1.5277... Val Loss: 1.3364\n",
      "Epoch: 10/10... Step: 31070... Loss: 1.4063... Val Loss: 1.3350\n",
      "Epoch: 10/10... Step: 31080... Loss: 1.4788... Val Loss: 1.3351\n",
      "Epoch: 10/10... Step: 31090... Loss: 1.4126... Val Loss: 1.3351\n",
      "Epoch: 10/10... Step: 31100... Loss: 1.3489... Val Loss: 1.3345\n",
      "Epoch: 10/10... Step: 31110... Loss: 1.4527... Val Loss: 1.3343\n",
      "Epoch: 10/10... Step: 31120... Loss: 1.3908... Val Loss: 1.3342\n",
      "Epoch: 10/10... Step: 31130... Loss: 1.3316... Val Loss: 1.3358\n",
      "Epoch: 10/10... Step: 31140... Loss: 1.3955... Val Loss: 1.3376\n",
      "Epoch: 10/10... Step: 31150... Loss: 1.3223... Val Loss: 1.3392\n",
      "Epoch: 10/10... Step: 31160... Loss: 1.3631... Val Loss: 1.3381\n",
      "Epoch: 10/10... Step: 31170... Loss: 1.4783... Val Loss: 1.3372\n",
      "Epoch: 10/10... Step: 31180... Loss: 1.3649... Val Loss: 1.3364\n",
      "Epoch: 10/10... Step: 31190... Loss: 1.4958... Val Loss: 1.3341\n",
      "Epoch: 10/10... Step: 31200... Loss: 1.5647... Val Loss: 1.3347\n",
      "Epoch: 10/10... Step: 31210... Loss: 1.5700... Val Loss: 1.3348\n",
      "Epoch: 10/10... Step: 31220... Loss: 1.4062... Val Loss: 1.3339\n",
      "Epoch: 10/10... Step: 31230... Loss: 1.3294... Val Loss: 1.3340\n",
      "Epoch: 10/10... Step: 31240... Loss: 1.3403... Val Loss: 1.3344\n",
      "Epoch: 10/10... Step: 31250... Loss: 1.3461... Val Loss: 1.3337\n",
      "Epoch: 10/10... Step: 31260... Loss: 1.4172... Val Loss: 1.3348\n",
      "Epoch: 10/10... Step: 31270... Loss: 1.3706... Val Loss: 1.3353\n",
      "Epoch: 10/10... Step: 31280... Loss: 1.4162... Val Loss: 1.3342\n",
      "Epoch: 10/10... Step: 31290... Loss: 1.4507... Val Loss: 1.3342\n",
      "Epoch: 10/10... Step: 31300... Loss: 1.4211... Val Loss: 1.3340\n",
      "Epoch: 10/10... Step: 31310... Loss: 1.4369... Val Loss: 1.3336\n",
      "Epoch: 10/10... Step: 31320... Loss: 1.4359... Val Loss: 1.3338\n",
      "Epoch: 10/10... Step: 31330... Loss: 1.4877... Val Loss: 1.3342\n",
      "Epoch: 10/10... Step: 31340... Loss: 1.2904... Val Loss: 1.3342\n",
      "Epoch: 10/10... Step: 31350... Loss: 1.3289... Val Loss: 1.3346\n",
      "Epoch: 10/10... Step: 31360... Loss: 1.4518... Val Loss: 1.3343\n",
      "Epoch: 10/10... Step: 31370... Loss: 1.4752... Val Loss: 1.3346\n",
      "Epoch: 10/10... Step: 31380... Loss: 1.3936... Val Loss: 1.3347\n",
      "Epoch: 10/10... Step: 31390... Loss: 1.4268... Val Loss: 1.3344\n",
      "Epoch: 10/10... Step: 31400... Loss: 1.4289... Val Loss: 1.3356\n",
      "Epoch: 10/10... Step: 31410... Loss: 1.3755... Val Loss: 1.3373\n",
      "Epoch: 10/10... Step: 31420... Loss: 1.3185... Val Loss: 1.3371\n",
      "Epoch: 10/10... Step: 31430... Loss: 1.3375... Val Loss: 1.3351\n",
      "Epoch: 10/10... Step: 31440... Loss: 1.3757... Val Loss: 1.3359\n",
      "Epoch: 10/10... Step: 31450... Loss: 1.4465... Val Loss: 1.3357\n",
      "Epoch: 10/10... Step: 31460... Loss: 1.3671... Val Loss: 1.3353\n",
      "Epoch: 10/10... Step: 31470... Loss: 1.3709... Val Loss: 1.3357\n",
      "Epoch: 10/10... Step: 31480... Loss: 1.4738... Val Loss: 1.3356\n",
      "Epoch: 10/10... Step: 31490... Loss: 1.3934... Val Loss: 1.3352\n",
      "Epoch: 10/10... Step: 31500... Loss: 1.3263... Val Loss: 1.3353\n",
      "Epoch: 10/10... Step: 31510... Loss: 1.3846... Val Loss: 1.3351\n",
      "Epoch: 10/10... Step: 31520... Loss: 1.4380... Val Loss: 1.3330\n",
      "Epoch: 10/10... Step: 31530... Loss: 1.4247... Val Loss: 1.3322\n",
      "Epoch: 10/10... Step: 31540... Loss: 1.4376... Val Loss: 1.3324\n",
      "Epoch: 10/10... Step: 31550... Loss: 1.4604... Val Loss: 1.3330\n",
      "Epoch: 10/10... Step: 31560... Loss: 1.4159... Val Loss: 1.3332\n",
      "Epoch: 10/10... Step: 31570... Loss: 1.3302... Val Loss: 1.3344\n",
      "Epoch: 10/10... Step: 31580... Loss: 1.4089... Val Loss: 1.3349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10... Step: 31590... Loss: 1.3373... Val Loss: 1.3364\n",
      "Epoch: 10/10... Step: 31600... Loss: 1.3471... Val Loss: 1.3363\n",
      "Epoch: 10/10... Step: 31610... Loss: 1.3650... Val Loss: 1.3359\n",
      "Epoch: 10/10... Step: 31620... Loss: 1.4061... Val Loss: 1.3352\n",
      "Epoch: 10/10... Step: 31630... Loss: 1.3700... Val Loss: 1.3351\n",
      "Epoch: 10/10... Step: 31640... Loss: 1.4881... Val Loss: 1.3350\n",
      "Epoch: 10/10... Step: 31650... Loss: 1.3237... Val Loss: 1.3363\n",
      "Epoch: 10/10... Step: 31660... Loss: 1.4668... Val Loss: 1.3369\n",
      "Epoch: 10/10... Step: 31670... Loss: 1.4303... Val Loss: 1.3356\n",
      "Epoch: 10/10... Step: 31680... Loss: 1.4201... Val Loss: 1.3360\n",
      "Epoch: 10/10... Step: 31690... Loss: 1.4062... Val Loss: 1.3371\n",
      "Epoch: 10/10... Step: 31700... Loss: 1.4181... Val Loss: 1.3372\n",
      "Epoch: 10/10... Step: 31710... Loss: 1.3965... Val Loss: 1.3374\n",
      "Epoch: 10/10... Step: 31720... Loss: 1.4006... Val Loss: 1.3369\n",
      "Epoch: 10/10... Step: 31730... Loss: 1.5370... Val Loss: 1.3355\n",
      "Epoch: 10/10... Step: 31740... Loss: 1.3163... Val Loss: 1.3353\n",
      "Epoch: 10/10... Step: 31750... Loss: 1.4903... Val Loss: 1.3350\n",
      "Epoch: 10/10... Step: 31760... Loss: 1.4537... Val Loss: 1.3334\n",
      "Epoch: 10/10... Step: 31770... Loss: 1.5564... Val Loss: 1.3335\n",
      "Epoch: 10/10... Step: 31780... Loss: 1.4170... Val Loss: 1.3327\n",
      "Epoch: 10/10... Step: 31790... Loss: 1.4526... Val Loss: 1.3345\n",
      "Epoch: 10/10... Step: 31800... Loss: 1.4130... Val Loss: 1.3354\n",
      "Epoch: 10/10... Step: 31810... Loss: 1.4401... Val Loss: 1.3335\n",
      "Epoch: 10/10... Step: 31820... Loss: 1.3360... Val Loss: 1.3329\n",
      "Epoch: 10/10... Step: 31830... Loss: 1.4022... Val Loss: 1.3334\n",
      "Epoch: 10/10... Step: 31840... Loss: 1.3891... Val Loss: 1.3342\n",
      "Epoch: 10/10... Step: 31850... Loss: 1.4239... Val Loss: 1.3353\n",
      "Epoch: 10/10... Step: 31860... Loss: 1.3633... Val Loss: 1.3343\n",
      "Epoch: 10/10... Step: 31870... Loss: 1.4474... Val Loss: 1.3333\n",
      "Epoch: 10/10... Step: 31880... Loss: 1.4429... Val Loss: 1.3341\n",
      "Epoch: 10/10... Step: 31890... Loss: 1.3639... Val Loss: 1.3354\n",
      "Epoch: 10/10... Step: 31900... Loss: 1.4671... Val Loss: 1.3362\n",
      "Epoch: 10/10... Step: 31910... Loss: 1.4798... Val Loss: 1.3365\n",
      "Epoch: 10/10... Step: 31920... Loss: 1.4051... Val Loss: 1.3357\n",
      "Epoch: 10/10... Step: 31930... Loss: 1.4761... Val Loss: 1.3363\n",
      "Epoch: 10/10... Step: 31940... Loss: 1.3525... Val Loss: 1.3355\n",
      "Epoch: 10/10... Step: 31950... Loss: 1.4063... Val Loss: 1.3359\n",
      "Epoch: 10/10... Step: 31960... Loss: 1.4607... Val Loss: 1.3345\n",
      "Epoch: 10/10... Step: 31970... Loss: 1.4684... Val Loss: 1.3344\n",
      "Epoch: 10/10... Step: 31980... Loss: 1.4730... Val Loss: 1.3345\n",
      "Epoch: 10/10... Step: 31990... Loss: 1.3752... Val Loss: 1.3358\n",
      "Epoch: 10/10... Step: 32000... Loss: 1.4765... Val Loss: 1.3343\n",
      "Epoch: 10/10... Step: 32010... Loss: 1.3478... Val Loss: 1.3338\n",
      "Epoch: 10/10... Step: 32020... Loss: 1.5472... Val Loss: 1.3341\n",
      "Epoch: 10/10... Step: 32030... Loss: 1.4603... Val Loss: 1.3340\n",
      "Epoch: 10/10... Step: 32040... Loss: 1.4629... Val Loss: 1.3325\n",
      "Epoch: 10/10... Step: 32050... Loss: 1.4422... Val Loss: 1.3315\n",
      "Epoch: 10/10... Step: 32060... Loss: 1.4554... Val Loss: 1.3324\n",
      "Epoch: 10/10... Step: 32070... Loss: 1.3729... Val Loss: 1.3315\n",
      "Epoch: 10/10... Step: 32080... Loss: 1.3735... Val Loss: 1.3313\n",
      "Epoch: 10/10... Step: 32090... Loss: 1.3112... Val Loss: 1.3319\n",
      "Epoch: 10/10... Step: 32100... Loss: 1.2886... Val Loss: 1.3313\n",
      "Epoch: 10/10... Step: 32110... Loss: 1.4581... Val Loss: 1.3321\n",
      "Epoch: 10/10... Step: 32120... Loss: 1.4131... Val Loss: 1.3321\n",
      "Epoch: 10/10... Step: 32130... Loss: 1.3320... Val Loss: 1.3311\n",
      "Epoch: 10/10... Step: 32140... Loss: 1.4288... Val Loss: 1.3297\n",
      "Epoch: 10/10... Step: 32150... Loss: 1.4322... Val Loss: 1.3303\n",
      "Epoch: 10/10... Step: 32160... Loss: 1.4178... Val Loss: 1.3305\n",
      "Epoch: 10/10... Step: 32170... Loss: 1.4059... Val Loss: 1.3325\n",
      "Epoch: 10/10... Step: 32180... Loss: 1.3567... Val Loss: 1.3329\n",
      "Epoch: 10/10... Step: 32190... Loss: 1.4164... Val Loss: 1.3313\n",
      "Epoch: 10/10... Step: 32200... Loss: 1.3951... Val Loss: 1.3312\n",
      "Epoch: 10/10... Step: 32210... Loss: 1.3518... Val Loss: 1.3312\n",
      "Epoch: 10/10... Step: 32220... Loss: 1.2993... Val Loss: 1.3327\n",
      "Epoch: 10/10... Step: 32230... Loss: 1.4257... Val Loss: 1.3319\n",
      "Epoch: 10/10... Step: 32240... Loss: 1.4275... Val Loss: 1.3305\n",
      "Epoch: 10/10... Step: 32250... Loss: 1.3751... Val Loss: 1.3302\n",
      "Epoch: 10/10... Step: 32260... Loss: 1.3265... Val Loss: 1.3299\n",
      "Epoch: 10/10... Step: 32270... Loss: 1.2691... Val Loss: 1.3305\n",
      "Epoch: 10/10... Step: 32280... Loss: 1.3420... Val Loss: 1.3326\n",
      "Epoch: 10/10... Step: 32290... Loss: 1.2600... Val Loss: 1.3343\n",
      "Epoch: 10/10... Step: 32300... Loss: 1.3634... Val Loss: 1.3325\n",
      "Epoch: 10/10... Step: 32310... Loss: 1.4483... Val Loss: 1.3328\n",
      "Epoch: 10/10... Step: 32320... Loss: 1.4304... Val Loss: 1.3346\n",
      "Epoch: 10/10... Step: 32330... Loss: 1.3672... Val Loss: 1.3341\n",
      "Epoch: 10/10... Step: 32340... Loss: 1.3704... Val Loss: 1.3332\n",
      "Epoch: 10/10... Step: 32350... Loss: 1.4279... Val Loss: 1.3323\n",
      "Epoch: 10/10... Step: 32360... Loss: 1.4752... Val Loss: 1.3311\n",
      "Epoch: 10/10... Step: 32370... Loss: 1.4158... Val Loss: 1.3301\n",
      "Epoch: 10/10... Step: 32380... Loss: 1.4032... Val Loss: 1.3309\n",
      "Epoch: 10/10... Step: 32390... Loss: 1.4028... Val Loss: 1.3297\n",
      "Epoch: 10/10... Step: 32400... Loss: 1.3897... Val Loss: 1.3297\n",
      "Epoch: 10/10... Step: 32410... Loss: 1.2217... Val Loss: 1.3306\n",
      "Epoch: 10/10... Step: 32420... Loss: 1.3319... Val Loss: 1.3314\n",
      "Epoch: 10/10... Step: 32430... Loss: 1.3436... Val Loss: 1.3314\n",
      "Epoch: 10/10... Step: 32440... Loss: 1.4077... Val Loss: 1.3310\n",
      "Epoch: 10/10... Step: 32450... Loss: 1.3806... Val Loss: 1.3314\n",
      "Epoch: 10/10... Step: 32460... Loss: 1.5148... Val Loss: 1.3319\n",
      "Epoch: 10/10... Step: 32470... Loss: 1.3161... Val Loss: 1.3323\n",
      "Epoch: 10/10... Step: 32480... Loss: 1.5512... Val Loss: 1.3318\n",
      "Epoch: 10/10... Step: 32490... Loss: 1.3213... Val Loss: 1.3320\n",
      "Epoch: 10/10... Step: 32500... Loss: 1.3338... Val Loss: 1.3311\n",
      "Epoch: 10/10... Step: 32510... Loss: 1.4013... Val Loss: 1.3304\n",
      "Epoch: 10/10... Step: 32520... Loss: 1.4121... Val Loss: 1.3309\n",
      "Epoch: 10/10... Step: 32530... Loss: 1.3568... Val Loss: 1.3335\n",
      "Epoch: 10/10... Step: 32540... Loss: 1.3528... Val Loss: 1.3326\n",
      "Epoch: 10/10... Step: 32550... Loss: 1.2610... Val Loss: 1.3323\n",
      "Epoch: 10/10... Step: 32560... Loss: 1.4005... Val Loss: 1.3330\n",
      "Epoch: 10/10... Step: 32570... Loss: 1.4463... Val Loss: 1.3338\n",
      "Epoch: 10/10... Step: 32580... Loss: 1.4203... Val Loss: 1.3316\n",
      "Epoch: 10/10... Step: 32590... Loss: 1.4817... Val Loss: 1.3305\n",
      "Epoch: 10/10... Step: 32600... Loss: 1.5050... Val Loss: 1.3301\n",
      "Epoch: 10/10... Step: 32610... Loss: 1.3829... Val Loss: 1.3313\n",
      "Epoch: 10/10... Step: 32620... Loss: 1.4460... Val Loss: 1.3320\n",
      "Epoch: 10/10... Step: 32630... Loss: 1.2802... Val Loss: 1.3315\n",
      "Epoch: 10/10... Step: 32640... Loss: 1.2716... Val Loss: 1.3313\n",
      "Epoch: 10/10... Step: 32650... Loss: 1.3919... Val Loss: 1.3309\n",
      "Epoch: 10/10... Step: 32660... Loss: 1.4608... Val Loss: 1.3322\n",
      "Epoch: 10/10... Step: 32670... Loss: 1.3533... Val Loss: 1.3320\n",
      "Epoch: 10/10... Step: 32680... Loss: 1.3425... Val Loss: 1.3316\n",
      "Epoch: 10/10... Step: 32690... Loss: 1.4265... Val Loss: 1.3322\n",
      "Epoch: 10/10... Step: 32700... Loss: 1.4767... Val Loss: 1.3331\n",
      "Epoch: 10/10... Step: 32710... Loss: 1.3527... Val Loss: 1.3331\n",
      "Epoch: 10/10... Step: 32720... Loss: 1.4080... Val Loss: 1.3322\n",
      "Epoch: 10/10... Step: 32730... Loss: 1.3701... Val Loss: 1.3311\n",
      "Epoch: 10/10... Step: 32740... Loss: 1.5505... Val Loss: 1.3315\n",
      "Epoch: 10/10... Step: 32750... Loss: 1.4585... Val Loss: 1.3316\n",
      "Epoch: 10/10... Step: 32760... Loss: 1.4215... Val Loss: 1.3308\n",
      "Epoch: 10/10... Step: 32770... Loss: 1.5028... Val Loss: 1.3300\n",
      "Epoch: 10/10... Step: 32780... Loss: 1.5149... Val Loss: 1.3302\n",
      "Epoch: 10/10... Step: 32790... Loss: 1.6355... Val Loss: 1.3300\n",
      "Epoch: 10/10... Step: 32800... Loss: 1.4390... Val Loss: 1.3294\n",
      "Epoch: 10/10... Step: 32810... Loss: 1.4244... Val Loss: 1.3300\n",
      "Epoch: 10/10... Step: 32820... Loss: 1.5402... Val Loss: 1.3304\n",
      "Epoch: 10/10... Step: 32830... Loss: 1.4549... Val Loss: 1.3304\n",
      "Epoch: 10/10... Step: 32840... Loss: 1.3579... Val Loss: 1.3306\n",
      "Epoch: 10/10... Step: 32850... Loss: 1.4645... Val Loss: 1.3295\n",
      "Epoch: 10/10... Step: 32860... Loss: 1.4519... Val Loss: 1.3287\n",
      "Epoch: 10/10... Step: 32870... Loss: 1.4276... Val Loss: 1.3290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10... Step: 32880... Loss: 1.5012... Val Loss: 1.3325\n",
      "Epoch: 10/10... Step: 32890... Loss: 1.4250... Val Loss: 1.3305\n",
      "Epoch: 10/10... Step: 32900... Loss: 1.4419... Val Loss: 1.3290\n",
      "Epoch: 10/10... Step: 32910... Loss: 1.3974... Val Loss: 1.3296\n",
      "Epoch: 10/10... Step: 32920... Loss: 1.3941... Val Loss: 1.3309\n",
      "Epoch: 10/10... Step: 32930... Loss: 1.3297... Val Loss: 1.3313\n",
      "Epoch: 10/10... Step: 32940... Loss: 1.3218... Val Loss: 1.3320\n",
      "Epoch: 10/10... Step: 32950... Loss: 1.3685... Val Loss: 1.3334\n",
      "Epoch: 10/10... Step: 32960... Loss: 1.4843... Val Loss: 1.3337\n",
      "Epoch: 10/10... Step: 32970... Loss: 1.4074... Val Loss: 1.3306\n",
      "Epoch: 10/10... Step: 32980... Loss: 1.3301... Val Loss: 1.3308\n",
      "Epoch: 10/10... Step: 32990... Loss: 1.4775... Val Loss: 1.3308\n",
      "Epoch: 10/10... Step: 33000... Loss: 1.3750... Val Loss: 1.3336\n",
      "Epoch: 10/10... Step: 33010... Loss: 1.3679... Val Loss: 1.3337\n",
      "Epoch: 10/10... Step: 33020... Loss: 1.3986... Val Loss: 1.3326\n",
      "Epoch: 10/10... Step: 33030... Loss: 1.2954... Val Loss: 1.3324\n",
      "Epoch: 10/10... Step: 33040... Loss: 1.4555... Val Loss: 1.3326\n",
      "Epoch: 10/10... Step: 33050... Loss: 1.3513... Val Loss: 1.3321\n",
      "Epoch: 10/10... Step: 33060... Loss: 1.4466... Val Loss: 1.3316\n",
      "Epoch: 10/10... Step: 33070... Loss: 1.4057... Val Loss: 1.3310\n",
      "Epoch: 10/10... Step: 33080... Loss: 1.3411... Val Loss: 1.3308\n",
      "Epoch: 10/10... Step: 33090... Loss: 1.4197... Val Loss: 1.3298\n",
      "Epoch: 10/10... Step: 33100... Loss: 1.4412... Val Loss: 1.3304\n",
      "Epoch: 10/10... Step: 33110... Loss: 1.3756... Val Loss: 1.3309\n",
      "Epoch: 10/10... Step: 33120... Loss: 1.5102... Val Loss: 1.3313\n",
      "Epoch: 10/10... Step: 33130... Loss: 1.4461... Val Loss: 1.3318\n",
      "Epoch: 10/10... Step: 33140... Loss: 1.4242... Val Loss: 1.3319\n",
      "Epoch: 10/10... Step: 33150... Loss: 1.3973... Val Loss: 1.3318\n",
      "Epoch: 10/10... Step: 33160... Loss: 1.4822... Val Loss: 1.3325\n",
      "Epoch: 10/10... Step: 33170... Loss: 1.3013... Val Loss: 1.3319\n",
      "Epoch: 10/10... Step: 33180... Loss: 1.5006... Val Loss: 1.3319\n",
      "Epoch: 10/10... Step: 33190... Loss: 1.4303... Val Loss: 1.3311\n",
      "Epoch: 10/10... Step: 33200... Loss: 1.3587... Val Loss: 1.3306\n",
      "Epoch: 10/10... Step: 33210... Loss: 1.4226... Val Loss: 1.3311\n",
      "Epoch: 10/10... Step: 33220... Loss: 1.3939... Val Loss: 1.3316\n",
      "Epoch: 10/10... Step: 33230... Loss: 1.4160... Val Loss: 1.3321\n",
      "Epoch: 10/10... Step: 33240... Loss: 1.3959... Val Loss: 1.3309\n",
      "Epoch: 10/10... Step: 33250... Loss: 1.4485... Val Loss: 1.3296\n",
      "Epoch: 10/10... Step: 33260... Loss: 1.3657... Val Loss: 1.3300\n",
      "Epoch: 10/10... Step: 33270... Loss: 1.4486... Val Loss: 1.3322\n",
      "Epoch: 10/10... Step: 33280... Loss: 1.3418... Val Loss: 1.3338\n",
      "Epoch: 10/10... Step: 33290... Loss: 1.4670... Val Loss: 1.3315\n",
      "Epoch: 10/10... Step: 33300... Loss: 1.4949... Val Loss: 1.3308\n",
      "Epoch: 10/10... Step: 33310... Loss: 1.4323... Val Loss: 1.3307\n",
      "Epoch: 10/10... Step: 33320... Loss: 1.3890... Val Loss: 1.3308\n",
      "Epoch: 10/10... Step: 33330... Loss: 1.2717... Val Loss: 1.3311\n",
      "Epoch: 10/10... Step: 33340... Loss: 1.4881... Val Loss: 1.3307\n",
      "Epoch: 10/10... Step: 33350... Loss: 1.4053... Val Loss: 1.3303\n",
      "Epoch: 10/10... Step: 33360... Loss: 1.3887... Val Loss: 1.3312\n",
      "Epoch: 10/10... Step: 33370... Loss: 1.4731... Val Loss: 1.3318\n",
      "Epoch: 10/10... Step: 33380... Loss: 1.5635... Val Loss: 1.3314\n",
      "Epoch: 10/10... Step: 33390... Loss: 1.4206... Val Loss: 1.3319\n",
      "Epoch: 10/10... Step: 33400... Loss: 1.4485... Val Loss: 1.3320\n",
      "Epoch: 10/10... Step: 33410... Loss: 1.3378... Val Loss: 1.3324\n",
      "Epoch: 10/10... Step: 33420... Loss: 1.3677... Val Loss: 1.3310\n",
      "Epoch: 10/10... Step: 33430... Loss: 1.2824... Val Loss: 1.3318\n",
      "Epoch: 10/10... Step: 33440... Loss: 1.3655... Val Loss: 1.3326\n",
      "Epoch: 10/10... Step: 33450... Loss: 1.4142... Val Loss: 1.3332\n",
      "Epoch: 10/10... Step: 33460... Loss: 1.3950... Val Loss: 1.3342\n",
      "Epoch: 10/10... Step: 33470... Loss: 1.3810... Val Loss: 1.3345\n",
      "Epoch: 10/10... Step: 33480... Loss: 1.4024... Val Loss: 1.3346\n",
      "Epoch: 10/10... Step: 33490... Loss: 1.5144... Val Loss: 1.3341\n",
      "Epoch: 10/10... Step: 33500... Loss: 1.5297... Val Loss: 1.3350\n",
      "Epoch: 10/10... Step: 33510... Loss: 1.4352... Val Loss: 1.3350\n",
      "Epoch: 10/10... Step: 33520... Loss: 1.4754... Val Loss: 1.3344\n",
      "Epoch: 10/10... Step: 33530... Loss: 1.4024... Val Loss: 1.3342\n",
      "Epoch: 10/10... Step: 33540... Loss: 1.4253... Val Loss: 1.3357\n",
      "Epoch: 10/10... Step: 33550... Loss: 1.4958... Val Loss: 1.3343\n",
      "Epoch: 10/10... Step: 33560... Loss: 1.4612... Val Loss: 1.3324\n",
      "Epoch: 10/10... Step: 33570... Loss: 1.5244... Val Loss: 1.3312\n",
      "Epoch: 10/10... Step: 33580... Loss: 1.3647... Val Loss: 1.3320\n",
      "Epoch: 10/10... Step: 33590... Loss: 1.3358... Val Loss: 1.3317\n",
      "Epoch: 10/10... Step: 33600... Loss: 1.4174... Val Loss: 1.3326\n",
      "Epoch: 10/10... Step: 33610... Loss: 1.3210... Val Loss: 1.3343\n",
      "Epoch: 10/10... Step: 33620... Loss: 1.3973... Val Loss: 1.3337\n",
      "Epoch: 10/10... Step: 33630... Loss: 1.4276... Val Loss: 1.3331\n",
      "Epoch: 10/10... Step: 33640... Loss: 1.3204... Val Loss: 1.3331\n",
      "Epoch: 10/10... Step: 33650... Loss: 1.5033... Val Loss: 1.3349\n",
      "Epoch: 10/10... Step: 33660... Loss: 1.4086... Val Loss: 1.3357\n",
      "Epoch: 10/10... Step: 33670... Loss: 1.3829... Val Loss: 1.3346\n",
      "Epoch: 10/10... Step: 33680... Loss: 1.5374... Val Loss: 1.3335\n",
      "Epoch: 10/10... Step: 33690... Loss: 1.4012... Val Loss: 1.3330\n",
      "Epoch: 10/10... Step: 33700... Loss: 1.5136... Val Loss: 1.3335\n",
      "Epoch: 10/10... Step: 33710... Loss: 1.3023... Val Loss: 1.3341\n",
      "Epoch: 10/10... Step: 33720... Loss: 1.3617... Val Loss: 1.3344\n",
      "Epoch: 10/10... Step: 33730... Loss: 1.3919... Val Loss: 1.3343\n",
      "Epoch: 10/10... Step: 33740... Loss: 1.3877... Val Loss: 1.3344\n",
      "Epoch: 10/10... Step: 33750... Loss: 1.2956... Val Loss: 1.3345\n",
      "Epoch: 10/10... Step: 33760... Loss: 1.4102... Val Loss: 1.3361\n",
      "Epoch: 10/10... Step: 33770... Loss: 1.3314... Val Loss: 1.3368\n",
      "Epoch: 10/10... Step: 33780... Loss: 1.4596... Val Loss: 1.3350\n",
      "Epoch: 10/10... Step: 33790... Loss: 1.3124... Val Loss: 1.3344\n",
      "Epoch: 10/10... Step: 33800... Loss: 1.4391... Val Loss: 1.3335\n",
      "Epoch: 10/10... Step: 33810... Loss: 1.6648... Val Loss: 1.3328\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seq_length = 14\n",
    "n_epochs = 10 # \n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_10epochs.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the whole model\n",
    "torch.save(net, 'model1.pt')\n",
    "# torch.save(net.state_dict(), 'model_try.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(88, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=88, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the whole Model\n",
    "# net.load_state_dict(torch.load('model.pt'))\n",
    "# net.eval()\n",
    "\n",
    "#Loading the whole Model\n",
    "net = torch.load('model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='whose conduct others', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou wert , and fall  and with the last and flattering treasure  and life the shore of mortal hairly love , i see  all thick and leaves that may as thought and friend  then she well would i spoke i love a silent way  when thou thou hast i shall the world and thee  the blushing show , who without his soften flames  are woo 'd them to a well to take the sea  the foe of the figure to thy love  and so , and though the work that strong and faire  some with thy honey far whose fatal world in morning first  and with a stone of hearts  with strains through the life and to be but i see  and she , and tenth thee to her home that stands  the sense to tress of married face  so warry , so , thy house as if though in the best  to live , a sight , and with my serfect , and which i seem 'd  the wheat thoughes to show the base  to take the steel of worship to his hour  that truth , and light and fair  only i lave , and then , as in the soul and flood  of the leaf , when they this bright looks , that shall that s\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='thou wert', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who are  to see the lighter , which i waked as sight  the soul with selfe as there , the latest lord  the barbeds , satisfied a troubled borrows , shall not far  with streams that wild by signes shares the love  when in strength of such sole strength , a praise  the worth , in shore , to love and truth and spread  the bounds of love , thou spoke i would not be to see  so were went and strong to the flowers of storm  still to a stain of meane , when i as the beauty of the sun \n",
      "when shall to be a frost thing  the sea with those thou all thou art and true  who lie with sights the sun  the still their fleases of my love and star and breath  and who , and love in some a many fate  who with this shell  that i the book , and walls to see  would not supplied in thee , they to my fame , but i said , startled with the late  of sudden friend , before the stray  she held them woman 's world too said  trembles , till high to listening the sun , the live  the word of that so meaning  then too me stand , beh\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='who are', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
